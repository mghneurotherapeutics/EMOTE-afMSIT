{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Preprocessing\n",
    "## Behavior Analysis\n",
    "### Generate trial regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import concat, read_csv\n",
    "from scipy.stats import gamma\n",
    "def normalize(arr): return (arr - arr.min()) / (arr.max() - arr.min())\n",
    "\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/behavior'\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "threshold = 0.005\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load / Concatenate / Prepare Data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "df = []\n",
    "for subject in subjects:\n",
    "    \n",
    "    ## Load CSV.\n",
    "    csv = read_csv(os.path.join(root_dir,'%s_msit_data.txt' %subject))\n",
    "    \n",
    "    ## Limit columns.\n",
    "    csv = csv[['SubjID','trial','iaps','DBS','interference','valence','arousal','responseTime','responseCorrect']]\n",
    "    \n",
    "    ## Rename columns.\n",
    "    csv.columns = ['Subject', 'Trial', 'IAPS', 'DBS', 'Interference', 'Valence_Obj', 'Arousal_Obj', 'RT', 'Accuracy']\n",
    "\n",
    "    ## Load IAPS ratings.\n",
    "    iaps = read_csv(os.path.join(root_dir,'%s_IAPS_SAM.csv' %subject))\n",
    "    iaps = iaps[['IAPS_Number','Valence','Arousal']]\n",
    "    iaps.columns = ['IAPS','Valence_Subj','Arousal_Subj']\n",
    "\n",
    "    ## Merge. Append.\n",
    "    csv = csv.merge(iaps, on='IAPS')\n",
    "    cols = ['Subject', 'Trial', 'IAPS', 'DBS', 'Interference', 'Valence_Obj', 'Arousal_Obj', \n",
    "            'Valence_Subj', 'Arousal_Subj', 'RT', 'Accuracy']\n",
    "    csv = csv[cols]\n",
    "    df.append(csv)\n",
    "\n",
    "## Merge data. Sort.\n",
    "df = concat(df)\n",
    "df['DBS'] = np.where(df['DBS']=='DBSoff',0,1)\n",
    "df = df.sort_values(['Subject','DBS','Trial']).reset_index(drop=True)\n",
    "\n",
    "## Normalize regressors.\n",
    "df['nsArousal'] = normalize(df.Arousal_Subj)\n",
    "df['nsValence'] = normalize(df.Valence_Subj)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Determine Trials for Inclusion/Exclusion.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Set missing RTs to NaNs.\n",
    "df['RT'] = np.where(df.Accuracy==-1, np.nan, df.RT)\n",
    "df['Accuracy'] = np.where(df.Accuracy==-1, np.nan, df.Accuracy)\n",
    "df['Missing'] = df.Accuracy.isnull().astype(int)\n",
    "\n",
    "## Add Error column.\n",
    "df['Error'] = 1 - df.Accuracy\n",
    "\n",
    "## Add Post-Error Column.\n",
    "df['PostError'] = 0\n",
    "for subject in df.Subject.unique():\n",
    "    error = df.loc[df.Subject==subject,'Error']\n",
    "    posterror = np.insert(np.roll(error,1)[1:], 0, 0)\n",
    "    df.loc[df.Subject==subject,'PostError'] = posterror\n",
    "\n",
    "## Iteratively detect outliers across subjects by fitting a Gamma distribution.\n",
    "df['GammaCDF'], df['Outlier'] = 0, 0\n",
    "for subject in df.Subject.unique():\n",
    "    \n",
    "    ## Fit Gamma to reaction time distribution.\n",
    "    shape, loc, scale = gamma.fit(df.loc[(df.Subject==subject)&(~df.RT.isnull()),'RT'], floc=0)\n",
    "    \n",
    "    ## Find outliers given likelihood threshold.\n",
    "    cdf = gamma.cdf(df.loc[(df.Subject==subject)&(~df.RT.isnull()),'RT'], shape, loc=loc, scale=scale)\n",
    "    outliers = (cdf < threshold) | (cdf > 1 - threshold)\n",
    "    \n",
    "    ## Append information.\n",
    "    df.loc[(df.Subject==subject)&(~df.RT.isnull()), 'GammaCDF'] += cdf\n",
    "    df.loc[(df.Subject==subject)&(~df.RT.isnull()), 'Outlier'] += outliers.astype(int)\n",
    "    \n",
    "## Generate exclude.\n",
    "df['Exclude'] = np.where( df[['Missing','Error','PostError','Outlier']].sum(axis=1), 1, 0)\n",
    "print '%s trials (%0.2f%%) excluded.' %(df.Exclude.sum(), df.Exclude.mean())\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Save.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "df.to_csv('%s/afMSIT_group_data.csv' %root_dir, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parcellation\n",
    "### Make EMOTE Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from mne import read_label, read_source_spaces, read_surface, set_log_level\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "fs_dir = '/space/sophia/2/users/EMOTE-DBS/freesurfs'\n",
    "subject = 'BRTU'\n",
    "\n",
    "parc = 'laus250'\n",
    "label_dir = os.path.join(fs_dir,subject,'label',parc)\n",
    "out_dir = os.path.join(fs_dir,subject,'label','april2016')\n",
    "\n",
    "if os.path.isdir(out_dir): shutil.rmtree(out_dir)\n",
    "os.makedirs(out_dir)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Build Left Hemisphere Labels.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "hemi = 'lh'\n",
    "rr, _ = read_surface(os.path.join(fs_dir, subject, 'surf', '%s.inflated' %hemi))\n",
    "src = read_source_spaces(os.path.join(fs_dir, subject, 'bem', '%s-oct-6-src.fif' %subject))[0]\n",
    "\n",
    "lhdict = {'dlpfc_1-lh':['caudalmiddlefrontal_1', 'caudalmiddlefrontal_5', 'caudalmiddlefrontal_6'],\n",
    "          'dlpfc_2-lh':['caudalmiddlefrontal_2', 'caudalmiddlefrontal_3', 'caudalmiddlefrontal_4'],\n",
    "          'dlpfc_3-lh':['rostralmiddlefrontal_2', 'rostralmiddlefrontal_3'],\n",
    "          'dlpfc_4-lh':['rostralmiddlefrontal_1', 'rostralmiddlefrontal_5'],\n",
    "          'dlpfc_5-lh':['parstriangularis_2', 'parsopercularis_2'],\n",
    "          'dlpfc_6-lh':['parsopercularis_3', 'parsopercularis_4'],\n",
    "          'racc-lh':['rostralanteriorcingulate_1','rostralanteriorcingulate_2'],\n",
    "          'dacc-lh':['caudalanteriorcingulate_1','caudalanteriorcingulate_2',],\n",
    "          'pcc-lh':['posteriorcingulate_2','posteriorcingulate_3']}\n",
    "\n",
    "for k,V in lhdict.iteritems():\n",
    "    \n",
    "    label = np.sum([read_label(os.path.join(label_dir,'%s-%s.label' %(v,hemi)), subject=subject) \n",
    "                    for v in V])\n",
    "    n_vert = np.intersect1d(src['vertno'], label.vertices).shape[0]\n",
    "    print '%s\\t%s' %(n_vert,k)\n",
    "    label.save(os.path.join(out_dir, '%s.label' %k))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Build Right Hemisphere Labels.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "hemi = 'rh'\n",
    "rr, _ = read_surface(os.path.join(fs_dir, subject, 'surf', '%s.inflated' %hemi))\n",
    "src = read_source_spaces(os.path.join(fs_dir, subject, 'bem', '%s-oct-6-src.fif' %subject))[1]\n",
    "\n",
    "rhdict = {'dlpfc_1-rh':['caudalmiddlefrontal_1', 'caudalmiddlefrontal_2', 'caudalmiddlefrontal_5'],\n",
    "          'dlpfc_2-rh':['caudalmiddlefrontal_3', 'caudalmiddlefrontal_4'],\n",
    "          'dlpfc_3-rh':['rostralmiddlefrontal_2', 'rostralmiddlefrontal_3'],\n",
    "          'dlpfc_4-rh':['rostralmiddlefrontal_1', 'rostralmiddlefrontal_5'],\n",
    "          'dlpfc_5-rh':['parstriangularis_2', 'parsopercularis_1'],\n",
    "          'dlpfc_6-rh':['parsopercularis_3', 'parsopercularis_4'],\n",
    "          'racc-rh':['rostralanteriorcingulate_1','rostralanteriorcingulate_2'],\n",
    "          'dacc-rh':['caudalanteriorcingulate_1','caudalanteriorcingulate_2','caudalanteriorcingulate_3'],\n",
    "          'pcc-rh':['posteriorcingulate_2','posteriorcingulate_3']}\n",
    "\n",
    "for k,V in rhdict.iteritems():\n",
    "    label = np.sum([read_label(os.path.join(label_dir,'%s-%s.label' %(v,hemi)), subject=subject) \n",
    "                    for v in V])\n",
    "    n_vert = np.intersect1d(src['vertno'], label.vertices).shape[0]\n",
    "    print '%s\\t%s' %(n_vert,k)\n",
    "    label.save(os.path.join(out_dir, '%s.label' %k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesing 1: Raw Data\n",
    "### Fixing MEWA: Digitization\n",
    "Something got way messed up. Here we make MNE knows what is EEG and what is extra points.\n",
    "\n",
    "NOTE: Copied over one of the original files for MEWA and renamed it MEWA_msit_unmasked_raw.fif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne.io import Raw\n",
    "from pandas import read_table\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Specify parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT_april2016'\n",
    "raw_file = 'MEWA_msit_unmasked_raw.fif'\n",
    "out_file = 'MEWA_msit_raw.fif'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load and prepare digitizations.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load data. Get digitization from raw.\n",
    "raw = Raw(os.path.join(root_dir,'raw',raw_file),preload=False,verbose=False)\n",
    "digitization = raw.info['dig']\n",
    "\n",
    "## The last 101 points are extra. Set them to kind=4.\n",
    "for d in digitization[-101:]: d['kind'] = 4\n",
    "    \n",
    "## Get coordinates for EEG points (excluding ref/EOG).\n",
    "rr = np.array([d['r'] for d in dig if d['kind']==3])[:-2]\n",
    "\n",
    "## Get channels\n",
    "chs = raw.info['chs']\n",
    "\n",
    "## Update location information. This was a huge pain in the ass to figure out.\n",
    "## We ignore the first four channels (Triggers, EOG) and the last channel (STI014).\n",
    "for ch, r in zip(chs[4:-1], rr): ch['loc'][:3] = r \n",
    "\n",
    "## Update digitization/chs.\n",
    "raw.info['dig'] = digitization\n",
    "raw.info['chs'] = chs\n",
    "raw.save(os.path.join(root_dir,'raw',out_file), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing MEWA: Masking channel jumps\n",
    "Time windows were manually inspected. This step isn't strictly necessary but seemed to help with EOG projections.\n",
    "\n",
    "NOTE: Copied over one of the original files for MEWA and renamed it MEWA_msit_unmasked_raw.fif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from mne.io import Raw, RawArray\n",
    "\n",
    "## Specify parameters.\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT_april2016'\n",
    "raw_file = 'MEWA_msit_unmasked_raw.fif'\n",
    "\n",
    "## Load data.\n",
    "raw = Raw(os.path.join(root_dir,'raw',raw_file),preload=True,verbose=False)\n",
    "\n",
    "## Get data in matrix form.\n",
    "data = raw._data\n",
    "\n",
    "## Get list of usuable channels\n",
    "ch_info = [(n,ch) for n,ch in enumerate(raw.ch_names)]\n",
    "good_ch = [(n,ch) for n,ch in ch_info if ch not in raw.info['bads']]\n",
    "good_ch = np.array(good_ch)[4:-1]\n",
    "\n",
    "## Make mask.\n",
    "mask = np.zeros(data.shape[1])\n",
    "times = [(384,394), (663,669)]\n",
    "for t1, t2 in times:\n",
    "    mask[(raw.times >= t1) & (raw.times <= t2)] += 1\n",
    "mask = mask.astype(bool)\n",
    "\n",
    "## Apply mask.\n",
    "for ch in good_ch[:,0].astype(int):\n",
    "    data[ch,mask] = 0\n",
    "\n",
    "## Make new array. Save.\n",
    "raw = RawArray(data, raw.info, first_samp=raw.first_samp)\n",
    "raw.add_eeg_average_proj()\n",
    "raw.save(os.path.join(root_dir,'raw','MEWA_msit_raw.fif'), overwrite=True, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projections: EOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mne import write_proj\n",
    "from mne.preprocessing import compute_proj_eog\n",
    "from mne.io import Raw\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Setup\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## File params.\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT_april2016'\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "subjects = ['MEWA']\n",
    "\n",
    "# NOTE: Not all subjects work with EOG channel = EOG. \n",
    "# Some require other frontal channels due to concatenation.\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "for subj in subjects:    \n",
    "    print 'Making EOG file for %s.' %subj\n",
    "    \n",
    "    ## Load files.\n",
    "    raw_file  = os.path.join( root_dir, 'raw', '%s_msit_raw.fif' %subj )\n",
    "    raw = Raw(raw_file, preload=True, verbose=False, add_eeg_ref=False)\n",
    "    raw.del_proj(0)\n",
    "    ## Make EOG proj. Save.\n",
    "    proj, _ = compute_proj_eog(raw, n_eeg = 4, average=True, filter_length='20s',\n",
    "                               reject=dict(eeg=5e-4), flat=dict(eeg=5e-8),  ch_name='F2', n_jobs=3)\n",
    "    write_proj(os.path.join( root_dir, 'raw', '%s_msit_eog-proj.fif' %subj ), proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projections: ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mne import read_proj, write_proj\n",
    "from mne.preprocessing import compute_proj_ecg\n",
    "from mne.io import Raw\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Setup\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## File params.\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT_april2016'\n",
    "subjects = ['CHDR']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "for subj in subjects:    \n",
    "    print 'Making ECG file for %s.' %subj\n",
    "    \n",
    "    ## Load files.\n",
    "    raw_file = os.path.join( root_dir, 'raw', '%s_msit_raw.fif' %subj )\n",
    "    eog_file = os.path.join( root_dir, 'raw', '%s_msit-proj.fif' %subj )\n",
    "    raw = Raw(raw_file, preload=True, verbose=False)\n",
    "    eog_proj = read_proj(eog_file)\n",
    "    raw.add_proj(eog_proj, remove_existing=True)\n",
    "    raw.apply_proj()\n",
    "    \n",
    "    ## Make ECG proj. Save.\n",
    "    ecg_proj, _ = compute_proj_ecg(raw, n_eeg = 4, h_freq = 35., average=True, filter_length='20s',\n",
    "                                reject=dict(eeg=5e-4), flat=dict(eeg=5e-8), ch_name='P9', n_jobs=3)\n",
    "    proj = eog_proj + [ecg for ecg in ecg_proj if ecg['desc'] not in [eog['desc'] for eog in eog_proj]]\n",
    "    write_proj(os.path.join( root_dir, 'raw', '%s_msit-proj.fif' %subj ), proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing 2: Epoching\n",
    "### Make Forward Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mne import read_trans, read_bem_solution, read_source_spaces\n",
    "from mne import make_forward_solution, write_forward_solution\n",
    "from mne.io import Raw\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "\n",
    "## Main loop.\n",
    "root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT_april2016'\n",
    "fs_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/freesurfs'\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    print 'Making forward solution for %s.' %subject\n",
    "    \n",
    "    ## Load files.\n",
    "    raw = Raw(os.path.join(root_dir, 'raw', '%s_msit_raw.fif' %subject), preload=False, verbose=False)\n",
    "    trans = read_trans(os.path.join(fs_dir,subject,'mri','T1-neuromag','sets','COR-%s.fif' %subject))\n",
    "    src = read_source_spaces(os.path.join(fs_dir,subject,'bem','%s-oct-6p-src.fif' %subject), verbose=False)\n",
    "    bem = read_bem_solution(os.path.join(fs_dir,subject,'bem','%s-5120-5120-5120-bem-sol.fif' %subject), verbose=False)\n",
    "    \n",
    "    ## Compute and save forward solution.\n",
    "    make_forward_solution(raw.info, trans, src, bem, fname=os.path.join(root_dir,'fwd','%s_msit-fwd.fif' %subject),\n",
    "                          meg=False, eeg=True, mindist=1.0, overwrite=True, n_jobs=3, verbose=False)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne import compute_covariance, Epochs, EpochsArray, find_events, read_proj, pick_types, set_log_level\n",
    "from mne.io import Raw\n",
    "from pandas import read_csv\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "\n",
    "## Filtering parameters.\n",
    "l_freq = 0.5\n",
    "h_freq = 50\n",
    "l_trans_bandwidth = l_freq / 2.\n",
    "h_trans_bandwidth = 1.0\n",
    "filter_length = '20s'\n",
    "n_jobs = 3\n",
    "\n",
    "## Epoching parameters.\n",
    "event_id = dict( FN=1, FI=2, NN=3, NI=4 )      # Alik's convention, isn't he smart!?\n",
    "tmin = -1.5                                    # Leave some breathing room.\n",
    "tmax =  3.4                                    # Trial is 1900ms, leave 1500ms of room.\n",
    "resp_buffer = 1.5                              # 1500ms on either side of response.\n",
    "baseline = (-0.5,-0.1)\n",
    "reject_tmin = -0.5\n",
    "reject_tmax = 1.9\n",
    "reject   = dict(eeg=150e-6)\n",
    "flat     = dict(eeg=5e-7)\n",
    "detrend = None\n",
    "decim = 1\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load behavior.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT'\n",
    "data_file = os.path.join( root_dir, 'behavior', 'afMSIT_group_data.csv' )\n",
    "df = read_csv(data_file)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load behavior.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for subj in subjects:\n",
    "    \n",
    "    print 'Loading data for %s.' %subj\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    # Define paths.\n",
    "    raw_file  = os.path.join( root_dir, 'raw', '%s_%s_raw.fif' %(subj,task) )\n",
    "    proj_file = os.path.join( root_dir, 'raw', '%s_%s-proj.fif' %(subj,task) )\n",
    "    \n",
    "    # Load data.\n",
    "    raw = Raw(raw_file,preload=True,verbose=False)\n",
    "    proj = read_proj(proj_file)\n",
    "    \n",
    "    ## Add projections.\n",
    "    proj = [p for p in proj if 'ref' not in p['desc']]\n",
    "    raw.add_proj(proj, remove_existing=True)\n",
    "    raw.add_eeg_average_proj()\n",
    "    raw.apply_proj()\n",
    "    print raw.info['projs']\n",
    "    \n",
    "    ## Reduce dataframe to subject.\n",
    "    data = df[df.Subject==subj]\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Make events.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    print 'Identifying events for %s.' %subj,  \n",
    "    events = find_events(raw, stim_channel='Trig1', output='onset', min_duration=0.25, verbose=False)\n",
    "\n",
    "    # Error catching.\n",
    "    if data.shape[0] != events.shape[0]: raise ValueError('Mismatching number of stimulus onsets!')\n",
    "    print '%s events found.' %events.shape[0]\n",
    "    \n",
    "    # Update event identifiers.\n",
    "    n = 1\n",
    "    for dbs in [0,1]:\n",
    "        for cond in [0,1]:\n",
    "            ix, = np.where((data.DBS==dbs)&(data.Interference==cond))\n",
    "            events[ix,-1] = n\n",
    "            n+=1\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Filter\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    print 'Applying bandpass filter to raw [%s, %s].' %(l_freq, h_freq)\n",
    "    \n",
    "    Fs = raw.info['sfreq']\n",
    "    raw.filter(l_freq = l_freq, h_freq = h_freq, filter_length=filter_length, n_jobs=n_jobs,\n",
    "               l_trans_bandwidth=l_trans_bandwidth, h_trans_bandwidth=h_trans_bandwidth)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Make stimulus-locked epochs.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    # Build initial epochs object.\n",
    "    picks = pick_types(raw.info, meg=False, eeg=True, exclude='bads')\n",
    "    epochs = Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=baseline, picks=picks,\n",
    "                    reject=reject, flat=flat, reject_tmin=reject_tmin, reject_tmax=reject_tmax, \n",
    "                    proj=True, detrend=detrend, decim=decim)\n",
    "    \n",
    "    # First round of rejections.\n",
    "    epochs.drop_bad()                                                       # Remove bad epochs.\n",
    "    copy = data.ix[[True if not log else False for log in epochs.drop_log]] # Update CSV based on rejections.\n",
    "    \n",
    "    '''NOTE: Making a new dataframe copy is just a shortcut for easy indexing between the Pandas \n",
    "       DataFrame and the Epochs object. This is due to the three rounds of rejections being \n",
    "       applied to the data (e.g. amplitude, behavior exclusion, equalization).'''\n",
    "    \n",
    "    # Drop epochs based on behavior.\n",
    "    epochs.drop(copy.Exclude.astype(bool))\n",
    "    \n",
    "    data = data.ix[[True if not log else False for log in epochs.drop_log]]\n",
    "    print '%s trials remain after rejections.' %(len(epochs))\n",
    "    print epochs\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Make Response-locked epochs.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    print 'Making response-locked epochs.'\n",
    "\n",
    "    # Build response-locked events.\n",
    "    response_indices = raw.time_as_index(0.4 + data.RT)             # Compensating for MSIT-lock.\n",
    "    response_events  = epochs.events.copy()\n",
    "    response_events[:,0] = response_events[:,0] + response_indices\n",
    "\n",
    "    # Get data.\n",
    "    arr = epochs.get_data()\n",
    "    times = epochs.times\n",
    "\n",
    "    # Calculate lengths of response-locked epochs.\n",
    "    response_times = data.RT + 0.4                                  # Compensating for MSIT-lock.\n",
    "    response_windows = np.array([response_times-resp_buffer, response_times+resp_buffer]).T\n",
    "\n",
    "    # Iteratively build epochs array.\n",
    "    trials = []\n",
    "    for n in xrange(len(epochs)):\n",
    "        mask = (times >= response_windows[n,0]) & (times <= response_windows[n,1])\n",
    "        trials.append( arr[n,:,mask] )\n",
    "    trials = np.array(trials).swapaxes(1,2)\n",
    "\n",
    "    # Finally, make epochs objects.\n",
    "    resp_epochs = EpochsArray(trials, epochs.info, response_events, tmin=-resp_buffer, event_id=event_id,)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Save data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    print 'Saving epoch files.'\n",
    "    epochs.save(os.path.join(root_dir,'ave','%s_%s_%s_stim-epo.fif' %(subj,task,h_freq)))\n",
    "    resp_epochs.save(os.path.join(root_dir,'ave','%s_%s_%s_resp-epo.fif' %(subj,task,h_freq)))\n",
    "    data.to_csv(os.path.join(root_dir,'ave','%s_%s_%s-epo.csv' %(subj,task,h_freq)), index=False)\n",
    "    \n",
    "    print '\\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\\n'\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Covariance Matrices / Inverse Solutions / Morph Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mne import EpochsArray, read_epochs, read_forward_solution, set_log_level\n",
    "from mne import compute_covariance, write_cov\n",
    "from mne import compute_morph_matrix, read_source_spaces\n",
    "from mne.filter import low_pass_filter\n",
    "from mne.minimum_norm import make_inverse_operator, write_inverse_operator\n",
    "from scipy.io import savemat\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "\n",
    "## Analysis parameters.\n",
    "fmax = 50\n",
    "\n",
    "## Source localization parameters.\n",
    "loose = 0.2\n",
    "depth = 0.8\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Iteratively load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT'\n",
    "fs_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/freesurfs'\n",
    "src = read_source_spaces(os.path.join(fs_dir,'fscopy','bem','fscopy-oct-6p-src.fif'))\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    print 'Processing %s' %subject\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load files.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Load in files.\n",
    "    epo_file = os.path.join(root_dir,'ave','%s_msit_%s_stim-epo.fif' %(subject,fmax))\n",
    "    epochs = read_epochs(epo_file, verbose=False)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Secondary objects.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    fwd = read_forward_solution(os.path.join(root_dir, 'fwd', '%s_%s-fwd.fif' %(subject,task)), \n",
    "                                surf_ori=True, verbose=False)\n",
    "    \n",
    "    ## Compute/save noise covariance matrix & inverse operator.\n",
    "    noise_cov = compute_covariance(epochs, tmin=-0.5, tmax=0.0, method='shrunk', n_jobs=1)\n",
    "    write_cov(os.path.join(root_dir,'cov','%s_%s_%s-cov.fif' %(subject,task,h_freq)), noise_cov)\n",
    "    inv = make_inverse_operator(epochs.info, fwd, noise_cov, loose=loose, depth=depth, verbose=False)\n",
    "    write_inverse_operator(os.path.join(root_dir,'cov','%s_%s_%s-inv.fif' %(subject,task,fmax)), inv)\n",
    "\n",
    "    ## Pre-compute morph matrix.\n",
    "    vertices_from = [inv['src'][n]['vertno'] for n in xrange(2)]\n",
    "    vertices_to = [src[n]['vertno'] for n in xrange(2)]\n",
    "    morph_mat = compute_morph_matrix(subject, 'fsaverage', vertices_from=vertices_from, \n",
    "                                     vertices_to=vertices_to,subjects_dir=fs_dir, smooth=25)\n",
    "    savemat(os.path.join(root_dir, 'morph_maps', '%s-fsaverage_morph.mat' %subject),\n",
    "            mdict=dict(morph_mat=morph_mat))\n",
    "    \n",
    "print 'Done.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "279px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
