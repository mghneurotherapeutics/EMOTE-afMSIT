{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Primary Analyses\n",
    "## Sensor Space Analysis\n",
    "### Prepare time domain epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne import read_epochs\n",
    "from mne.filter import low_pass_filter\n",
    "from pandas import read_csv, concat\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "event_id = ['FN', 'FI', 'NN', 'NI']\n",
    "analysis = 'resp'\n",
    "h_freq = 50\n",
    "\n",
    "## Channel definitions. \n",
    "pick_channels = ['FZ']\n",
    "\n",
    "## Processing parameters.\n",
    "fmax = 15\n",
    "decim = 3\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Iteratively load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for n, subject in enumerate(subjects):\n",
    "    \n",
    "    ## Load behavioral data.\n",
    "    csv = read_csv(os.path.join('ave','%s_%s_%s-epo.csv' %(subject,task,h_freq)))\n",
    "\n",
    "    ## Load in epochs.\n",
    "    epo_file = os.path.join('ave','%s_%s_%s_%s-epo.fif' %(subject,task,h_freq,analysis))\n",
    "    epochs = read_epochs(epo_file, verbose=False)\n",
    "    \n",
    "    ## Restrict to times/channels of interest.\n",
    "    if analysis == 'stim': epochs.crop(-0.5,2.0)\n",
    "    elif analysis == 'resp': epochs.crop(-1.0,1.0)\n",
    "    epochs.pick_channels(pick_channels)\n",
    "\n",
    "    ## Extract epochs.\n",
    "    Fs = epochs.info['sfreq']\n",
    "    times = epochs.times\n",
    "    epochs = epochs.get_data().swapaxes(0,1)\n",
    "    \n",
    "    ## Merge.\n",
    "    if not n: \n",
    "        trials = epochs\n",
    "        df = csv\n",
    "    else:\n",
    "        trials = np.concatenate([trials,epochs], axis=1)\n",
    "        df = concat([df,csv])\n",
    "        \n",
    "## Apply lowpass filter. \n",
    "trials = low_pass_filter(trials, Fs, fmax, filter_length='20s', n_jobs=3, verbose=False)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "## Downsample.\n",
    "trials = trials[:,:,::decim]\n",
    "times = times[::decim]\n",
    "\n",
    "## Convert to uV.\n",
    "trials *= 1e6\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Make directories (if not already made). Save.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Make directory.\n",
    "out_dir = 'sensor'\n",
    "if not os.path.isdir(out_dir): os.makedirs(out_dir)\n",
    "    \n",
    "## Save data.\n",
    "for arr, ch in zip(trials,pick_channels): np.savez_compressed(os.path.join(out_dir, 'afMSIT_sensor_%s_%s_%s' %(analysis,ch,fmax)),\n",
    "                                                              data=arr, times=times, fmax=fmax)\n",
    "df.to_csv(os.path.join(out_dir, 'afMSIT_sensor_info.csv'), index=False)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare power amplitudes for single-trial epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne import read_epochs\n",
    "from mne.filter import low_pass_filter\n",
    "from mne.time_frequency import single_trial_power\n",
    "from pandas import read_csv\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "event_id = ['FN', 'FI', 'NN', 'NI']\n",
    "h_freq = 50\n",
    "\n",
    "## Channel definitions. \n",
    "pick_channels = ['FZ']\n",
    "\n",
    "## Time-frequency parameters.\n",
    "fdict = dict(theta = (4,8), alpha = (8,15), beta = (15,30))\n",
    "freqs = np.logspace( np.log10(2), np.log10(50), num=25)\n",
    "n_cycles = 3\n",
    "baseline = (-0.5, -0.1)\n",
    "Fs = 1450.\n",
    "decim = 14\n",
    "n_jobs = 3\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "epo_dir = 'ave'\n",
    "root_dir = 'sensor'\n",
    "df = read_csv(os.path.join(root_dir, 'afMSIT_sensor_info.csv'))\n",
    "\n",
    "for ch in pick_channels:\n",
    "\n",
    "    for analysis in ['stim', 'resp']:\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Iteratively load and merge.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        trials = []\n",
    "        for subject in subjects:\n",
    "\n",
    "            ## Load in epochs.\n",
    "            epo_file = os.path.join(epo_dir, '%s_%s_%s_%s-epo.fif' %(subject,task,h_freq,analysis))\n",
    "            epochs = read_epochs(epo_file, verbose=False)\n",
    "\n",
    "            ## Get label index and extract.\n",
    "            epochs.pick_channels([ch])\n",
    "            trials.append(epochs._data.squeeze())\n",
    "\n",
    "        ## Concatenate.\n",
    "        trials = np.concatenate(trials, axis=0)\n",
    "        times = epochs.times\n",
    "\n",
    "        if not df.shape[0] == trials.shape[0]: raise ValueError('Incompatible dimensions!')\n",
    "        \n",
    "         #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Phase-lock removal.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "       \n",
    "        for subject in df.Subject.unique():\n",
    "            for dbs in [0, 1]:\n",
    "                for cond in [0, 1]:\n",
    "                    ix, = np.where((df.Subject==subject)&(df.DBS==dbs)&(df.Interference==cond))\n",
    "                    trials[ix] -= trials[ix].mean(axis=0)\n",
    "                    \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute power.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "                    \n",
    "        print 'Computing single trial power: %s %s.' %(ch, analysis)\n",
    "        trials = np.expand_dims(trials,1)\n",
    "        power = single_trial_power(trials, sfreq=Fs, frequencies=freqs, n_cycles=n_cycles, \n",
    "                                    baseline=None, n_jobs=n_jobs, verbose=False)\n",
    "        power = power.squeeze()\n",
    "        del trials\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Within-trial normalization.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        power = (power.T / np.median(power, axis=-1).T).T # median, not mean\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Prepare baseline normalization (within subject, DBS).\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        ## Compute baseline normalization.\n",
    "        if analysis == 'stim':\n",
    "\n",
    "            ## Make time mask.\n",
    "            mask = (times >= baseline[0]) & (times <= baseline[1])\n",
    "            \n",
    "            ## Iteratively compute over subjects.\n",
    "            blnorm = []\n",
    "            for subject in df.Subject.unique():\n",
    "                \n",
    "                ## Iteratively compute over DBS conditions..\n",
    "                sbl = []\n",
    "                for dbs in [0,1]:\n",
    "\n",
    "                    ix, = np.where((df.Subject==subject)&(df.DBS==dbs))\n",
    "                    sbl.append( np.apply_over_axes(np.median, power[ix][:,:,mask], axes=[0,2]).squeeze() )\n",
    "            \n",
    "                blnorm.append(sbl)\n",
    "            \n",
    "            ## Merge.\n",
    "            blnorm = np.array(blnorm)\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Apply baseline normalization (within subject, DBS).\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        ## Setup index vectors.\n",
    "        n_trials, n_freqs, n_times = power.shape        \n",
    "        _, subj_ix = np.unique(df.Subject, return_inverse=True)\n",
    "        dbs_ix = df.DBS.as_matrix()\n",
    "        trial_ix = np.arange(n_trials) \n",
    "        \n",
    "        ## Main loop.\n",
    "        for i,j,k in zip(trial_ix,subj_ix,dbs_ix):\n",
    "            \n",
    "            for m in xrange(n_times):\n",
    "                \n",
    "                power[i,:,m] /= blnorm[j,k,:]\n",
    "                \n",
    "        if analysis == 'resp': del blnorm\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Final preprocessing steps.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        power = np.log10(power) * 10\n",
    "            \n",
    "        ## Crop times.\n",
    "        if analysis == 'stim': mask = (times >= -0.5) & (times <= 2.0)\n",
    "        elif analysis == 'resp': mask = (times >= -1.0) & (times <= 1.0)\n",
    "        power = power[:, :, mask]\n",
    "        times = times[mask]\n",
    "            \n",
    "        ## Iteratively average, transform, and save.\n",
    "        for k,v in fdict.iteritems():\n",
    "            \n",
    "            ## Average across frequencies.\n",
    "            ix, = np.where((freqs>=v[0])&(freqs<=v[1]))\n",
    "            band = power[:,ix,:].mean(axis=1)\n",
    "        \n",
    "            ## Save.\n",
    "            f = os.path.join(root_dir, 'afMSIT_sensor_%s_%s_%s' %(analysis, ch, k))\n",
    "            np.savez_compressed(f, data=band[:,::decim], times=times[::decim], freqs=freqs, n_cycles=n_cycles)\n",
    "        \n",
    "        del power\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform permutatations\n",
    "Please see: scripts/cmdline/afMSIT_permutations.py\n",
    "\n",
    "### Identify clusters, perform FDR corrections, and assemble results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from mne.stats import fdr_correction\n",
    "from pandas import DataFrame, concat\n",
    "from scipy.ndimage import measurements\n",
    "from scipy.stats import norm\n",
    "\n",
    "def largest_cluster(arr, threshold):\n",
    "    masked = np.abs( arr ) > threshold\n",
    "    clusters, n_clusters = measurements.label(masked)\n",
    "    cluster_sums = measurements.sum(arr, clusters, index=np.arange(n_clusters)+1)\n",
    "    if not len(cluster_sums): return 0\n",
    "    else: return np.abs(cluster_sums).max()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Specify parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## File parameters.\n",
    "labels = ['FZ', 'FCZ']\n",
    "analysis = 'resp'\n",
    "model_name = 'revised'\n",
    "freqs = ['theta','alpha','beta']\n",
    "domain = ['timedomain', 'frequency'][1]\n",
    "\n",
    "## Statistics parameters.\n",
    "alpha = 0.05\n",
    "min_cluster = 0.05 # seconds\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Initial preparations.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define contrasts.\n",
    "if model_name == 'revised': cols = ['Intercept', 'DBS', 'Interference', 'DBSxInt', 'nsArousal', 'nsValence', 'Trial']\n",
    "\n",
    "## Define threshold.\n",
    "threshold = norm.ppf(1 - alpha/2.)\n",
    "\n",
    "## Read in seeds.\n",
    "f = 'scripts/cmdline/seeds.txt'\n",
    "with open(f, 'r') as f: seeds = [s.strip() for s in f.readlines()]\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#    \n",
    "\n",
    "df = []\n",
    "    \n",
    "for label in labels:\n",
    "    \n",
    "    for freq in freqs:\n",
    "    \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Load files. Assemble permutations.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        root_dir = 'sensor'\n",
    "        results_dir = os.path.join(root_dir, model_name)\n",
    "        out_dir = os.path.join(root_dir, 'results')\n",
    "\n",
    "        ## Load true statistics.\n",
    "        npz = np.load(os.path.join(results_dir, 'afMSIT_sensor_%s_%s_%s_obs.npz' %(analysis, label, freq)))\n",
    "        t_scores = npz['t_scores'].squeeze()\n",
    "        times = npz['times'].squeeze()\n",
    "\n",
    "        ## Load null statistics.\n",
    "        for n, seed in enumerate(seeds):\n",
    "            npz = np.load(os.path.join(results_dir, 'afMSIT_sensor_%s_%s_%s_%s.npz' %(analysis, label, freq, seed)))\n",
    "            if not n: permuted = npz['t_scores']\n",
    "            else: permuted = np.concatenate([permuted, npz['t_scores']], axis=0)\n",
    "                \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute cluster statistics.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ## Get info.\n",
    "        n_shuffles, n_eff, n_times  = permuted.shape\n",
    "\n",
    "        ## Iteratively compute clusters.\n",
    "        results = defaultdict(list)\n",
    "\n",
    "        for n, con in enumerate(cols):\n",
    "\n",
    "            ## Find real clusters.\n",
    "            masked = np.abs( t_scores[n] ) > threshold\n",
    "            clusters, n_clusters = measurements.label(masked)\n",
    "            cluster_sums = measurements.sum(t_scores[n], clusters, index=np.arange(n_clusters)+1)\n",
    "\n",
    "            ## Compute null cluster sums.\n",
    "            null_sums = np.array([largest_cluster(permuted[m, n, :], threshold) for m in xrange(n_shuffles)])\n",
    "\n",
    "            ## Compute cluster bounds.\n",
    "            tmin = np.array([times[clusters==i].min() for i in np.arange(n_clusters)+1])\n",
    "            tmax = np.array([times[clusters==i].max() for i in np.arange(n_clusters)+1])\n",
    "\n",
    "            ## Find proportion of clusters that are larger.\n",
    "            p_values = [(np.abs(cs) < null_sums).mean() for cs in cluster_sums]\n",
    "\n",
    "            ## Store results.\n",
    "            for t1, t2, cs, pval in zip(tmin,tmax,cluster_sums,p_values):\n",
    "                results['Freq'] += [freq]\n",
    "                results['Label'] += [label]\n",
    "                results['Contrast'] += [con]\n",
    "                results['Tmin'] += [t1]\n",
    "                results['Tmax'] += [t2]\n",
    "                results['Score'] += [cs]\n",
    "                results['Pval'] += [pval]\n",
    "\n",
    "        ## Organize results and append.\n",
    "        results = DataFrame(results)\n",
    "        results['Tdiff'] = results['Tmax'] - results['Tmin']\n",
    "        results = results[results['Tdiff']>=min_cluster]\n",
    "        df.append(results)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute cluster statistics.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Merge dataframes.\n",
    "df = concat(df)\n",
    "\n",
    "## Organize columns and sort.\n",
    "cols = ['Contrast','Label','Freq','Tmin','Tmax','Tdiff','Score','Pval']\n",
    "df = df[cols].sort_values(['Contrast','Tmin']).reset_index(drop=True)\n",
    "\n",
    "## FDR correct within contrasts.\n",
    "df['FDR'] = 0\n",
    "for contrast in df.Contrast.unique():\n",
    "    _, fdr = fdr_correction(df.loc[df.Contrast==contrast,'Pval'], alpha)\n",
    "    df.loc[df.Contrast==contrast,'FDR'] = fdr\n",
    "    \n",
    "## Save.\n",
    "f = os.path.join(out_dir, '%s_%s_%s_results.csv' %(model_name, analysis,domain))\n",
    "df.to_csv(f, index=False)\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Space Analysis\n",
    "### Source localize single trial epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from mne import read_epochs, read_label, read_source_spaces, set_log_level\n",
    "from mne.minimum_norm import apply_inverse_epochs, read_inverse_operator\n",
    "from scipy.io import loadmat\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "analysis = 'resp'\n",
    "parc = 'april2016'\n",
    "fmax = 50\n",
    "\n",
    "## Source localization parameters.\n",
    "method = 'dSPM'\n",
    "snr = 1.0  \n",
    "lambda2 = 1.0 / snr ** 2\n",
    "pick_ori = 'normal'\n",
    "\n",
    "## Labels\n",
    "rois = ['dacc-lh', 'dacc-rh', 'dmpfc-lh', 'dmpfc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "        'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "        'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Iteratively load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT'\n",
    "fs_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/freesurfs'\n",
    "\n",
    "## Prepare fsaverage source space.\n",
    "src = read_source_spaces(os.path.join(fs_dir,'fscopy','bem','fscopy-oct-6p-src.fif'))\n",
    "vertices_to = [src[n]['vertno'] for n in xrange(2)]\n",
    "labels = [read_label(os.path.join(fs_dir,'fscopy','label','april2016','%s.label' %roi), subject='fsaverage')\n",
    "          for roi in rois]\n",
    "\n",
    "for subject in subjects:\n",
    "\n",
    "    print 'Performing source localization: %s' %subject\n",
    "\n",
    "    ## Load in epochs.\n",
    "    epochs = read_epochs(os.path.join(root_dir,'ave','%s_%s_%s_%s-epo.fif' %(subject,task,fmax,analysis)))\n",
    "    times = epochs.times\n",
    "    \n",
    "    ## Load in secondary files.\n",
    "    inv = read_inverse_operator(os.path.join(root_dir,'cov','%s_%s_%s-inv.fif' %(subject,task,fmax)))\n",
    "    morph_mat = loadmat(os.path.join(root_dir, 'morph_maps', '%s-fsaverage_morph.mat' %subject))['morph_mat']\n",
    "\n",
    "    ## Make generator object.\n",
    "    G = apply_inverse_epochs(epochs, inv, method=method, lambda2=lambda2, pick_ori=pick_ori, return_generator=True)\n",
    "    del epochs, inv\n",
    "\n",
    "    ## Iteratively compute and store label timecourse. \n",
    "    ltcs = []\n",
    "    for g in G:\n",
    "        g = g.morph_precomputed('fsaverage', vertices_to=vertices_to, morph_mat=morph_mat)\n",
    "        ltcs.append( g.extract_label_time_course(labels, src, mode='pca_flip') )\n",
    "    ltcs = np.array(ltcs)\n",
    "    \n",
    "    ## Save.\n",
    "    f = os.path.join(root_dir,'source','stcs','%s_%s_%s_%s_%s_epochs' %(subject,task,analysis,method,fmax))\n",
    "    np.savez_compressed(f, ltcs=ltcs, times=times, labels=np.array([l.name for l in labels]))\n",
    "    del ltcs\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reassemble source localized epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne.filter import low_pass_filter\n",
    "from mne.time_frequency import single_trial_power\n",
    "from pandas import read_csv\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Data parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "method = 'dSPM'\n",
    "h_freq = 50\n",
    "\n",
    "## Label parameters.\n",
    "rois = ['dacc-lh', 'dacc-rh', 'dmpfc-lh', 'dmpfc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "        'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "        'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "\n",
    "## Processing parameters.\n",
    "fmax = 15\n",
    "sfreq = 1450.\n",
    "decim = 3\n",
    "n_jobs = 3\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/source'\n",
    "df = read_csv(os.path.join(root_dir, 'afMSIT_source_info.csv'))\n",
    "\n",
    "for analysis in ['stim', 'resp']:\n",
    "\n",
    "    for roi in rois:\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Iteratively load and merge.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ltcs = []\n",
    "        for subject in subjects:\n",
    "\n",
    "            ## Load NPZ.\n",
    "            npz = np.load(os.path.join(root_dir, 'stcs', '%s_msit_%s_%s_%s_epochs.npz' %(subject,analysis,method,h_freq)))\n",
    "\n",
    "            ## Get label index and extract.\n",
    "            ix = npz['labels'].tolist().index(roi)\n",
    "            arr = npz['ltcs'][:,ix,:]\n",
    "\n",
    "            ## Append.\n",
    "            ltcs.append(arr)\n",
    "\n",
    "        ## Concatenate.\n",
    "        ltcs = np.concatenate(ltcs, axis=0)\n",
    "        times = npz['times']\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Time-domain processing.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ## Make ERP objects.\n",
    "        ERPs = low_pass_filter(ltcs, sfreq, fmax, filter_length='20s', n_jobs=n_jobs)\n",
    "\n",
    "        ## Crop times.\n",
    "        if analysis == 'stim': mask = (times >= -0.5) & (times <= 2.0)\n",
    "        elif analysis == 'resp': mask = (times >= -1.0) & (times <= 1.0)\n",
    "        ERPs = ERPs[:, mask]\n",
    "        times = times[mask]\n",
    "        \n",
    "        ## Save.\n",
    "        np.savez_compressed(os.path.join(root_dir, 'afMSIT_source_%s_%s_%s' %(analysis,roi,fmax)), \n",
    "                            data=ERPs[:,::decim], times=times[::decim], method=method)\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute power of source localized epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne.filter import low_pass_filter\n",
    "from mne.time_frequency import single_trial_power\n",
    "from pandas import read_csv\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Data parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "method = 'dSPM'\n",
    "h_freq = 50\n",
    "\n",
    "## Label parameters.\n",
    "rois = ['dacc-lh', 'dacc-rh', 'dmpfc-lh', 'dmpfc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "        'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "        'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "\n",
    "## Time-frequency parameters.\n",
    "fdict = dict(theta = (4,8), alpha = (8,15), beta = (15,30))\n",
    "freqs = np.logspace( np.log10(2), np.log10(50), num=25)\n",
    "n_cycles = 3\n",
    "baseline = (-0.5, -0.1)\n",
    "Fs = 1450.\n",
    "decim = 14\n",
    "n_jobs = 3\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/source'\n",
    "df = read_csv(os.path.join(root_dir, 'afMSIT_source_info.csv'))\n",
    "\n",
    "for roi in rois:\n",
    "\n",
    "    for analysis in ['stim', 'resp']:\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Iteratively load and merge.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ltcs = []\n",
    "        for subject in subjects:\n",
    "\n",
    "            ## Load NPZ.\n",
    "            npz = np.load(os.path.join(root_dir, 'stcs', '%s_msit_%s_%s_%s_epochs.npz' %(subject,analysis,method,h_freq)))\n",
    "\n",
    "            ## Get label index and extract.\n",
    "            ix = npz['labels'].tolist().index(roi)\n",
    "            arr = npz['ltcs'][:,ix,:]\n",
    "\n",
    "            ## Append.\n",
    "            ltcs.append(arr)\n",
    "\n",
    "        ## Concatenate.\n",
    "        ltcs = np.concatenate(ltcs, axis=0)\n",
    "        times = npz['times']\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Phase-lock removal.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "       \n",
    "        for subject in df.Subject.unique():\n",
    "            for dbs in [0, 1]:\n",
    "                for cond in [0, 1]:\n",
    "                    ix, = np.where((df.Subject==subject)&(df.DBS==dbs)&(df.Interference==cond))\n",
    "                    ltcs[ix] -= ltcs[ix].mean(axis=0)\n",
    "                    \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute power.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "                    \n",
    "        print 'Computing single trial power: %s %s.' %(roi, analysis)\n",
    "        ltcs = np.expand_dims(ltcs,1)\n",
    "        power = single_trial_power(ltcs, sfreq=Fs, frequencies=freqs, n_cycles=n_cycles, \n",
    "                                    baseline=None, n_jobs=n_jobs, verbose=False)\n",
    "        power = power.squeeze()\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Within-trial normalization.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        power = (power.T / np.median(power, axis=-1).T).T # median, not mean\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Prepare baseline normalization (within subject, DBS).\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        ## Compute baseline normalization.\n",
    "        if analysis == 'stim':\n",
    "\n",
    "            ## Make time mask.\n",
    "            mask = (times >= baseline[0]) & (times <= baseline[1])\n",
    "            \n",
    "            ## Iteratively compute over subjects.\n",
    "            blnorm = []\n",
    "            for subject in df.Subject.unique():\n",
    "                \n",
    "                ## Iteratively compute over DBS conditions..\n",
    "                sbl = []\n",
    "                for dbs in [0,1]:\n",
    "\n",
    "                    ix, = np.where((df.Subject==subject)&(df.DBS==dbs))\n",
    "                    sbl.append( np.apply_over_axes(np.median, power[ix][:,:,mask], axes=[0,2]).squeeze() )\n",
    "            \n",
    "                blnorm.append(sbl)\n",
    "            \n",
    "            ## Merge.\n",
    "            blnorm = np.array(blnorm)\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Apply baseline normalization (within subject, DBS).\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        ## Setup index vectors.\n",
    "        n_trials, n_freqs, n_times = power.shape        \n",
    "        _, subj_ix = np.unique(df.Subject, return_inverse=True)\n",
    "        dbs_ix = df.DBS.as_matrix()\n",
    "        trial_ix = np.arange(n_trials) \n",
    "        \n",
    "        ## Main loop.\n",
    "        for i,j,k in zip(trial_ix,subj_ix,dbs_ix):\n",
    "            \n",
    "            for m in xrange(n_times):\n",
    "                \n",
    "                power[i,:,m] /= blnorm[j,k,:]\n",
    "                \n",
    "        if analysis == 'resp': del blnorm\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Final preprocessing steps.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            \n",
    "        ## Convert to decibels.\n",
    "        power = np.log10(power) * 10\n",
    "            \n",
    "        ## Crop times.\n",
    "        if analysis == 'stim': mask = (times >= -0.5) & (times <= 2.0)\n",
    "        elif analysis == 'resp': mask = (times >= -1.0) & (times <= 1.0)\n",
    "        power = power[:, :, mask]\n",
    "        times = times[mask]\n",
    "            \n",
    "        ## Iteratively average, transform, and save.\n",
    "        for k,v in fdict.iteritems():\n",
    "            \n",
    "            ## Average across frequencies.\n",
    "            ix, = np.where((freqs>=v[0])&(freqs<=v[1]))\n",
    "            band = power[:,ix,:].mean(axis=1)\n",
    "        \n",
    "            ## Save.\n",
    "            f = os.path.join(root_dir, 'afMSIT_source_%s_%s_%s' %(analysis, roi, k))\n",
    "            np.savez_compressed(f, data=band[:,::decim], times=times[::decim], freqs=freqs, n_cycles=n_cycles)\n",
    "        \n",
    "        del power\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform permutatations\n",
    "Please see: /space/sophia/2/users/EMOTE-DBS/afMSIT/scripts/cmdline/afMSIT_permutations.py\n",
    "\n",
    "### Identify clusters, perform FDR corrections, and assemble results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from mne.stats import fdr_correction\n",
    "from pandas import DataFrame, concat\n",
    "from scipy.ndimage import measurements\n",
    "from scipy.stats import norm\n",
    "\n",
    "def largest_cluster(arr, threshold):\n",
    "    masked = np.abs( arr ) > threshold\n",
    "    clusters, n_clusters = measurements.label(masked)\n",
    "    cluster_sums = measurements.sum(arr, clusters, index=np.arange(n_clusters)+1)\n",
    "    if not len(cluster_sums): return 0\n",
    "    else: return np.abs(cluster_sums).max()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Specify parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## File parameters.\n",
    "labels = ['dacc-lh', 'dacc-rh', 'dmpfc-lh', 'dmpfc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "          'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "          'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "\n",
    "analysis = 'resp'\n",
    "model_name = 'revised'\n",
    "freqs = ['theta', 'alpha', 'beta']\n",
    "domain = ['timedomain', 'frequency'][1]\n",
    "\n",
    "## Statistics parameters.\n",
    "alpha = 0.05\n",
    "min_cluster = 0.05 # seconds\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Initial preparations.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define contrasts.\n",
    "if model_name == 'revised': cols = ['Intercept', 'DBS', 'Interference', 'DBSxInt', 'nsArousal', 'nsValence', 'Trial']\n",
    "\n",
    "## Define threshold.\n",
    "threshold = norm.ppf(1 - alpha/2.)\n",
    "\n",
    "## Read in seeds.\n",
    "f = '/space/sophia/2/users/EMOTE-DBS/afMSIT/scripts/cmdline/seeds.txt'\n",
    "with open(f, 'r') as f: seeds = [s.strip() for s in f.readlines()]\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#    \n",
    "\n",
    "df = []\n",
    "    \n",
    "for label in labels:\n",
    "    \n",
    "    for freq in freqs:\n",
    "    \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Load files. Assemble permutations.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT/source'\n",
    "        results_dir = os.path.join(root_dir, model_name)\n",
    "        out_dir = os.path.join(root_dir, 'results')\n",
    "\n",
    "        ## Load true statistics.\n",
    "        npz = np.load(os.path.join(results_dir, 'afMSIT_source_%s_%s_%s_obs.npz' %(analysis, label, freq)))\n",
    "        t_scores = npz['t_scores'].squeeze()\n",
    "        times = npz['times'].squeeze()\n",
    "\n",
    "        ## Load null statistics.\n",
    "        for n, seed in enumerate(seeds):\n",
    "            npz = np.load(os.path.join(results_dir, 'afMSIT_source_%s_%s_%s_%s.npz' %(analysis, label, freq, seed)))\n",
    "            if not n: permuted = npz['t_scores']\n",
    "            else: permuted = np.concatenate([permuted, npz['t_scores']], axis=0)\n",
    "                \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute cluster statistics.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ## Get info.\n",
    "        n_shuffles, n_eff, n_times  = permuted.shape\n",
    "\n",
    "        ## Iteratively compute clusters.\n",
    "        results = defaultdict(list)\n",
    "\n",
    "        for n, con in enumerate(cols):\n",
    "\n",
    "            ## Find real clusters.\n",
    "            masked = np.abs( t_scores[n] ) > threshold\n",
    "            clusters, n_clusters = measurements.label(masked)\n",
    "            cluster_sums = measurements.sum(t_scores[n], clusters, index=np.arange(n_clusters)+1)\n",
    "\n",
    "            ## Compute null cluster sums.\n",
    "            null_sums = np.array([largest_cluster(permuted[m, n, :], threshold) for m in xrange(n_shuffles)])\n",
    "\n",
    "            ## Compute cluster bounds.\n",
    "            tmin = np.array([times[clusters==i].min() for i in np.arange(n_clusters)+1])\n",
    "            tmax = np.array([times[clusters==i].max() for i in np.arange(n_clusters)+1])\n",
    "\n",
    "            ## Find proportion of clusters that are larger.\n",
    "            p_values = [((np.abs(cs) < null_sums).sum() + 1.) / (null_sums.shape[0] + 1.) for cs in cluster_sums]\n",
    "            \n",
    "            ## Store results.\n",
    "            for t1, t2, cs, pval in zip(tmin,tmax,cluster_sums,p_values):\n",
    "                results['Freq'] += [freq]\n",
    "                results['Label'] += [label]\n",
    "                results['Contrast'] += [con]\n",
    "                results['Tmin'] += [t1]\n",
    "                results['Tmax'] += [t2]\n",
    "                results['Score'] += [cs]\n",
    "                results['Pval'] += [pval]\n",
    "\n",
    "        ## Organize results and append.\n",
    "        results = DataFrame(results)\n",
    "        results['Tdiff'] = results['Tmax'] - results['Tmin']\n",
    "        results = results[results['Tdiff']>=min_cluster]\n",
    "        df.append(results)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute cluster statistics.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Merge dataframes.\n",
    "df = concat(df)\n",
    "\n",
    "## Organize columns and sort.\n",
    "cols = ['Contrast','Label','Freq','Tmin','Tmax','Tdiff','Score','Pval']\n",
    "df = df[cols].sort_values(['Contrast','Tmin']).reset_index(drop=True)\n",
    "\n",
    "## FDR correct within contrasts.\n",
    "df['FDR'] = 0\n",
    "for contrast in df.Contrast.unique():\n",
    "    _, fdr = fdr_correction(df.loc[df.Contrast==contrast,'Pval'], alpha)\n",
    "    df.loc[df.Contrast==contrast,'FDR'] = fdr\n",
    "    \n",
    "## Save.\n",
    "f = os.path.join(out_dir, '%s_%s_%s_results.csv' %(model_name, analysis,domain))\n",
    "df.to_csv(f, index=False)\n",
    "        \n",
    "print 'Done.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "243px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
