{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Secondary Analyses\n",
    "## Individual Regressions on DLPFC-5 LH \n",
    "Here we look for the fraction of subjects that show the same DBSon/DBSoff effects in DLPFC-5 LH at the single-subject level.\n",
    "### Perform permutations\n",
    "Please see: /space/sophia/2/users/EMOTE-DBS/afMSIT/scripts/cmdline/subject_permutations.py\n",
    "### Identify clusters, perform FDR corrections, and assemble results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from mne.stats import fdr_correction\n",
    "from pandas import DataFrame, concat\n",
    "from scipy.ndimage import measurements\n",
    "from scipy.stats import norm\n",
    "\n",
    "def largest_cluster(arr, threshold):\n",
    "    masked = np.abs( arr ) > threshold\n",
    "    clusters, n_clusters = measurements.label(masked)\n",
    "    cluster_sums = measurements.sum(arr, clusters, index=np.arange(n_clusters)+1)\n",
    "    if not len(cluster_sums): return 0\n",
    "    else: return np.abs(cluster_sums).max()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Specify parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## File parameters.\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "labels = ['dlpfc_5-lh']\n",
    "\n",
    "space = 'source'\n",
    "analysis = 'stim'\n",
    "model_name = 'revised'\n",
    "freqs = ['theta']\n",
    "domain = ['timedomain', 'frequency'][1]\n",
    "\n",
    "## Statistics parameters.\n",
    "alpha = 0.05\n",
    "min_cluster = 0.05 # seconds\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Initial preparations.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define contrasts.\n",
    "if model_name == 'revised': cols = ['Intercept', 'DBS', 'Interference', 'DBSxInt', 'nsArousal', 'nsValence', 'Trial']\n",
    "\n",
    "## Define threshold.\n",
    "threshold = norm.ppf(1 - alpha/2.)\n",
    "\n",
    "## Read in seeds.\n",
    "f = '/space/sophia/2/users/EMOTE-DBS/afMSIT/scripts/cmdline/seeds.txt'\n",
    "with open(f, 'r') as f: seeds = [s.strip() for s in f.readlines()]\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#    \n",
    "\n",
    "df = []\n",
    "    \n",
    "for subject in subjects: \n",
    "        \n",
    "    for label in labels:\n",
    "\n",
    "        for freq in freqs:\n",
    "\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            ### Load files. Assemble permutations.\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT/%s' %space\n",
    "            results_dir = os.path.join(root_dir, model_name)\n",
    "            out_dir = os.path.join(root_dir, 'results')\n",
    "\n",
    "            ## Load true statistics.\n",
    "            npz = np.load(os.path.join(results_dir, '%s_%s_%s_%s_%s_obs.npz' %(subject, space, analysis, label, freq)))\n",
    "            t_scores = npz['t_scores'].squeeze()\n",
    "            times = npz['times'].squeeze()\n",
    "\n",
    "            ## Load null statistics.\n",
    "            for n, seed in enumerate(seeds):\n",
    "                npz = np.load(os.path.join(results_dir, '%s_%s_%s_%s_%s_%s.npz' %(subject, space, analysis, label, freq, seed)))\n",
    "                if not n: permuted = npz['t_scores']\n",
    "                else: permuted = np.concatenate([permuted, npz['t_scores']], axis=0)\n",
    "\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            ### Compute cluster statistics.\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "            ## Get info.\n",
    "            n_shuffles, n_eff, n_times  = permuted.shape\n",
    "\n",
    "            ## Iteratively compute clusters.\n",
    "            results = defaultdict(list)\n",
    "\n",
    "            for n, con in enumerate(cols):\n",
    "\n",
    "                ## Find real clusters.\n",
    "                masked = np.abs( t_scores[n] ) > threshold\n",
    "                clusters, n_clusters = measurements.label(masked)\n",
    "                cluster_sums = measurements.sum(t_scores[n], clusters, index=np.arange(n_clusters)+1)\n",
    "\n",
    "                ## Compute null cluster sums.\n",
    "                null_sums = np.array([largest_cluster(permuted[m, n, :], threshold) for m in xrange(n_shuffles)])\n",
    "\n",
    "                ## Compute cluster bounds.\n",
    "                tmin = np.array([times[clusters==i].min() for i in np.arange(n_clusters)+1])\n",
    "                tmax = np.array([times[clusters==i].max() for i in np.arange(n_clusters)+1])\n",
    "\n",
    "                ## Find proportion of clusters that are larger.\n",
    "                p_values = [((np.abs(cs) < null_sums).sum() + 1.) / (null_sums.shape[0] + 1.) for cs in cluster_sums]\n",
    "\n",
    "                ## Store results.\n",
    "                for t1, t2, cs, pval in zip(tmin,tmax,cluster_sums,p_values):\n",
    "                    results['Subject'] += [subject]\n",
    "                    results['Freq'] += [freq]\n",
    "                    results['Label'] += [label]\n",
    "                    results['Contrast'] += [con]\n",
    "                    results['Tmin'] += [t1]\n",
    "                    results['Tmax'] += [t2]\n",
    "                    results['Score'] += [cs]\n",
    "                    results['Pval'] += [pval]\n",
    "\n",
    "            ## Organize results and append.\n",
    "            results = DataFrame(results)\n",
    "            results['Tdiff'] = results['Tmax'] - results['Tmin']\n",
    "            results = results[results['Tdiff']>=min_cluster]\n",
    "            df.append(results)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute cluster statistics.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Merge dataframes.\n",
    "df = concat(df)\n",
    "\n",
    "## Organize columns and sort.\n",
    "cols = ['Subject','Contrast','Label','Freq','Tmin','Tmax','Tdiff','Score','Pval']\n",
    "df = df[cols].sort_values(['Subject','Contrast','Tmin']).reset_index(drop=True)\n",
    "\n",
    "## FDR correct within contrasts.\n",
    "df['FDR'] = 0\n",
    "for contrast in df.Contrast.unique():\n",
    "    _, fdr = fdr_correction(df.loc[df.Contrast==contrast,'Pval'], alpha)\n",
    "    df.loc[df.Contrast==contrast,'FDR'] = fdr\n",
    "    \n",
    "## Save.\n",
    "f = os.path.join(out_dir, 'subjects_%s_%s_%s_results.csv' %(model_name, analysis, domain))\n",
    "df.to_csv(f, index=False)\n",
    "        \n",
    "print 'Done.'\n",
    "\n",
    "df[(df.Contrast=='DBS')&(df.FDR<0.05)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reaction Time Correlations\n",
    "### Perform permutations\n",
    "Please see: /space/sophia/2/users/EMOTE-DBS/afMSIT/scripts/cmdline/afMSIT_permutations.py\n",
    "### Identify clusters, perform FDR corrections, and assemble results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import concat, read_csv\n",
    "from mne.stats import fdr_correction\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O Parameters.\n",
    "space = 'source'\n",
    "analysis = 'resp'\n",
    "labels = ['dacc-lh', 'dacc-rh', 'dmpfc-lh', 'dmpfc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "          'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "          'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "domain = 'timedomain'\n",
    "freqs = ['15']\n",
    "model = 'revised'\n",
    "\n",
    "## Cluster parameters.\n",
    "min_cluster = 0.05 # secs\n",
    "alpha = 0.05\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/%s/rt' %space\n",
    "results_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/%s/results' %space\n",
    "\n",
    "df = []\n",
    "for label in labels:\n",
    "    \n",
    "    for freq in freqs:\n",
    "        \n",
    "        df.append( read_csv(os.path.join(root_dir, 'afMSIT_%s_%s_%s_%s_rt.csv' %(space, analysis, label, freq))) )\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Merge, trim, correct, and save.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "## Merge and trim.\n",
    "df = concat(df)\n",
    "df = df[df.Tdiff > min_cluster]\n",
    "\n",
    "## FDR correct.\n",
    "df['FDR'] = fdr_correction(df.Pval, alpha=alpha)[-1]\n",
    "\n",
    "## Save.\n",
    "f = os.path.join(results_dir, '%s_%s_%s_rt.csv' %(model, analysis, domain))\n",
    "df.to_csv(f, index=False)\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MADRS Correlations\n",
    "### Permutations for MADRS Correlations\n",
    "Please see: /space/sophia/2/users/EMOTE-DBS/afMSIT/scripts/cmdline/afMSIT_permutations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "np.random.seed(47404)\n",
    "\n",
    "## Define parameters.\n",
    "n_subj = 8\n",
    "n_shuffles = 1000\n",
    "\n",
    "## Prepare\n",
    "ix = np.arange(n_subj)\n",
    "permutations = np.expand_dims(ix[::-1], 0)\n",
    "    \n",
    "while True: \n",
    "    np.random.shuffle(ix)\n",
    "    permutations = np.concatenate([permutations, np.expand_dims(ix, 0)], axis=0)\n",
    "    permutations = DataFrame(permutations).drop_duplicates()\n",
    "    if permutations.shape[0] < n_shuffles: permutations = permutations.as_matrix()\n",
    "    else: break\n",
    "        \n",
    "## Save.\n",
    "np.save('/space/sophia/2/users/EMOTE-DBS/afMSIT/scripts/cmdline/madrs_permutations', permutations.as_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MADRS Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from scipy.stats import spearmanr\n",
    "from mne.stats import fdr_correction\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "space = 'source'\n",
    "analysis = 'stim'\n",
    "domain = 'frequency'\n",
    "model = 'revised'\n",
    "contrast = 'DBS'\n",
    "freq = 'theta'\n",
    "fdr = 0.05\n",
    "\n",
    "subjects = np.array(['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2'])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Prepare MADRS Scores.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/%s' %space\n",
    "info = read_csv(os.path.join(root_dir, 'afMSIT_%s_info.csv' %space))\n",
    "\n",
    "## Prepare MADRS scores.\n",
    "scores = read_csv('behavior/afMSIT_demographics.csv', index_col=0)\n",
    "subjects = scores.index\n",
    "madrs = scores['MADRS_Now'] - scores['MADRS_Base']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "results = read_csv(os.path.join(root_dir, 'results', '%s_%s_%s_results.csv' %(model,analysis,domain)))\n",
    "results = results[(results.Contrast==contrast)&(results.FDR<fdr)&(results.Freq==freq)].reset_index(drop=True)\n",
    "results['MADRS_Corr_Rval'] = 0 \n",
    "results['MADRS_Corr_Pval'] = 0 \n",
    "\n",
    "for n in range(len(results)):\n",
    "    \n",
    "    label, freq, tmin, tmax = results.loc[n, ['Label','Freq','Tmin','Tmax']]\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and prepare data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    npz = np.load(os.path.join(root_dir, 'afMSIT_%s_%s_%s_%s.npz' %(space, analysis, label, freq)))\n",
    "    data = npz['data']\n",
    "    times = npz['times']\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute condition differences.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    delta = np.zeros(len(subjects))\n",
    "    mask = (times >= tmin) & (times <= tmax)    \n",
    "    \n",
    "    for m, subject in enumerate(subjects):\n",
    "        i, = np.where((info['Subject']==subject)&(info[contrast]==0))\n",
    "        j, = np.where((info['Subject']==subject)&(info[contrast]==1))\n",
    "        delta[m] += (data[j][:,mask].mean(axis=0) - data[i][:,mask].mean(axis=0)).mean()\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Perform correlations.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Compute true correlations.\n",
    "    r, p = spearmanr(madrs, delta)\n",
    "    if label == 'dlpfc_5-lh': test = delta.copy()\n",
    "    results.loc[n, 'MADRS_Corr_Rval'] = r\n",
    "    results.loc[n, 'MADRS_Corr_Pval'] = p\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Save.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Save.\n",
    "_, results['MADRS_Corr_Pval'] = fdr_correction(results['MADRS_Corr_Pval'])\n",
    "\n",
    "f = os.path.join(root_dir, 'results', '%s_%s_%s_madrs.csv' %(model, analysis, domain))\n",
    "# results.to_csv(f, index=False)\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contrast</th>\n",
       "      <th>Label</th>\n",
       "      <th>Freq</th>\n",
       "      <th>Tmin</th>\n",
       "      <th>Tmax</th>\n",
       "      <th>Tdiff</th>\n",
       "      <th>Score</th>\n",
       "      <th>Pval</th>\n",
       "      <th>FDR</th>\n",
       "      <th>MADRS_Corr_Rval</th>\n",
       "      <th>MADRS_Corr_Pval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DBS</td>\n",
       "      <td>dlpfc_4-lh</td>\n",
       "      <td>theta</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.581379</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>75.537991</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>0.041387</td>\n",
       "      <td>0.805118</td>\n",
       "      <td>0.059239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DBS</td>\n",
       "      <td>dlpfc_5-lh</td>\n",
       "      <td>theta</td>\n",
       "      <td>0.378621</td>\n",
       "      <td>0.639310</td>\n",
       "      <td>0.260690</td>\n",
       "      <td>76.045844</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>0.041387</td>\n",
       "      <td>0.853913</td>\n",
       "      <td>0.055722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBS</td>\n",
       "      <td>dlpfc_5-lh</td>\n",
       "      <td>theta</td>\n",
       "      <td>0.658621</td>\n",
       "      <td>1.170345</td>\n",
       "      <td>0.511724</td>\n",
       "      <td>184.423364</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.019314</td>\n",
       "      <td>0.780720</td>\n",
       "      <td>0.059239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Contrast       Label   Freq      Tmin      Tmax     Tdiff       Score  \\\n",
       "1      DBS  dlpfc_4-lh  theta  0.340000  0.581379  0.241379   75.537991   \n",
       "2      DBS  dlpfc_5-lh  theta  0.378621  0.639310  0.260690   76.045844   \n",
       "4      DBS  dlpfc_5-lh  theta  0.658621  1.170345  0.511724  184.423364   \n",
       "\n",
       "       Pval       FDR  MADRS_Corr_Rval  MADRS_Corr_Pval  \n",
       "1  0.004995  0.041387         0.805118         0.059239  \n",
       "2  0.004995  0.041387         0.853913         0.055722  \n",
       "4  0.000999  0.019314         0.780720         0.059239  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results.MADRS_Corr_Pval < 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLPFC_5-LH Findings\n",
    "### Special MADRS Correlation for DLPFC_5-LH (Stimulus-Locked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from pandas import read_csv\n",
    "from scipy.stats import spearmanr\n",
    "from mne.stats import fdr_correction\n",
    "np.random.seed(47404)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "space = 'source'\n",
    "analysis = 'stim'\n",
    "domain = 'frequency'\n",
    "model = 'revised'\n",
    "contrast = 'DBS'\n",
    "fdr = 0.05\n",
    "\n",
    "## Subject parameters.\n",
    "subjects = np.array(['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2'])\n",
    "\n",
    "## Bootstrapping parameters.\n",
    "n_straps = 1000\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "true_scores = np.zeros((2,2))\n",
    "bootstraps = np.zeros((2,n_straps))\n",
    "\n",
    "## Looping over including and excluding S2.\n",
    "for n in np.arange(2): \n",
    "\n",
    "    if n: subjects = np.delete(subjects, -1)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare MADRS Scores.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/%s' %space\n",
    "    info = read_csv(os.path.join(root_dir, 'afMSIT_%s_info.csv' %space))\n",
    "\n",
    "    ## Prepare MADRS scores.\n",
    "    ratings = read_csv('/space/sophia/2/users/EMOTE-DBS/afMSIT/behavior/Subject_Rating_Scales.csv')\n",
    "    ratings = ratings.set_index('Subject')\n",
    "    madrs = ratings.loc[subjects, 'MADRS_Now'] - ratings.loc[subjects, 'MADRS_Base']\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and prepare data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load cluster results.\n",
    "    results = read_csv(os.path.join(root_dir, 'results', '%s_%s_%s_results.csv' %(model,analysis,domain)))\n",
    "    results = results[(results.Contrast==contrast)&(results.FDR<fdr)].reset_index(drop=True)\n",
    "    results = results[results.Label == 'dlpfc_5-lh'] # Limit to the theta clusters.\n",
    "\n",
    "    ## Load time series data.\n",
    "    npz = np.load(os.path.join(root_dir, 'afMSIT_%s_%s_%s_%s.npz' %(space, analysis, 'dlpfc_5-lh', 'theta')))\n",
    "    data = npz['data']\n",
    "    times = npz['times']\n",
    "\n",
    "    ## Compute condition differences.\n",
    "    delta = np.zeros(subjects.shape[0])\n",
    "    mask = (times >= results.Tmin.min()) & (times <= results.Tmax.min())    \n",
    "\n",
    "    for m, subject in enumerate(subjects):\n",
    "        i, = np.where((info['Subject']==subject)&(info[contrast]==0))\n",
    "        j, = np.where((info['Subject']==subject)&(info[contrast]==1))\n",
    "        delta[m] += (data[j][:,mask].mean(axis=0) - data[i][:,mask].mean(axis=0)).mean()\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Perform correlations.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Compute true correlations.\n",
    "    true_scores[n] = spearmanr(madrs, delta)\n",
    "    \n",
    "    ## Perform bootstrapping\n",
    "    n_subj = subjects.shape[0]\n",
    "    for m in np.arange(n_straps):\n",
    "        \n",
    "        ix = np.random.choice(np.arange(n_subj), n_subj, replace=True)\n",
    "        bootstraps[n,m], _ = spearmanr(madrs[ix], delta[ix])\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plot Scatterplot.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ax = plt.subplot2grid((2,2),(0,n))\n",
    "    if not n: color = '#2b8cbe'\n",
    "    else: color = '#31a354'\n",
    "\n",
    "    ## Iteratively add points.\n",
    "    for x,y,subj in zip(madrs,delta,madrs.index):\n",
    "\n",
    "        ax.scatter(x,y,s=80,color=color)\n",
    "        ax.text(x+0.5,y+0.05,subj)\n",
    "\n",
    "    ## Add trend line.\n",
    "    x1, x2 = ax.get_xlim()\n",
    "    y1, y2 = ax.get_ylim()\n",
    "\n",
    "    z = np.polyfit(madrs, delta, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(np.linspace(x1,x2,1e3), p(np.linspace(x1,x2,1e3)), linewidth=2, color='k', alpha=0.8)\n",
    "    ax.set_xlim(x1,x2)\n",
    "    ax.set_ylim(y1,y2)\n",
    "\n",
    "    ## Add flourishes.\n",
    "    ax.set_xlabel('MADRS [Post - Pre]', fontsize=18)\n",
    "    ax.set_ylabel('Theta Power [On - Off]', fontsize=18)\n",
    "    if not n: ax.set_title('DLPFC-5 LH (S2 included)', fontsize=24)\n",
    "    else: ax.set_title('DLPFC-5 LH (S2 discarded)', fontsize=24)\n",
    "    ax.text(x1+0.5, y2*0.99, 'r = %0.3f\\np = %0.3f' %(true_scores[n,0],true_scores[n,1]), \n",
    "            fontsize=18, va='top')\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plot Histogram.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ax = plt.subplot2grid((2,2),(1,n))\n",
    "\n",
    "    ## Plot histogram.\n",
    "    x = bootstraps[n]\n",
    "    x = x[~np.isnan(x)]\n",
    "    ax.hist(x, bins=40, color=color)\n",
    "\n",
    "    ## Add true score.\n",
    "    y1, y2 = ax.get_ylim()\n",
    "    ax.vlines(true_scores[n,0], y1, y2, linestyle='--', linewidth=2)\n",
    "    ax.text(true_scores[n,0], y2*0.98, 'r = %0.3f' %true_scores[n,0], \n",
    "            ha='right', va='top', rotation='vertical', weight='heavy')\n",
    "    \n",
    "    ## Add flourishes.\n",
    "    ax.set_xlabel('Spearman R', fontsize=18) \n",
    "    ax.set_ylabel('Frequency', fontsize=18)\n",
    "    if not n: ax.set_title('%s Bootstraps (S2 included)' %n_straps, fontsize=20)\n",
    "    else: ax.set_title('%s Bootstraps (S2 discarded)' %n_straps, fontsize=20)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Save.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#        \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Hypomania Correlation for DLPFC_5-LH (Stimulus-Locked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from pandas import read_csv\n",
    "from scipy.stats import spearmanr\n",
    "from mne.stats import fdr_correction\n",
    "np.random.seed(47404)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "space = 'source'\n",
    "analysis = 'stim'\n",
    "domain = 'frequency'\n",
    "model = 'revised'\n",
    "contrast = 'DBS'\n",
    "fdr = 0.05\n",
    "\n",
    "## Subject parameters.\n",
    "subjects = np.array(['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA'])\n",
    "\n",
    "## Bootstrapping parameters.\n",
    "n_straps = 1000\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Prepare MADRS Scores.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/%s' %space\n",
    "info = read_csv(os.path.join(root_dir, 'afMSIT_%s_info.csv' %space))\n",
    "\n",
    "## Prepare MADRS scores.\n",
    "ratings = read_csv('/space/sophia/2/users/EMOTE-DBS/afMSIT/behavior/Subject_Characteristics.csv')\n",
    "ratings = ratings.set_index('Name')\n",
    "mania = ratings.loc[subjects, 'Hypomania']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load cluster results.\n",
    "results = read_csv(os.path.join(root_dir, 'results', '%s_%s_%s_results.csv' %(model,analysis,domain)))\n",
    "results = results[(results.Contrast==contrast)&(results.FDR<fdr)].reset_index(drop=True)\n",
    "results = results[results.Label == 'dlpfc_5-lh'] # Limit to the theta clusters.\n",
    "\n",
    "## Load time series data.\n",
    "npz = np.load(os.path.join(root_dir, 'afMSIT_%s_%s_%s_%s.npz' %(space, analysis, 'dlpfc_5-lh', 'theta')))\n",
    "data = npz['data']\n",
    "times = npz['times']\n",
    "\n",
    "## Compute condition differences.\n",
    "delta = np.zeros(subjects.shape[0])\n",
    "mask = (times >= results.Tmin.min()) & (times <= results.Tmax.min())    \n",
    "\n",
    "for m, subject in enumerate(subjects):\n",
    "    i, = np.where((info['Subject']==subject)&(info[contrast]==0))\n",
    "    j, = np.where((info['Subject']==subject)&(info[contrast]==1))\n",
    "    delta[m] += (data[j][:,mask].mean(axis=0) - data[i][:,mask].mean(axis=0)).mean()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Perform correlations.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Compute true correlations.\n",
    "true_scores = spearmanr(mania, delta)\n",
    "\n",
    "## Perform bootstrapping\n",
    "n_subj = subjects.shape[0]\n",
    "bootstraps = np.zeros(n_straps)\n",
    "\n",
    "for m in np.arange(n_straps):\n",
    "\n",
    "    ix = np.random.choice(np.arange(n_subj), n_subj, replace=True)\n",
    "    bootstraps[m], _ = spearmanr(mania[ix], delta[ix])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot Scatterplot.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = plt.subplot2grid((1,2),(0,0))\n",
    "color = '#2b8cbe'\n",
    "\n",
    "## Iteratively add points.\n",
    "for x,y,subj in zip(mania,delta,mania.index):\n",
    "\n",
    "    ax.scatter(x,y,s=80,color=color)\n",
    "    ax.text(x+0.01,y+0.05,subj)\n",
    "\n",
    "## Add trend line.\n",
    "x1, x2 = ax.get_xlim()\n",
    "y1, y2 = ax.get_ylim()\n",
    "\n",
    "z = np.polyfit(mania, delta, 1)\n",
    "p = np.poly1d(z)\n",
    "ax.plot(np.linspace(x1,x2,1e3), p(np.linspace(x1,x2,1e3)), linewidth=2, color='k', alpha=0.8)\n",
    "ax.set_xlim(x1,x2)\n",
    "ax.set_ylim(y1,y2)\n",
    "\n",
    "## Add flourishes.\n",
    "ax.set_xlabel('Hypomania', fontsize=18)\n",
    "ax.set_ylabel('Theta Power [On - Off]', fontsize=18)\n",
    "ax.set_title('DLPFC-5 LH', fontsize=24)\n",
    "ax.text(x1+0.01, y2*0.99, 'r = %0.3f\\np = %0.3f' %(true_scores[0],true_scores[1]), \n",
    "        fontsize=18, va='top')\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot Histogram.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "ax = plt.subplot2grid((1,2),(0,1))\n",
    "\n",
    "## Plot histogram.\n",
    "x = bootstraps[~np.isnan(bootstraps)]\n",
    "ax.hist(x, bins=40, color=color)\n",
    "\n",
    "## Add true score.\n",
    "y1, y2 = ax.get_ylim()\n",
    "ax.vlines(true_scores[0], y1, y2, linestyle='--', linewidth=2)\n",
    "ax.text(true_scores[0], y2*0.98, 'r = %0.3f' %true_scores[0], \n",
    "        ha='right', va='top', rotation='vertical', weight='heavy')\n",
    "\n",
    "## Add flourishes.\n",
    "ax.set_xlabel('Spearman R', fontsize=18) \n",
    "ax.set_ylabel('Frequency', fontsize=18)\n",
    "ax.set_title('%s Bootstraps' %n_straps, fontsize=20)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Save.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#        \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intertrial Interval Spectra Comparison\n",
    "### Make ITI epochs\n",
    "Each trial is 1.93s. On average 5.6s between each trial. Minimum time between trials is 3.0s between trials. We take 2s windows of time, centered at the time between trials, using the baseline for the proceeding trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne import compute_covariance, Epochs, EpochsArray, find_events, read_proj, pick_types, set_log_level\n",
    "from mne.io import Raw\n",
    "from pandas import read_csv\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "\n",
    "## Filtering parameters.\n",
    "l_freq = 0.5\n",
    "h_freq = 50\n",
    "l_trans_bandwidth = l_freq / 2.\n",
    "h_trans_bandwidth = 1.0\n",
    "filter_length = '20s'\n",
    "n_jobs = 3\n",
    "\n",
    "## Epoching parameters.\n",
    "event_id = dict( FN=1, FI=2, NN=3, NI=4 )      # Alik's convention, isn't he smart!?\n",
    "tmin = -1.25\n",
    "tmax = 1.25\n",
    "baseline = (-0.5, -0.1)\n",
    "reject   = dict(eeg=150e-6)\n",
    "flat     = dict(eeg=5e-7)\n",
    "detrend = None\n",
    "decim = 1\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load behavior.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT'\n",
    "data_file = os.path.join( root_dir, 'behavior', 'afMSIT_group_data.csv' )\n",
    "df = read_csv(data_file)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for subj in subjects:\n",
    "    \n",
    "    print 'Loading data for %s.' %subj\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    # Define paths.\n",
    "    root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT'\n",
    "    raw_file  = os.path.join( root_dir, 'raw', '%s_%s_raw.fif' %(subj,task) )\n",
    "    proj_file = os.path.join( root_dir, 'raw', '%s_%s-proj.fif' %(subj,task) )\n",
    "    \n",
    "    # Load data.\n",
    "    raw = Raw(raw_file,add_eeg_ref=False,preload=True,verbose=False)\n",
    "    proj = read_proj(proj_file)\n",
    "    \n",
    "    ## Add projections.\n",
    "    proj = [p for p in proj if 'ref' not in p['desc']]\n",
    "    raw.add_proj(proj, remove_existing=True)\n",
    "    raw.set_eeg_reference()\n",
    "    raw.apply_proj()\n",
    "    \n",
    "    ## Reduce dataframe to subject.\n",
    "    data = df[df.Subject==subj]\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Make events.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    trial_onsets  = find_events(raw, stim_channel='Trig1', output='onset', min_duration=0.25, verbose=False)\n",
    "    trial_offsets = find_events(raw, stim_channel='Trig2', output='offset', min_duration=0.25, verbose=False)\n",
    "    \n",
    "    ## Identify time at halfway point between end of trial(n-1) and start of trial(n).\n",
    "    iti_lengths = raw.times[trial_onsets[1:,0]] - raw.times[trial_offsets[:-1,0]]\n",
    "    iti_centers = raw.times[trial_offsets[:-1,0]] + iti_lengths / 2.\n",
    "\n",
    "    ## Prepend first ITI center as 1.5 prior to first onset (corresponding to a 3s \"ITI\" before first trial).\n",
    "    first_onset = raw.times[trial_onsets[0,0]]\n",
    "    iti_centers = np.insert(iti_centers,0,first_onset - 1.5)\n",
    "\n",
    "    ## Modify first event of DBS-on so that center of ITI prior to first onset occurs 1.5s before onset.\n",
    "    ix = np.where( (data.DBS==1) & (data.Trial==1) )[0][0]\n",
    "    first_onset = raw.times[trial_onsets[ix,0]]\n",
    "    iti_centers[ix] = first_onset - 1.5\n",
    "\n",
    "    ## Make events.\n",
    "    events = np.zeros_like(trial_offsets)\n",
    "    events.T[0] += raw.time_as_index(iti_centers)\n",
    "    \n",
    "    # Update event identifiers.\n",
    "    n = 1\n",
    "    for dbs in [0,1]:\n",
    "        for cond in [0,1]:\n",
    "            ix, = np.where((data.DBS==dbs)&(data.Interference==cond))\n",
    "            events[ix,-1] = n\n",
    "            n+=1\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Filter\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    Fs = raw.info['sfreq']\n",
    "    raw.filter(l_freq = l_freq, h_freq = h_freq, filter_length=filter_length, n_jobs=n_jobs,\n",
    "               l_trans_bandwidth=l_trans_bandwidth, h_trans_bandwidth=h_trans_bandwidth)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Make epochs.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    # Make initial ITI epochs.\n",
    "    picks = pick_types(raw.info, meg=False, eeg=True, exclude='bads')\n",
    "    epochs = Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax, \n",
    "                    baseline=None, picks=picks, reject=None, flat=None, proj=True, \n",
    "                    detrend=detrend, decim=decim)\n",
    "\n",
    "    # Make baseline periods.\n",
    "    bl = Epochs(raw, trial_onsets, tmin=baseline[0], tmax=baseline[1], \n",
    "                baseline=None, picks=picks, reject=None, flat=None, proj=True, \n",
    "                detrend=detrend, decim=decim)\n",
    "    \n",
    "    # Baseline correction.\n",
    "    arr = epochs.get_data()\n",
    "    bl = bl.get_data().mean(axis=-1)\n",
    "    arr = (arr.T - bl.T).T\n",
    "    \n",
    "    # Finalize ITI epochs.\n",
    "    epochs = EpochsArray(arr, epochs.info, events=epochs.events, tmin=epochs.tmin, \n",
    "                         baseline=None, reject=reject, flat=flat, \n",
    "                         proj=True, verbose=False)\n",
    "    print epochs\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Save data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    epochs.save(os.path.join(root_dir,'ave','%s_%s_%s_iti-epo.fif' %(subj,task,h_freq)))\n",
    "    \n",
    "    print '\\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\\n'\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make source localized ITI epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from mne import read_epochs, read_label, read_source_spaces, set_log_level\n",
    "from mne.minimum_norm import apply_inverse_epochs, read_inverse_operator\n",
    "from scipy.io import loadmat\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "analysis = 'iti_bl'\n",
    "parc = 'april2016'\n",
    "fmax = 50\n",
    "\n",
    "## Source localization parameters.\n",
    "method = 'dSPM'\n",
    "snr = 1.0  \n",
    "lambda2 = 1.0 / snr ** 2\n",
    "pick_ori = 'normal'\n",
    "\n",
    "## Labels\n",
    "rois = ['dlpfc_5-lh']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Iteratively load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT'\n",
    "fs_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/freesurfs'\n",
    "\n",
    "## Prepare fsaverage source space.\n",
    "src = read_source_spaces(os.path.join(fs_dir,'fscopy','bem','fscopy-oct-6p-src.fif'))\n",
    "vertices_to = [src[n]['vertno'] for n in xrange(2)]\n",
    "labels = [read_label(os.path.join(fs_dir,'fscopy','label','april2016','%s.label' %roi), subject='fsaverage')\n",
    "          for roi in rois]\n",
    "\n",
    "for subject in subjects:\n",
    "\n",
    "    print 'Performing source localization: %s' %subject\n",
    "\n",
    "    ## Load in epochs.\n",
    "    epochs = read_epochs(os.path.join(root_dir,'ave','%s_%s_%s_%s-epo.fif' %(subject,task,fmax,analysis)))\n",
    "    times = epochs.times\n",
    "    \n",
    "    ## Load in secondary files.\n",
    "    inv = read_inverse_operator(os.path.join(root_dir,'cov','%s_%s_%s-inv.fif' %(subject,task,fmax)))\n",
    "    morph_mat = loadmat(os.path.join(root_dir, 'morph_maps', '%s-fsaverage_morph.mat' %subject))['morph_mat']\n",
    "\n",
    "    ## Make generator object.\n",
    "    G = apply_inverse_epochs(epochs, inv, method=method, lambda2=lambda2, pick_ori=pick_ori, return_generator=True)\n",
    "    del inv\n",
    "\n",
    "    ## Iteratively compute and store label timecourse. \n",
    "    ltcs = []\n",
    "    for g in G:\n",
    "        g = g.morph_precomputed('fsaverage', vertices_to=vertices_to, morph_mat=morph_mat)\n",
    "        ltcs.append( g.extract_label_time_course(labels, src, mode='pca_flip') )\n",
    "    ltcs = np.array(ltcs)\n",
    "    \n",
    "    ## Save.\n",
    "    f = os.path.join(root_dir,'source','stcs','%s_%s_%s_%s_%s_epochs' %(subject,task,analysis,method,fmax))\n",
    "    np.savez_compressed(f, ltcs=ltcs, times=times, labels=np.array([l.name for l in labels]), conds=epochs.events.T[-1])\n",
    "    del ltcs\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make power baseline for ITI spectral epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne.filter import low_pass_filter\n",
    "from mne.time_frequency import single_trial_power\n",
    "from pandas import read_csv\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Data parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "analysis = 'stim'\n",
    "method = 'dSPM'\n",
    "h_freq = 50\n",
    "\n",
    "## Label parameters.\n",
    "rois = [ 'dlpfc_5-lh']\n",
    "\n",
    "## Time-frequency parameters.\n",
    "freqs = np.logspace( np.log10(2), np.log10(50), num=25)\n",
    "n_cycles = 3\n",
    "baseline = (-0.5, -0.1)\n",
    "Fs = 1450.\n",
    "decim = 14\n",
    "n_jobs = 3\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/source'\n",
    "df = read_csv(os.path.join(root_dir, 'afMSIT_source_info.csv'))\n",
    "\n",
    "for roi in rois:\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Iteratively load and merge.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ltcs = []\n",
    "    for subject in subjects:\n",
    "\n",
    "        ## Load NPZ.\n",
    "        npz = np.load(os.path.join(root_dir, 'stcs', '%s_msit_%s_%s_%s_epochs.npz' %(subject,analysis,method,h_freq)))\n",
    "\n",
    "        ## Get label index and extract.\n",
    "        ix = npz['labels'].tolist().index(roi)\n",
    "        arr = npz['ltcs'][:,ix,:]\n",
    "\n",
    "        ## Append.\n",
    "        ltcs.append(arr)\n",
    "\n",
    "    ## Concatenate.\n",
    "    ltcs = np.concatenate(ltcs, axis=0)\n",
    "    times = npz['times']\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Phase-lock removal.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    for subject in df.Subject.unique():\n",
    "        for dbs in [0, 1]:\n",
    "            for cond in [0, 1]:\n",
    "                ix, = np.where((df.Subject==subject)&(df.DBS==dbs)&(df.Interference==cond))\n",
    "                ltcs[ix] -= ltcs[ix].mean(axis=0)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute power.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    print 'Computing single trial power: %s %s.' %(roi, analysis)\n",
    "    ltcs = np.expand_dims(ltcs,1)\n",
    "    power = single_trial_power(ltcs, sfreq=Fs, frequencies=freqs, n_cycles=n_cycles, \n",
    "                                baseline=None, n_jobs=n_jobs, verbose=False)\n",
    "    power = power.squeeze()\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Within-trial normalization.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    power = (power.T / np.median(power, axis=-1).T).T # median, not mean\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare baseline normalization (within subject, DBS).\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Compute baseline normalization.\n",
    "    if analysis == 'stim':\n",
    "\n",
    "        ## Make time mask.\n",
    "        mask = (times >= baseline[0]) & (times <= baseline[1])\n",
    "\n",
    "        ## Iteratively compute over subjects.\n",
    "        blnorm = []\n",
    "        for subject in df.Subject.unique():\n",
    "\n",
    "            ## Iteratively compute over DBS conditions..\n",
    "            sbl = []\n",
    "            for dbs in [0,1]:\n",
    "\n",
    "                ix, = np.where((df.Subject==subject)&(df.DBS==dbs))\n",
    "                sbl.append( np.apply_over_axes(np.median, power[ix][:,:,mask], axes=[0,2]).squeeze() )\n",
    "\n",
    "            blnorm.append(sbl)\n",
    "\n",
    "        ## Merge.\n",
    "        blnorm = np.array(blnorm)\n",
    "      \n",
    "        ## Save.\n",
    "        np.save(os.path.join(root_dir, 'afMSIT_source_iti_%s_bl' %roi), blnorm)\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute power of source timecourses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne.filter import low_pass_filter\n",
    "from mne.time_frequency import single_trial_power\n",
    "from pandas import DataFrame\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Data parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "method = 'dSPM'\n",
    "h_freq = 50\n",
    "\n",
    "## Label parameters.\n",
    "rois = ['dlpfc_5-lh']\n",
    "\n",
    "## Time-frequency parameters.\n",
    "freqs = np.logspace( np.log10(2), np.log10(50), num=25)\n",
    "n_cycles = 3\n",
    "Fs = 1450.\n",
    "decim = 14\n",
    "n_jobs = 3\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/source'\n",
    "\n",
    "for roi in rois:\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Iteratively load and merge.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ltcs = []\n",
    "    df = dict(Condition = [], Subject = [])\n",
    "    \n",
    "    for subject in subjects:\n",
    "\n",
    "        ## Load NPZ.\n",
    "        npz = np.load(os.path.join(root_dir, 'stcs', '%s_msit_iti_%s_%s_epochs.npz' %(subject,method,h_freq)))\n",
    "\n",
    "        ## Get label index and extract.\n",
    "        ix = npz['labels'].tolist().index(roi)\n",
    "        arr = npz['ltcs'][:,ix,:]\n",
    "\n",
    "        ## Phase-lock removal.\n",
    "        for cond in [1,2,3,4]:\n",
    "            ix, = np.where(npz['conds']==cond)\n",
    "            arr[ix] -= arr[ix].mean(axis=0)\n",
    "        \n",
    "        ## Append.\n",
    "        ltcs.append(arr)\n",
    "        df['Condition'] += npz['conds'].tolist()\n",
    "        df['Subject'] += [subject] * len(npz['conds'])\n",
    "\n",
    "    ## Concatenate.\n",
    "    ltcs = np.concatenate(ltcs, axis=0)\n",
    "    times = npz['times']\n",
    "    df = DataFrame(df)\n",
    "    df['DBS'] = (df.Condition > 2).astype(int)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute power.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    print 'Computing single trial power: %s.' %roi\n",
    "    ltcs = np.expand_dims(ltcs,1)\n",
    "    power = single_trial_power(ltcs, sfreq=Fs, frequencies=freqs, n_cycles=n_cycles, \n",
    "                                baseline=None, n_jobs=n_jobs, verbose=False)\n",
    "    power = power.squeeze()\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Within-trial normalization.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    power = (power.T / np.median(power, axis=-1).T).T # median, not mean\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Apply baseline normalization (within subject, DBS).\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load baseline for for normalization.\n",
    "    blnorm = np.load(os.path.join(root_dir, 'afMSIT_source_iti_%s_bl.npy' %roi))\n",
    "    \n",
    "    ## Setup index vectors.\n",
    "    n_trials, n_freqs, n_times = power.shape        \n",
    "    _, subj_ix = np.unique(df.Subject, return_inverse=True)\n",
    "    dbs_ix = df.DBS.as_matrix()\n",
    "    trial_ix = np.arange(n_trials) \n",
    "\n",
    "    ## Main loop.\n",
    "    for i,j,k in zip(trial_ix,subj_ix,dbs_ix):\n",
    "\n",
    "        for m in xrange(n_times):\n",
    "\n",
    "            power[i,:,m] /= blnorm[j,k,:]\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Final preprocessing steps.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Crop times.\n",
    "    mask = (times >= -1) & (times <= 1)\n",
    "    power = power[:, :, mask]\n",
    "    times = times[mask]\n",
    "\n",
    "    ## Decimate\n",
    "    power = power[:,:,::decim]\n",
    "    times = times[::decim]\n",
    "    \n",
    "    ## Save.\n",
    "    f = os.path.join(root_dir, 'afMSIT_source_iti_%s_spectra' %roi)\n",
    "    np.savez_compressed(f, power=power, times=times, freqs=freqs, n_cycles=n_cycles, \n",
    "                        subject=df.Subject.as_matrix(), conds=df.Condition.as_matrix())\n",
    "\n",
    "    del power\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resting State Spectral Comparisons\n",
    "### Projections: EOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mne import write_proj\n",
    "from mne.preprocessing import compute_proj_eog\n",
    "from mne.io import Raw\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Setup\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## File params.\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/resting/raw'\n",
    "subjects = ['S2']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "raw_files = os.listdir(root_dir)\n",
    "raw_files = [f for f in raw_files if f.split('_')[0] in subjects and f.endswith('raw.fif')]\n",
    "\n",
    "for raw_file in raw_files:    \n",
    "    \n",
    "    ## Load files.\n",
    "    raw_file = os.path.join(root_dir, raw_file)\n",
    "    raw = Raw(raw_file, preload=True, verbose=False, add_eeg_ref=False)\n",
    "    raw.add_eeg_average_proj()\n",
    "    \n",
    "    proj, _ = compute_proj_eog(raw, n_eeg = 4, average=True, filter_length='20s',\n",
    "                               reject=dict(eeg=5e-4), flat=dict(eeg=5e-8),  ch_name='EOG', n_jobs=3)\n",
    "    write_proj(raw_file.replace('_raw.fif', '_eog-proj.fif'), proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projections: ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mne import read_proj, write_proj\n",
    "from mne.preprocessing import compute_proj_ecg\n",
    "from mne.io import Raw\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Setup\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## File params.\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/resting/raw'\n",
    "subjects = ['CHDR']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "raw_files = os.listdir(root_dir)\n",
    "raw_files = [f for f in raw_files if f.split('_')[0] in subjects and f.endswith('raw.fif')]\n",
    "\n",
    "for raw_file in raw_files:    \n",
    "    \n",
    "    ## Load files.\n",
    "    raw_file = os.path.join(root_dir, raw_file)\n",
    "    raw = Raw(raw_file, preload=True, verbose=False, add_eeg_ref=False)\n",
    "    raw.add_eeg_average_proj()\n",
    "    \n",
    "    ## Make ECG proj. Save.\n",
    "    proj, _ = compute_proj_ecg(raw, n_eeg = 4, h_freq = 35., average=True, filter_length='20s',\n",
    "                                reject=dict(eeg=5e-4), flat=dict(eeg=5e-8), ch_name='P9', n_jobs=3)\n",
    "    write_proj(raw_file.replace('_raw.fif', '_ecg-proj.fif'), proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from mne import pick_channels, read_proj, set_log_level\n",
    "from mne.io import Raw\n",
    "from mne.time_frequency import psd_multitaper\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define Parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "subjects = ['BRTU', 'CHDR', 'JADE', 'S2']\n",
    "conds = ['resting_dbsoff_eo', 'resting_dbson_eo']\n",
    "\n",
    "## Filtering parameters.\n",
    "fmin, fmax = 0.5, 30\n",
    "tmin, tmax = 10, 40\n",
    "chs = ['FCZ']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/resting/raw'\n",
    "\n",
    "psds = []\n",
    "for subject in subjects:\n",
    "    \n",
    "    psds.append([])\n",
    "    \n",
    "    for cond in conds:\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Prepare data. \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        ## Load data.\n",
    "        raw = os.path.join(root_dir, '%s_%s_raw.fif' %(subject,cond))\n",
    "        raw = Raw(raw, preload=True, add_eeg_ref=False, verbose=False)\n",
    "        \n",
    "        ## Load/apply projection(s).\n",
    "        proj = os.path.join(root_dir, '%s_%s-proj.fif' %(subject,cond))\n",
    "        \n",
    "        if os.path.isfile(proj): \n",
    "            proj = read_proj(proj)\n",
    "            raw.add_proj(proj)\n",
    "        else:\n",
    "            raw.set_eeg_reference()\n",
    "        \n",
    "        raw.apply_proj()\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute PSD.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        picks = pick_channels(raw.ch_names, chs)\n",
    "        psd, freqs = psd_multitaper(raw, fmin=fmin, fmax=fmax, tmin=10, tmax=40, picks=picks, n_jobs=3)\n",
    "        psds[-1].append(psd)\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Merge and plot.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Merge.\n",
    "psds = np.array(psds).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Normalize.\n",
    "psd = psds.mean(axis=0)\n",
    "auc = psd.sum(axis=-1)\n",
    "psd = (psd.T / auc).T\n",
    "\n",
    "## Plot.\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "for arr, label, color in zip(psd, ['DBSoff','DBSon'], ['#0571b0','#ca0020']):\n",
    "    ax.plot(freqs, arr, linewidth=4, label=label, color=color, alpha=0.7)\n",
    "ax.legend(loc=1, frameon=False, fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.close('all')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "423px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
