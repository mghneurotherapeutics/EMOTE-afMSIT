{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Localization\n",
    "## Step 1: Perform MNE on Single Trial Epochs\n",
    "Due to the large size of these datasets, we will perform source localization across subjects and store the resulting trials separately label-by-label. This will help with the I/O problem down the line in not having to load in / analyze large datasets all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from mne import read_epochs, read_label, read_source_spaces, set_log_level\n",
    "from mne.minimum_norm import apply_inverse_epochs, read_inverse_operator\n",
    "from scipy.io import loadmat\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "analysis = 'resp'\n",
    "parc = 'april2016'\n",
    "fmax = 50\n",
    "\n",
    "## Source localization parameters.\n",
    "method = 'dSPM'\n",
    "snr = 1.0  \n",
    "lambda2 = 1.0 / snr ** 2\n",
    "pick_ori = 'normal'\n",
    "\n",
    "## Labels\n",
    "rois = ['dacc-lh', 'dacc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "        'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "        'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Iteratively load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT'\n",
    "fs_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/freesurfs'\n",
    "\n",
    "## Prepare fsaverage source space.\n",
    "src = read_source_spaces(os.path.join(fs_dir,'fscopy','bem','fscopy-oct-6p-src.fif'))\n",
    "vertices_to = [src[n]['vertno'] for n in xrange(2)]\n",
    "labels = [read_label(os.path.join(fs_dir,'fscopy','label','april2016','%s.label' %roi), subject='fsaverage')\n",
    "          for roi in rois]\n",
    "\n",
    "for subject in subjects:\n",
    "\n",
    "    print 'Performing source localization: %s' %subject\n",
    "\n",
    "    ## Load in epochs.\n",
    "    epochs = read_epochs(os.path.join(root_dir,'ave','%s_%s_%s_%s-epo.fif' %(subject,task,fmax,analysis)))\n",
    "    times = epochs.times\n",
    "    \n",
    "    ## Load in secondary files.\n",
    "    inv = read_inverse_operator(os.path.join(root_dir,'cov','%s_%s_%s-inv.fif' %(subject,task,fmax)))\n",
    "    morph_mat = loadmat(os.path.join(root_dir, 'morph_maps', '%s-fsaverage_morph.mat' %subject))['morph_mat']\n",
    "\n",
    "    ## Make generator object.\n",
    "    G = apply_inverse_epochs(epochs, inv, mtethod=method, lambda2=lambda2, pick_ori=pick_ori, return_generator=True)\n",
    "    del epochs, inv\n",
    "\n",
    "    ## Iteratively compute and store label timecourse. \n",
    "    ltcs = []\n",
    "    for g in G:\n",
    "        g = g.morph_precomputed('fsaverage', vertices_to=vertices_to, morph_mat=morph_mat)\n",
    "        ltcs.append( g.extract_label_time_course(labels, src, mode='pca_flip') )\n",
    "    ltcs = np.array(ltcs)\n",
    "    \n",
    "    ## Save.\n",
    "    f = os.path.join(root_dir,'source','stcs','%s_%s_%s_%s_%s_epochs' %(subject,task,analysis,method,fmax))\n",
    "    np.savez_compressed(f, ltcs=ltcs, times=times, labels=np.array([l.name for l in labels]))\n",
    "    del ltcs\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Assemble by label. Make ERP/power objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne.filter import low_pass_filter\n",
    "from mne.time_frequency import single_trial_power\n",
    "from pandas import read_csv\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Data parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "method = 'dSPM'\n",
    "h_freq = 50\n",
    "\n",
    "## Label parameters.\n",
    "rois = ['dacc-lh', 'dacc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "        'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "        'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "\n",
    "## ERP parameters.\n",
    "fmax = 15\n",
    "erp_decim = 3\n",
    "\n",
    "## Time-frequency parameters.\n",
    "freqs = np.logspace( np.log10(2), np.log10(50), num=25)\n",
    "n_cycles = 3\n",
    "baseline = (-0.5, -0.1)\n",
    "Fs = 1450.\n",
    "power_decim = 14\n",
    "n_jobs = 3\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/source'\n",
    "\n",
    "for analysis in ['stim', 'resp']:\n",
    "\n",
    "    df = read_csv(os.path.join(root_dir, analysis, 'afMSIT_source_%s_info.csv' %analysis))\n",
    "\n",
    "    for roi in rois:\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Iteratively load and merge.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ltcs = []\n",
    "        for subject in subjects:\n",
    "\n",
    "            ## Load NPZ.\n",
    "            npz = np.load(os.path.join(root_dir, 'stcs', '%s_msit_%s_%s_%s_epochs.npz' %(subject,analysis,method,h_freq)))\n",
    "\n",
    "            ## Get label index and extract.\n",
    "            ix = npz['labels'].tolist().index(roi)\n",
    "            arr = npz['ltcs'][:,ix,:]\n",
    "\n",
    "            ## Append.\n",
    "            ltcs.append(arr)\n",
    "\n",
    "        ## Concatenate.\n",
    "        ltcs = np.concatenate(ltcs, axis=0)\n",
    "        times = npz['times']\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Time-domain processing.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ## Make ERP objects.\n",
    "        ERPs = low_pass_filter(ltcs, Fs, fmax, filter_length='20s', n_jobs=n_jobs)\n",
    "\n",
    "        ## Save.\n",
    "        np.savez_compressed(os.path.join(root_dir, analysis, 'afMSIT_%s_%s_%s_%s' %(analysis,method,roi,fmax)), \n",
    "                            ltcs=ERPs[:,::erp_decim], times=times[::erp_decim])\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Power-domain processing.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ## Phase-lock removal.\n",
    "        for subject in df.Subject.unique():\n",
    "            for dbs in [0, 1]:\n",
    "                for cond in [0, 1]:\n",
    "                    ix, = np.where((df.Subject==subject)&(df.DBS==dbs)&(df.Interference==cond))\n",
    "                    ltcs[ix] -= ltcs[ix].mean(axis=0)\n",
    "\n",
    "\n",
    "        ## Compute power.\n",
    "        print 'Computing single trial power.'\n",
    "        ltcs = np.expand_dims(ltcs,1)\n",
    "        power = single_trial_power(ltcs, sfreq=Fs, frequencies=freqs, n_cycles=n_cycles, \n",
    "                                    baseline=None, decim=power_decim, n_jobs=n_jobs, verbose=False)\n",
    "        power = power.squeeze()\n",
    "        times = times[::power_decim]\n",
    "\n",
    "        ## Baseline correct.\n",
    "        if analysis == 'stim':\n",
    "            mask = (times >= baseline[0]) & ((times <= baseline[1]))\n",
    "            bl = power[:,:,mask].mean(axis=-1)\n",
    "        else: \n",
    "            bl = np.load(os.path.join(root_dir, 'stim', 'afMSIT_baseline_power_%s.npy' %roi))\n",
    "\n",
    "        power = np.array([arr / bl.T for arr in power.T]).T\n",
    "        power = np.log10(power) * 10\n",
    "\n",
    "        ## Iteratively save.    \n",
    "        ix, = np.where((freqs>=4)&(freqs<=8))\n",
    "        theta = power[:,ix,:].mean(axis=1)\n",
    "\n",
    "        ix, = np.where((freqs>=8)&(freqs<=15))\n",
    "        alpha = power[:,ix,:].mean(axis=1)\n",
    "\n",
    "        ix, = np.where((freqs>=15)&(freqs<=30))\n",
    "        beta = power[:,ix,:].mean(axis=1)\n",
    "\n",
    "        ## Save data.\n",
    "        np.save(os.path.join(root_dir, analysis, 'afMSIT_baseline_power_%s' %roi), bl)\n",
    "        np.savez_compressed(os.path.join(root_dir, analysis, 'afMSIT_%s_%s_%s_power' %(analysis,method,roi)), \n",
    "                            power=power, theta=theta, alpha=alpha, beta=beta, \n",
    "                            times=times, freqs=freqs, n_cycles=n_cycles)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Plot Spectrograms of Labels\n",
    "This step is necessary to ensure against any possible artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from cmap_utils import  *\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Data parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "method = 'dSPM'\n",
    "h_freq = 50\n",
    "\n",
    "## Label parameters.\n",
    "rois = ['dacc-lh', 'dacc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "        'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "        'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Setup colormap.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "cdict = {'red':   ((0.0, 0.0, 0.0),\n",
    "                  (0.5, 0.0, 0.1),\n",
    "                  (1.0, 1.0, 1.0)),\n",
    "\n",
    "        'green': ((0.0, 0.0, 0.0),\n",
    "                  (1.0, 0.0, 0.0)),\n",
    "\n",
    "        'blue':  ((0.0, 0.0, 1.0),\n",
    "                  (0.5, 0.1, 0.0),\n",
    "                  (1.0, 0.0, 0.0))\n",
    "        }\n",
    "\n",
    "bbr = LinearSegmentedColormap('bbr', cdict)\n",
    "plt.register_cmap(cmap=bbr)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Setup colormap.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "img_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/plots/source/spectrograms'\n",
    "\n",
    "for roi in rois:\n",
    "    \n",
    "    fig, axes = plt.subplots(1,2,figsize=(12,6), sharey=True)\n",
    "    \n",
    "    for ax, analysis in zip(axes,['stim','resp']):\n",
    "    \n",
    "        ## Load and extract data.\n",
    "        root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/source/%s' %analysis\n",
    "        npz = np.load(os.path.join(root_dir, 'afMSIT_%s_%s_%s_power.npz' %(analysis,method,roi)))\n",
    "        power = npz['power'].mean(axis=0)\n",
    "        times = npz['times']\n",
    "        freqs = npz['freqs']\n",
    "\n",
    "        ## Cropping.\n",
    "        if analysis == 'stim': mask = (times >= -0.5) & (times <= 2.0)\n",
    "        elif analysis == 'resp': mask = (times >= -1.0) & (times <= 1.0)\n",
    "        else: raise ValueError('What you trying to pull, bub?')\n",
    "        times = times[mask]\n",
    "        power = power[:,mask]\n",
    "        \n",
    "        ## Plot power.\n",
    "        cmap = center_color_map(power, 'bwr')\n",
    "        cbar = ax.imshow(power, cmap=cmap, aspect='auto', origin='lower')\n",
    "        plt.colorbar(cbar, ax=ax)\n",
    "        \n",
    "        ## Add flourishes.\n",
    "        ax.set_xticks(np.linspace(0,times.shape[0],9))\n",
    "        ax.set_xticklabels(np.linspace(times.min(), times.max(), 9).round(1))\n",
    "        ax.set_xlabel('Time (s)', fontsize=18)\n",
    "        ax.set_yticks(np.linspace(0,freqs.shape[0],9))\n",
    "        ax.set_yticklabels(np.linspace(freqs.min(), freqs.max(), 9).astype(int))\n",
    "        if analysis == 'stim': ax.set_ylabel('Frequency', fontsize=18)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        ax.set_title('%s %s' %(roi.upper(),analysis.capitalize()), fontsize=24)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(img_dir, 'spectrogram_%s.png' %roi))\n",
    "    plt.close('all')\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Domain Statistics\n",
    "## Step 1: Perform Iterative ME-GLM\n",
    "We will use mixed effects regression with statsmodels. See:\n",
    "1. http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4416363/\n",
    "2. http://www.ncbi.nlm.nih.gov/pubmed/15570152\n",
    "\n",
    "NOTE: All permutations performed on cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compute cluster statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from mne.stats import fdr_correction\n",
    "from pandas import DataFrame, concat\n",
    "from scipy.ndimage import measurements\n",
    "from scipy.stats import norm\n",
    "\n",
    "def largest_cluster(arr, threshold):\n",
    "    masked = np.abs( arr ) > threshold\n",
    "    clusters, n_clusters = measurements.label(masked)\n",
    "    cluster_sums = measurements.sum(arr, clusters, index=np.arange(n_clusters)+1)\n",
    "    if not len(cluster_sums): return 0\n",
    "    else: return np.abs(cluster_sums).max()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Specify parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## File parameters.\n",
    "analyses = ['stim','resp']\n",
    "labels = ['dacc-lh', 'dacc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', \n",
    "          'dlpfc_2-lh', 'dlpfc_2-rh', 'dlpfc_3-lh', 'dlpfc_3-rh', \n",
    "          'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh',\n",
    "          'dlpfc_6-lh', 'dlpfc_6-rh']\n",
    "method = 'dSPM'\n",
    "model_name = 'core'\n",
    "fmax = 15\n",
    "\n",
    "## Statistics parameters.\n",
    "alpha = 0.05\n",
    "min_cluster = 0.05 #ms\n",
    "\n",
    "threshold = norm.ppf(1 - alpha/2.)\n",
    "if model_name == 'core': contrasts = ['DBS', 'Interference', 'Arousal.High']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for analysis in analyses:\n",
    "    \n",
    "    df = []\n",
    "    \n",
    "    for label in labels:\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Load files. Assemble permutations.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT/source'\n",
    "        results_dir = os.path.join(root_dir, analysis, model_name)\n",
    "\n",
    "        ## Load true statistics.\n",
    "        f = np.load(os.path.join(results_dir, '%s_%s_%s_%s_obs.npz' %(method, label, model_name, fmax)))\n",
    "        if not np.isclose(f['convergence'].mean(), 1): print 'WARNING: Not all models converged.'\n",
    "        t_scores = f['t_scores'].squeeze() # Remove intercept.\n",
    "        times = f['times']\n",
    "\n",
    "        ## Load null statistics.\n",
    "        npz = sorted([f for f in os.listdir(results_dir) if '%s_%s_%s_perm' %(label,model_name,fmax) in f])\n",
    "        for n, f in enumerate(npz):\n",
    "            f = np.load(os.path.join(results_dir, f))\n",
    "            if not np.isclose(f['convergence'].mean(), 1): print 'WARNING: Not all models converged.'\n",
    "            if not n: permuted = f['t_scores']\n",
    "            else: permuted = np.concatenate([permuted, f['t_scores']], axis=-1)\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute cluster statistics.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ## Get info.\n",
    "        n_eff, n_times, n_shuffles = permuted.shape\n",
    "\n",
    "        ## Remove intercept.\n",
    "        t_scores = t_scores[1:]\n",
    "        permuted = permuted[1:]\n",
    "\n",
    "        ## Iteratively compute clusters.\n",
    "        results = defaultdict(list)\n",
    "\n",
    "        for n, con in enumerate(contrasts):\n",
    "\n",
    "            ## Find real clusters.\n",
    "            masked = np.abs( t_scores[n] ) > threshold\n",
    "            clusters, n_clusters = measurements.label(masked)\n",
    "            cluster_sums = measurements.sum(t_scores[n], clusters, index=np.arange(n_clusters)+1)\n",
    "\n",
    "            ## Compute null cluster sums.\n",
    "            null_sums = np.array([largest_cluster(permuted[n,:,m], threshold) for m in xrange(n_shuffles)])\n",
    "\n",
    "            ## Compute cluster bounds.\n",
    "            tmin = np.array([times[clusters==i].min() for i in np.arange(n_clusters)+1])\n",
    "            tmax = np.array([times[clusters==i].max() for i in np.arange(n_clusters)+1])\n",
    "\n",
    "            ## Find proportion of clusters that are larger.\n",
    "            p_values = [(np.abs(cs) < null_sums).mean() for cs in cluster_sums]\n",
    "\n",
    "            ## Store results.\n",
    "            for t1, t2, cs, pval in zip(tmin,tmax,cluster_sums,p_values):\n",
    "                results['Contrast'] += [con]\n",
    "                results['Tmin'] += [t1]\n",
    "                results['Tmax'] += [t2]\n",
    "                results['Score'] += [cs]\n",
    "                results['Pval'] += [pval]\n",
    "\n",
    "        ## Organize results and append.\n",
    "        results['Label'] = label\n",
    "        results = DataFrame(results)[['Label','Contrast','Tmin','Tmax','Score','Pval']]\n",
    "        results['Length'] = results['Tmax'] - results['Tmin']\n",
    "        results = results[results['Length']>=min_cluster]\n",
    "        df.append(results)\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute cluster statistics.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Merge. FDR Correct.\n",
    "    df = concat(df)\n",
    "    df['FDR'] = fdr_correction(df['Pval'])[1]\n",
    "\n",
    "    ## Save.\n",
    "    f = os.path.join(results_dir, '%s_%s_%s_results.csv' %(method,model_name,fmax))\n",
    "    df.to_csv(f, index=False)\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Plotting of Corrected Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from pandas import read_csv\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/source'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## File parameters.\n",
    "analyses = ['stim','resp']\n",
    "labels = ['dacc-lh', 'dacc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', \n",
    "          'dlpfc_2-lh', 'dlpfc_2-rh', 'dlpfc_3-lh', 'dlpfc_3-rh', \n",
    "          'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh',\n",
    "          'dlpfc_6-lh', 'dlpfc_6-rh']\n",
    "\n",
    "model_name = 'core'\n",
    "method = 'dSPM'\n",
    "fmax = 15\n",
    "\n",
    "if model_name == 'core':\n",
    "    contrasts = ['DBS','Interference','Arousal.High']\n",
    "    colors = ['#0571b0', '#ca0020', '#7b3294', '#008837', '#a6611a', '#018571']\n",
    "    legends = ['Off','On','Neu','Int','Low','High']\n",
    "\n",
    "for analysis in analyses:\n",
    "    \n",
    "    for label in labels:\n",
    "    \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Load data.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ## Load trial info.\n",
    "        f = os.path.join(root_dir, analysis, 'afMSIT_source_%s_info.csv' %analysis)\n",
    "        df = read_csv(f)\n",
    "\n",
    "        ## Load cluster results.\n",
    "        f = os.path.join(root_dir, analysis, model_name, '%s_%s_%s_results.csv' %(method, model_name, fmax))\n",
    "        cs = read_csv(f)\n",
    "\n",
    "        ## Load ERP data.\n",
    "        npz = np.load(os.path.join(root_dir, analysis, 'afMSIT_%s_%s_%s_%s.npz' %(analysis,method,label,fmax)))\n",
    "        Y = npz['ltcs']\n",
    "        times = npz['times']\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Plotting.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        img_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/plots/source'\n",
    "\n",
    "        n_eff = len(contrasts)\n",
    "        fig, axes = plt.subplots(1,3,figsize=(n_eff*8,6), sharex=True, sharey=True)\n",
    "\n",
    "        for n, con in enumerate(contrasts):\n",
    "            for m in [0,1]: \n",
    "\n",
    "                ## Derive indices.\n",
    "                ix, = np.where(df[con]==m)\n",
    "                i = n*2+m\n",
    "\n",
    "                ## Plot ERP.\n",
    "                mu = Y[ix].mean(axis=0)\n",
    "                axes[n].plot(times, mu, linewidth=3, label=legends[i],\n",
    "                             color=colors[i], alpha=0.8, )\n",
    "\n",
    "                ## Plot SE.\n",
    "                se = Y[ix].std(axis=0) / np.sqrt(Y.shape[0])\n",
    "                axes[n].fill_between(times, mu-se, mu+se, color=colors[i], alpha=0.2)\n",
    "\n",
    "            ## Add flourishes.\n",
    "            axes[n].legend(loc=4, fontsize=20, frameon=False, borderpad=0, handletextpad=0.2)\n",
    "            axes[n].set_xlabel('Time (s)', fontsize=24)\n",
    "            if not n: axes[n].set_ylabel('dSPM', fontsize=24)\n",
    "            axes[n].tick_params(axis='both', which='major', labelsize=14)\n",
    "            axes[n].set_title('%s %s' %(label,con), fontsize=24)\n",
    "\n",
    "            ## Time-lock specific.\n",
    "            if analysis == 'stim':\n",
    "                y1, y2 = axes[n].get_ylim()\n",
    "                axes[n].set_xlim(-0.25,2.0)\n",
    "                for x,s in zip([0, 0.4, 1.127],['IAPS','MSIT','Resp']): \n",
    "                    axes[n].text(x+0.02,y1+0.01,s,fontsize=16)\n",
    "                    axes[n].vlines(x,y1,y2,linestyle='--',alpha=0.3)\n",
    "                axes[n].set_ylim(y1,y2)\n",
    "\n",
    "            elif analysis == 'resp':\n",
    "                y1, y2 = axes[n].get_ylim()\n",
    "                axes[n].set_xlim(-1,1)\n",
    "                axes[n].text(0.02,y1+0.01,'Resp',fontsize=16)\n",
    "                axes[n].vlines(0.0,y1,y2,linestyle='--',alpha=0.3)\n",
    "                axes[n].set_ylim(y1,y2)\n",
    "\n",
    "            ## Plot significant clusters.\n",
    "            for ix in np.where((cs.Label==label)&(cs.Contrast==con)&(cs.FDR<0.05))[0]:\n",
    "                tmin, tmax = cs.loc[ix,'Tmin'], cs.loc[ix,'Tmax']\n",
    "                axes[n].fill_between(np.linspace(tmin,tmax,1e3), y1, y2, color='k', alpha=0.2)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # plt.show()\n",
    "        plt.savefig(os.path.join(img_dir, '%s_evokeds_clusters_%s_%s_%s.png' %(label,analysis,method,fmax)))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency-Domain Statistics\n",
    "## Step 1: Perform Iterative ME-GLM\n",
    "We will use mixed effects regression with statsmodels. See:\n",
    "1. http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4416363/\n",
    "2. http://www.ncbi.nlm.nih.gov/pubmed/15570152\n",
    "\n",
    "NOTE: All permutations performed on cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compute cluster statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from mne.stats import fdr_correction\n",
    "from pandas import DataFrame, concat\n",
    "from scipy.ndimage import measurements\n",
    "from scipy.stats import norm\n",
    "\n",
    "def largest_cluster(arr, threshold):\n",
    "    masked = np.abs( arr ) > threshold\n",
    "    clusters, n_clusters = measurements.label(masked)\n",
    "    cluster_sums = measurements.sum(arr, clusters, index=np.arange(n_clusters)+1)\n",
    "    if not len(cluster_sums): return 0\n",
    "    else: return np.abs(cluster_sums).max()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Specify parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## File parameters.\n",
    "analyses = ['stim','resp']\n",
    "labels = ['dacc-lh', 'dacc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', \n",
    "          'dlpfc_2-lh', 'dlpfc_2-rh', 'dlpfc_3-lh', 'dlpfc_3-rh', \n",
    "          'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh',\n",
    "          'dlpfc_6-lh', 'dlpfc_6-rh']\n",
    "bands = ['theta','alpha','beta']\n",
    "method = 'dSPM'\n",
    "model_name = 'core'\n",
    "\n",
    "## Statistics parameters.\n",
    "alpha = 0.05\n",
    "min_cluster = 0.05 #ms\n",
    "\n",
    "threshold = norm.ppf(1 - alpha/2.)\n",
    "if model_name == 'core': contrasts = ['DBS', 'Interference', 'Arousal.High']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for analysis in analyses:\n",
    "    \n",
    "    df = []\n",
    "    \n",
    "    for label in labels:\n",
    "        \n",
    "        for band in bands:\n",
    "        \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            ### Load files. Assemble permutations.\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT/source'\n",
    "            results_dir = os.path.join(root_dir, analysis, model_name)\n",
    "\n",
    "            ## Load true statistics.\n",
    "            f = np.load(os.path.join(results_dir, '%s_%s_%s_%s_obs.npz' %(method, label, model_name, band)))\n",
    "            if not np.isclose(f['convergence'].mean(), 1): print 'WARNING: Not all models converged.'\n",
    "            t_scores = f['t_scores'].squeeze() # Remove intercept.\n",
    "            times = f['times']\n",
    "\n",
    "            ## Load null statistics.\n",
    "            npz = sorted([f for f in os.listdir(results_dir) if '%s_%s_%s_perm' %(label,model_name,band) in f])\n",
    "            for n, f in enumerate(npz):\n",
    "                f = np.load(os.path.join(results_dir, f))\n",
    "                if not np.isclose(f['convergence'].mean(), 1): print 'WARNING: Not all models converged.'\n",
    "                if not n: permuted = f['t_scores']\n",
    "                else: permuted = np.concatenate([permuted, f['t_scores']], axis=-1)\n",
    "\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            ### Compute cluster statistics.\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "            ## Get info.\n",
    "            n_eff, n_times, n_shuffles = permuted.shape\n",
    "\n",
    "            ## Remove intercept.\n",
    "            t_scores = t_scores[1:]\n",
    "            permuted = permuted[1:]\n",
    "\n",
    "            ## Iteratively compute clusters.\n",
    "            results = defaultdict(list)\n",
    "\n",
    "            for n, con in enumerate(contrasts):\n",
    "\n",
    "                ## Find real clusters.\n",
    "                masked = np.abs( t_scores[n] ) > threshold\n",
    "                clusters, n_clusters = measurements.label(masked)\n",
    "                cluster_sums = measurements.sum(t_scores[n], clusters, index=np.arange(n_clusters)+1)\n",
    "\n",
    "                ## Compute null cluster sums.\n",
    "                null_sums = np.array([largest_cluster(permuted[n,:,m], threshold) for m in xrange(n_shuffles)])\n",
    "\n",
    "                ## Compute cluster bounds.\n",
    "                tmin = np.array([times[clusters==i].min() for i in np.arange(n_clusters)+1])\n",
    "                tmax = np.array([times[clusters==i].max() for i in np.arange(n_clusters)+1])\n",
    "\n",
    "                ## Find proportion of clusters that are larger.\n",
    "                p_values = [(np.abs(cs) < null_sums).mean() for cs in cluster_sums]\n",
    "\n",
    "                ## Store results.\n",
    "                for t1, t2, cs, pval in zip(tmin,tmax,cluster_sums,p_values):\n",
    "                    results['Contrast'] += [con]\n",
    "                    results['Tmin'] += [t1]\n",
    "                    results['Tmax'] += [t2]\n",
    "                    results['Score'] += [cs]\n",
    "                    results['Pval'] += [pval]\n",
    "\n",
    "            ## Organize results and append.\n",
    "            results['Label'] = label\n",
    "            results['Band'] = band\n",
    "            results = DataFrame(results)[['Label','Band','Contrast','Tmin','Tmax','Score','Pval']]\n",
    "            results['Length'] = results['Tmax'] - results['Tmin']\n",
    "            results = results[results['Length']>=min_cluster]\n",
    "            df.append(results)\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute cluster statistics.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Merge. FDR Correct.\n",
    "    df = concat(df)\n",
    "    df['FDR'] = fdr_correction(df['Pval'])[1]\n",
    "\n",
    "    ## Save.\n",
    "    f = os.path.join(results_dir, '%s_%s_power_results.csv' %(method,model_name))\n",
    "    df.to_csv(f, index=False)\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Plotting of Corrected Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from pandas import read_csv\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/source'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## File parameters.\n",
    "analyses = ['stim','resp']\n",
    "rois = ['dacc-lh', 'dacc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', \n",
    "          'dlpfc_2-lh', 'dlpfc_2-rh', 'dlpfc_3-lh', 'dlpfc_3-rh', \n",
    "          'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh',\n",
    "          'dlpfc_6-lh', 'dlpfc_6-rh']\n",
    "bands = ['theta','alpha','beta']\n",
    "method = 'dSPM'\n",
    "model_name = 'core'\n",
    "\n",
    "if model_name == 'core':\n",
    "    contrasts = ['DBS','Interference','Arousal.High']\n",
    "    colors = ['#0571b0', '#ca0020', '#7b3294', '#008837', '#a6611a', '#018571']\n",
    "    labels = ['Off','On','Neu','Int','Low','High']\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "img_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/plots/source/freq_domain/'\n",
    "\n",
    "for analysis in analyses:\n",
    "\n",
    "    for roi in rois:\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Load data.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ## Load trial info.\n",
    "        f = os.path.join(root_dir, analysis, 'afMSIT_source_%s_info.csv' %analysis)\n",
    "        df = read_csv(f)\n",
    "\n",
    "        ## Load cluster results.\n",
    "        f = os.path.join(root_dir, analysis, model_name, '%s_%s_power_results.csv' %(method, model_name))\n",
    "        cs = read_csv(f)\n",
    "\n",
    "        ## Load ERP data.\n",
    "        npz = np.load(os.path.join(root_dir, analysis, 'afMSIT_%s_%s_%s_power.npz' %(analysis,method,roi)))\n",
    "        times = npz['times']\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Plotting\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        n_bands, n_eff = len(bands), len(contrasts)\n",
    "        fig, axes = plt.subplots(n_bands,n_eff,figsize=(n_eff*8,n_bands*6), sharex=True, sharey=True)\n",
    "\n",
    "        for n, band in enumerate(bands):\n",
    "\n",
    "            Y = npz[band]\n",
    "\n",
    "            for m, con in enumerate(contrasts):\n",
    "\n",
    "                for c in [0,1]: \n",
    "\n",
    "                    ## Derive indices.\n",
    "                    ix, = np.where(df[con]==c)\n",
    "                    i = m*2+c\n",
    "\n",
    "                    ## Plot ERP.\n",
    "                    mu = Y[ix].mean(axis=0)\n",
    "                    axes[n,m].plot(times, mu, linewidth=3, label=labels[i],\n",
    "                                 color=colors[i], alpha=0.8, )\n",
    "\n",
    "                    ## Plot SE.\n",
    "                    se = Y[ix].std(axis=0) / np.sqrt(Y.shape[0])\n",
    "                    axes[n,m].fill_between(times, mu-se, mu+se, color=colors[i], alpha=0.2)\n",
    "\n",
    "                ## Add flourishes.\n",
    "                axes[n,m].legend(loc=4, fontsize=20, frameon=False, borderpad=0, handletextpad=0.2)\n",
    "                axes[n,m].set_xlabel('Time (s)', fontsize=24)\n",
    "                if not m: axes[n,m].set_ylabel('Power [log10(db)]', fontsize=24)\n",
    "                axes[n,m].tick_params(axis='both', which='major', labelsize=14)\n",
    "                axes[n,m].set_title('%s %s %s' %(roi, band,con), fontsize=24)\n",
    "\n",
    "                ## Time-lock specific.\n",
    "                if analysis == 'stim':\n",
    "                    y1, y2 = axes[n,m].get_ylim()\n",
    "                    axes[n,m].set_xlim(-0.25,2.0)\n",
    "                    for x,s in zip([0, 0.4, 1.127],['IAPS','MSIT','Resp']): \n",
    "                        axes[n,m].text(x+0.02,y1+0.1,s,fontsize=16)\n",
    "                        axes[n,m].vlines(x,y1,y2,linestyle='--',alpha=0.3)\n",
    "                    axes[n,m].set_ylim(y1,y2)\n",
    "\n",
    "                elif analysis == 'resp':\n",
    "                    y1, y2 = axes[n,m].get_ylim()\n",
    "                    axes[n,m].set_xlim(-1,1)\n",
    "                    axes[n,m].text(0.02,y1+0.1,'Resp',fontsize=16)\n",
    "                    axes[n,m].vlines(0.0,y1,y2,linestyle='--',alpha=0.3)\n",
    "                    axes[n,m].set_ylim(y1,y2)\n",
    "\n",
    "                ## Plot significant clusters.\n",
    "                for ix in np.where((cs.Label==roi)&(cs.Band==band)&(cs.Contrast==con)&(cs.FDR<0.05))[0]:\n",
    "                    tmin, tmax = cs.loc[ix,'Tmin'], cs.loc[ix,'Tmax']\n",
    "                    axes[n,m].fill_between(np.linspace(tmin,tmax,1e3), y1, y2, color='k', alpha=0.2)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # plt.show()\n",
    "        plt.savefig(os.path.join(img_dir, '%s_evokeds_clusters_%s_%s_power.png' %(roi,analysis,method)))\n",
    "        plt.close()\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations/Miscellaneous\n",
    "## Perform source localization on time-domain evoked, per subject, within power band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from mne import read_epochs, read_forward_solution, read_source_spaces, set_log_level\n",
    "from mne import EpochsArray, compute_covariance\n",
    "from mne.filter import band_pass_filter\n",
    "from mne.minimum_norm import apply_inverse, make_inverse_operator\n",
    "from scipy.io import loadmat\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "analyses = ['stim','resp']\n",
    "parc = 'april2016'\n",
    "fmax = 50\n",
    "\n",
    "## Frequency parameters.\n",
    "fdict = dict(theta = [4,8], alpha = [8,15])\n",
    "\n",
    "## Source localization parameters.\n",
    "loose = 0.2\n",
    "depth = 0.8\n",
    "\n",
    "method = 'dSPM'\n",
    "snr = 3.0 # for evoked  \n",
    "lambda2 = 1.0 / snr ** 2\n",
    "pick_ori = 'normal'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define paths.\n",
    "fs_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/freesurfs'\n",
    "root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT'\n",
    "out_dir = os.path.join(root_dir, 'source', 'stcs', 'power')\n",
    "\n",
    "## Prepare fsaverage source space.\n",
    "src = read_source_spaces(os.path.join(fs_dir,'fscopy','bem','fscopy-oct-6p-src.fif'))\n",
    "vertices_to = [src[n]['vertno'] for n in xrange(2)]\n",
    "\n",
    "for subject in subjects:\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load secondary objects.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    morph_mat = loadmat(os.path.join(root_dir, 'morph_maps', '%s-fsaverage_morph.mat' %subject))['morph_mat']\n",
    "    fwd = read_forward_solution(os.path.join(root_dir, 'fwd', '%s_%s-fwd.fif' %(subject,task)), \n",
    "                                surf_ori=True, verbose=False)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Source localization loop.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    for analysis in analyses:\n",
    "        \n",
    "        print 'Beginning %s %s.' %(subject,analysis),\n",
    "        \n",
    "        for k, v in fdict.iteritems():\n",
    "    \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            ### Load and preprocess epochs.\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "            ## Load in epochs.\n",
    "            epochs = read_epochs(os.path.join(root_dir,'ave','%s_%s_%s_%s-epo.fif' %(subject,task,fmax,analysis)))\n",
    "\n",
    "            ## Extract necessary information from epochs.\n",
    "            arr = epochs._data\n",
    "            info = epochs.info\n",
    "\n",
    "            ## Bandpass filter.\n",
    "            fp1, fp2 = v\n",
    "            arr = band_pass_filter(arr, epochs.info['sfreq'], fp1, fp2, filter_length='20s', n_jobs=3, verbose=False)\n",
    "            \n",
    "            ## Re-combine epochs.\n",
    "            epochs = EpochsArray(arr, epochs.info, events=epochs.events, \n",
    "                                 tmin=epochs.times.min(), event_id=epochs.event_id)\n",
    "    \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            ### Compute noise covariance / inverse solution.\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "            ## Compute/save noise covariance matrix & inverse operator.\n",
    "            noise_cov = compute_covariance(epochs, tmin=-0.5, tmax=-0.1, method='shrunk', n_jobs=1)\n",
    "            inv = make_inverse_operator(epochs.info, fwd, noise_cov, loose=loose, depth=depth, verbose=False)\n",
    "    \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            ### Perform source localization.\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "            ## Iteratively source localize evoked.\n",
    "            for event in epochs.event_id.iterkeys():\n",
    "\n",
    "                ## Make evoked.\n",
    "                evoked = epochs[event].average()\n",
    "\n",
    "                ## Source localize.\n",
    "                stc = apply_inverse(evoked, inv,  method=method, lambda2=lambda2, pick_ori=pick_ori)\n",
    "                stc = stc.morph_precomputed('fsaverage', vertices_to=vertices_to, morph_mat=morph_mat)\n",
    "                del evoked\n",
    "\n",
    "                ## Save.\n",
    "                f = os.path.join(out_dir, '%s_%s_%s_%s_%s' %(subject,task,analysis,event,k))\n",
    "                stc.save(f)\n",
    "            \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average over bands / Compute contrasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne import read_source_estimate\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "analyses = ['stim','resp']\n",
    "bands = ['theta','alpha']\n",
    "conds = ['FN', 'FI', 'NN', 'NI']\n",
    "condict = dict(dbsoff = [1, 1, 0, 0], dbson = [0,0,1,1],\n",
    "               neu = [1, 0, 1, 0], int = [0, 1, 0, 1],\n",
    "               dbscon = [-1,-1, 1, 1], intcon = [-1, 1, -1, 1])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/source/stcs/power'\n",
    "\n",
    "for analysis in analyses:\n",
    "    \n",
    "    for band in bands:\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Aggregate STCs within conditions across subjects.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        stcs = []\n",
    "        for cond in conds:\n",
    "        \n",
    "            for n, subject in enumerate(subjects):\n",
    "                \n",
    "                f = os.path.join(root_dir, '%s_%s_%s_%s_%s-lh.stc' %(subject,task,analysis,cond,band))\n",
    "                if not n: stc = read_source_estimate(f)\n",
    "                else: stc += read_source_estimate(f)            \n",
    "\n",
    "            stcs.append( stc / len(subjects) )\n",
    "            del stc\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute contrasts. \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        for k, v in condict.iteritems():\n",
    "            \n",
    "            stc = np.dot(stcs, v)                       # Apply contrast\n",
    "            if np.sum(v): stc /= np.sum(v)              # Apply mean (where applicable)\n",
    "            if analysis == 'stim': stc.crop(-0.5,2.0)   # Crop stimulus-locked\n",
    "            elif analysis == 'resp': stc.crop(-1.0,1.0) # Crop response-locked\n",
    "            stc.save(os.path.join(root_dir, 'afMSIT_%s_%s_%s' %(analysis,band,k)))\n",
    "            \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rogue Code: Subject-Level Permutation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from mne import read_epochs, read_forward_solution, read_label, read_source_spaces, set_log_level\n",
    "from mne import EpochsArray, compute_covariance\n",
    "from mne.filter import low_pass_filter\n",
    "from mne.minimum_norm import apply_inverse, make_inverse_operator\n",
    "from scipy.io import loadmat\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "analyses = ['stim','resp']\n",
    "parc = 'april2016'\n",
    "fmax = 50\n",
    "\n",
    "events = ['FN', 'FI', 'NN', 'NI']\n",
    "fp = 15\n",
    "\n",
    "## Source localization parameters.\n",
    "loose = 0.2\n",
    "depth = 0.8\n",
    "\n",
    "method = 'dSPM'\n",
    "snr = 3.0 # for evoked  \n",
    "lambda2 = 1.0 / snr ** 2\n",
    "pick_ori = 'normal'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define paths.\n",
    "fs_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/freesurfs'\n",
    "root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT'\n",
    "out_dir = os.path.join(root_dir, 'source', 'stcs', 'rogue')\n",
    "\n",
    "## Prepare fsaverage source space.\n",
    "src = read_source_spaces(os.path.join(fs_dir,'fscopy','bem','fscopy-oct-6p-src.fif'))\n",
    "vertices_to = [src[n]['vertno'] for n in xrange(2)]\n",
    "label = read_label(os.path.join(out_dir, 'test-lh.label'))\n",
    "\n",
    "for subject in subjects:\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load secondary objects.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    morph_mat = loadmat(os.path.join(root_dir, 'morph_maps', '%s-fsaverage_morph.mat' %subject))['morph_mat']\n",
    "    fwd = read_forward_solution(os.path.join(root_dir, 'fwd', '%s_%s-fwd.fif' %(subject,task)), \n",
    "                                surf_ori=True, verbose=False)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Source localization loop.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    for analysis in analyses:\n",
    "        \n",
    "        print 'Beginning %s %s.' %(subject,analysis),\n",
    "            \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Load and preprocess epochs.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ## Load in epochs.\n",
    "        epochs = read_epochs(os.path.join(root_dir,'ave','%s_%s_%s_%s-epo.fif' %(subject,task,fmax,analysis)))\n",
    "\n",
    "        ## Extract necessary information from epochs.\n",
    "        arr = epochs._data\n",
    "        info = epochs.info\n",
    "\n",
    "        ## Bandpass filter.\n",
    "        arr = low_pass_filter(arr, epochs.info['sfreq'], fp, filter_length='20s', n_jobs=3, verbose=False)\n",
    "\n",
    "        ## Re-combine epochs.\n",
    "        epochs = EpochsArray(arr, epochs.info, events=epochs.events, \n",
    "                             tmin=epochs.times.min(), event_id=epochs.event_id)\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute noise covariance / inverse solution.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ## Compute/save noise covariance matrix & inverse operator.\n",
    "        if analysis == 'stim': \n",
    "            noise_cov = compute_covariance(epochs, tmin=-0.5, tmax=-0.1, method='shrunk', n_jobs=1)\n",
    "            inv = make_inverse_operator(epochs.info, fwd, noise_cov, loose=loose, depth=depth, verbose=False)\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Perform source localization.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ## Iteratively source localize evoked.\n",
    "        data = []\n",
    "        for event in events:\n",
    "\n",
    "            ## Make evoked.\n",
    "            evoked = epochs[event].average()\n",
    "\n",
    "            ## Source localize.\n",
    "            stc = apply_inverse(evoked, inv,  method=method, lambda2=lambda2, pick_ori=pick_ori)\n",
    "            \n",
    "            ## Morph. Crop. Restrict to label.\n",
    "            stc = stc.morph_precomputed('fsaverage', vertices_to=vertices_to, morph_mat=morph_mat)\n",
    "            if analysis == 'stim': stc.crop(-0.5,2.0)\n",
    "            elif analysis == 'resp': stc.crop(-1.0,1.0)\n",
    "            stc = stc.in_label(label)\n",
    "            \n",
    "            ## Extract data.\n",
    "            data.append(stc.data)\n",
    "            \n",
    "        ## Save.\n",
    "        f = os.path.join(out_dir, '%s_%s_%s_%s' %(subject,task,analysis,fp))\n",
    "        np.savez_compressed(f, data=np.array(data), conds=events, vertno=stc.vertices, times=stc.times)\n",
    "            \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne.stats import f_mway_rm\n",
    "\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/source/stcs/rogue'\n",
    "\n",
    "data = []\n",
    "for subject in subjects:\n",
    "    npz = np.load(os.path.join(root_dir,'%s_msit_stim_15.npz' %subject))\n",
    "    data.append(npz['data'])\n",
    "    \n",
    "data = np.array(data)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "765px",
   "left": "0px",
   "right": "auto",
   "top": "106px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
