{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Preprocessing\n",
    "## Behavior Analysis\n",
    "### Generate trial regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import concat, read_csv\n",
    "from scipy.stats import gamma\n",
    "def normalize(arr): return (arr - arr.min()) / (arr.max() - arr.min())\n",
    "\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/behavior'\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "threshold = 0.005\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load / Concatenate / Prepare Data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "df = []\n",
    "for subject in subjects:\n",
    "    \n",
    "    ## Load CSV.\n",
    "    csv = read_csv(os.path.join(root_dir,'%s_msit_data.txt' %subject))\n",
    "    \n",
    "    ## Limit columns.\n",
    "    csv = csv[['SubjID','trial','iaps','DBS','interference','valence','arousal','responseTime','responseCorrect']]\n",
    "    \n",
    "    ## Rename columns.\n",
    "    csv.columns = ['Subject', 'Trial', 'IAPS', 'DBS', 'Interference', 'Valence_Obj', 'Arousal_Obj', 'RT', 'Accuracy']\n",
    "\n",
    "    ## Load IAPS ratings.\n",
    "    iaps = read_csv(os.path.join(root_dir,'%s_IAPS_SAM.csv' %subject))\n",
    "    iaps = iaps[['IAPS_Number','Valence','Arousal']]\n",
    "    iaps.columns = ['IAPS','Valence_Subj','Arousal_Subj']\n",
    "\n",
    "    ## Merge. Append.\n",
    "    csv = csv.merge(iaps, on='IAPS')\n",
    "    cols = ['Subject', 'Trial', 'IAPS', 'DBS', 'Interference', 'Valence_Obj', 'Arousal_Obj', \n",
    "            'Valence_Subj', 'Arousal_Subj', 'RT', 'Accuracy']\n",
    "    csv = csv[cols]\n",
    "    df.append(csv)\n",
    "\n",
    "## Merge data. Sort.\n",
    "df = concat(df)\n",
    "df['DBS'] = np.where(df['DBS']=='DBSoff',0,1)\n",
    "df = df.sort_values(['Subject','DBS','Trial']).reset_index(drop=True)\n",
    "\n",
    "## Normalize regressors.\n",
    "df['nsArousal'] = normalize(df.Arousal_Subj)\n",
    "df['nsValence'] = normalize(df.Valence_Subj)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Determine Trials for Inclusion/Exclusion.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Set missing RTs to NaNs.\n",
    "df['RT'] = np.where(df.Accuracy==-1, np.nan, df.RT)\n",
    "df['Accuracy'] = np.where(df.Accuracy==-1, np.nan, df.Accuracy)\n",
    "df['Missing'] = df.Accuracy.isnull().astype(int)\n",
    "\n",
    "## Add Error column.\n",
    "df['Error'] = 1 - df.Accuracy\n",
    "\n",
    "## Add Post-Error Column.\n",
    "df['PostError'] = 0\n",
    "for subject in df.Subject.unique():\n",
    "    error = df.loc[df.Subject==subject,'Error']\n",
    "    posterror = np.insert(np.roll(error,1)[1:], 0, 0)\n",
    "    df.loc[df.Subject==subject,'PostError'] = posterror\n",
    "\n",
    "## Iteratively detect outliers across subjects by fitting a Gamma distribution.\n",
    "df['GammaCDF'], df['Outlier'] = 0, 0\n",
    "for subject in df.Subject.unique():\n",
    "    \n",
    "    ## Fit Gamma to reaction time distribution.\n",
    "    shape, loc, scale = gamma.fit(df.loc[(df.Subject==subject)&(~df.RT.isnull()),'RT'], floc=0)\n",
    "    \n",
    "    ## Find outliers given likelihood threshold.\n",
    "    cdf = gamma.cdf(df.loc[(df.Subject==subject)&(~df.RT.isnull()),'RT'], shape, loc=loc, scale=scale)\n",
    "    outliers = (cdf < threshold) | (cdf > 1 - threshold)\n",
    "    \n",
    "    ## Append information.\n",
    "    df.loc[(df.Subject==subject)&(~df.RT.isnull()), 'GammaCDF'] += cdf\n",
    "    df.loc[(df.Subject==subject)&(~df.RT.isnull()), 'Outlier'] += outliers.astype(int)\n",
    "    \n",
    "## Generate exclude.\n",
    "df['Exclude'] = np.where( df[['Missing','Error','PostError','Outlier']].sum(axis=1), 1, 0)\n",
    "print '%s trials (%0.2f%%) excluded.' %(df.Exclude.sum(), df.Exclude.mean())\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Save.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "df.to_csv('%s/afMSIT_group_data.csv' %root_dir, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parcellation\n",
    "### Make EMOTE Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from mne import read_label, read_source_spaces, read_surface, set_log_level\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "fs_dir = '/space/sophia/2/users/EMOTE-DBS/freesurfs'\n",
    "subject = 'BRTU'\n",
    "\n",
    "parc = 'laus250'\n",
    "label_dir = os.path.join(fs_dir,subject,'label',parc)\n",
    "out_dir = os.path.join(fs_dir,subject,'label','april2016')\n",
    "\n",
    "if os.path.isdir(out_dir): shutil.rmtree(out_dir)\n",
    "os.makedirs(out_dir)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Build Left Hemisphere Labels.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "hemi = 'lh'\n",
    "rr, _ = read_surface(os.path.join(fs_dir, subject, 'surf', '%s.inflated' %hemi))\n",
    "src = read_source_spaces(os.path.join(fs_dir, subject, 'bem', '%s-oct-6-src.fif' %subject))[0]\n",
    "\n",
    "lhdict = {'dlpfc_1-lh':['caudalmiddlefrontal_1', 'caudalmiddlefrontal_5', 'caudalmiddlefrontal_6'],\n",
    "          'dlpfc_2-lh':['caudalmiddlefrontal_2', 'caudalmiddlefrontal_3', 'caudalmiddlefrontal_4'],\n",
    "          'dlpfc_3-lh':['rostralmiddlefrontal_2', 'rostralmiddlefrontal_3'],\n",
    "          'dlpfc_4-lh':['rostralmiddlefrontal_1', 'rostralmiddlefrontal_5'],\n",
    "          'dlpfc_5-lh':['parstriangularis_2', 'parsopercularis_2'],\n",
    "          'dlpfc_6-lh':['parsopercularis_3', 'parsopercularis_4'],\n",
    "          'racc-lh':['rostralanteriorcingulate_1','rostralanteriorcingulate_2'],\n",
    "          'dacc-lh':['caudalanteriorcingulate_1','caudalanteriorcingulate_2',],\n",
    "          'pcc-lh':['posteriorcingulate_2','posteriorcingulate_3']}\n",
    "\n",
    "for k,V in lhdict.iteritems():\n",
    "    \n",
    "    label = np.sum([read_label(os.path.join(label_dir,'%s-%s.label' %(v,hemi)), subject=subject) \n",
    "                    for v in V])\n",
    "    n_vert = np.intersect1d(src['vertno'], label.vertices).shape[0]\n",
    "    print '%s\\t%s' %(n_vert,k)\n",
    "    label.save(os.path.join(out_dir, '%s.label' %k))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Build Right Hemisphere Labels.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "hemi = 'rh'\n",
    "rr, _ = read_surface(os.path.join(fs_dir, subject, 'surf', '%s.inflated' %hemi))\n",
    "src = read_source_spaces(os.path.join(fs_dir, subject, 'bem', '%s-oct-6-src.fif' %subject))[1]\n",
    "\n",
    "rhdict = {'dlpfc_1-rh':['caudalmiddlefrontal_1', 'caudalmiddlefrontal_2', 'caudalmiddlefrontal_5'],\n",
    "          'dlpfc_2-rh':['caudalmiddlefrontal_3', 'caudalmiddlefrontal_4'],\n",
    "          'dlpfc_3-rh':['rostralmiddlefrontal_2', 'rostralmiddlefrontal_3'],\n",
    "          'dlpfc_4-rh':['rostralmiddlefrontal_1', 'rostralmiddlefrontal_5'],\n",
    "          'dlpfc_5-rh':['parstriangularis_2', 'parsopercularis_1'],\n",
    "          'dlpfc_6-rh':['parsopercularis_3', 'parsopercularis_4'],\n",
    "          'racc-rh':['rostralanteriorcingulate_1','rostralanteriorcingulate_2'],\n",
    "          'dacc-rh':['caudalanteriorcingulate_1','caudalanteriorcingulate_2','caudalanteriorcingulate_3'],\n",
    "          'pcc-rh':['posteriorcingulate_2','posteriorcingulate_3']}\n",
    "\n",
    "for k,V in rhdict.iteritems():\n",
    "    label = np.sum([read_label(os.path.join(label_dir,'%s-%s.label' %(v,hemi)), subject=subject) \n",
    "                    for v in V])\n",
    "    n_vert = np.intersect1d(src['vertno'], label.vertices).shape[0]\n",
    "    print '%s\\t%s' %(n_vert,k)\n",
    "    label.save(os.path.join(out_dir, '%s.label' %k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dmPFC Labels\n",
    "Similarly made from Lausanne atlas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mne import read_label, split_label\n",
    "\n",
    "laus_dir = '/space/lilli/1/users/DARPA-Recons/fscopy/label/laus125/'\n",
    "out_dir  = '/space/sophia/2/users/EMOTE-DBS/freesurfs/fscopy/label/april2016'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Build Left Hemisphere Labels.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Initialize with superiorfrontal_4-lh\n",
    "lh_label = read_label(os.path.join(laus_dir, 'superiorfrontal_4-lh.label'), subject='fsaverage')\n",
    "\n",
    "## Add split labels from superiorfrontal_7 \n",
    "labels = read_label(os.path.join(laus_dir, 'superiorfrontal_7-lh.label'), subject='fsaverage')\n",
    "labels = split_label(labels, parts=7)\n",
    "for label in labels[2:5]: lh_label += label\n",
    "    \n",
    "## Add split labels from superiorfrontal_5\n",
    "labels = read_label(os.path.join(laus_dir, 'superiorfrontal_5-lh.label'), subject='fsaverage')\n",
    "labels = split_label(labels, parts=4)\n",
    "lh_label += labels[0]\n",
    "\n",
    "## Rename save.\n",
    "lh_label.name = 'dmpfc-lh'\n",
    "lh_label.save( os.path.join(out_dir,'dmpfc-lh') )\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Build Right Hemisphere Labels.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Initialize with superiorfrontal_3 (first split)\n",
    "labels = read_label(os.path.join(laus_dir, 'superiorfrontal_3-rh.label'), subject='fsaverage')\n",
    "labels = split_label(labels, parts=4)\n",
    "rh_label = labels[0]\n",
    "\n",
    "## Add split labels from superiorfrontal_6\n",
    "labels = read_label(os.path.join(laus_dir, 'superiorfrontal_6-rh.label'), subject='fsaverage')\n",
    "labels = split_label(labels, parts=4)\n",
    "for label in labels[:2]: rh_label += label\n",
    "    \n",
    "## Add split labels from superiorfrontal_5\n",
    "labels = read_label(os.path.join(laus_dir, 'superiorfrontal_5-rh.label'), subject='fsaverage')\n",
    "labels = split_label(labels, parts=4)\n",
    "for label in labels[:2]: rh_label += label\n",
    "    \n",
    "## Rename save.\n",
    "rh_label.name = 'dmpfc-rh'\n",
    "rh_label.save( os.path.join(out_dir,'dmpfc-rh') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morph EMOTE labels\n",
    "Morphing was done in the command line using the following code."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "setenv SRC BRTU\n",
    "foreach TARG (fscopy CHDR CRDA JADE JASE M5 MEWA S2)\n",
    "rm -r $SUBJECTS_DIR/$TARG/label/april2016\n",
    "mne_morph_labels --from $SRC --to $TARG --labeldir $SUBJECTS_DIR/$SRC/label/april2016\n",
    "mv $SUBJECTS_DIR/$SRC/label/april2016/$TARG $SUBJECTS_DIR/$TARG/label/april2016\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Label Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne import read_source_spaces, read_label, set_log_level\n",
    "set_log_level(verbose=False)\n",
    "fs_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/freesurfs'\n",
    "src = read_source_spaces(os.path.join(fs_dir,'fscopy','bem','fscopy-oct-6p-src.fif'))\n",
    "rois = ['dacc-lh', 'dacc-rh', 'dmpfc-lh', 'dmpfc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "        'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "        'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "labels = [read_label(os.path.join(fs_dir,'fscopy','label','april2016','%s.label' %roi), subject='fsaverage')\n",
    "          for roi in rois]\n",
    "\n",
    "for label in labels:\n",
    "    if label.hemi == 'lh': print np.intersect1d(src[0]['vertno'], label.vertices).shape[0], label.name\n",
    "    else:  print np.intersect1d(src[1]['vertno'], label.vertices).shape[0], label.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot EMOTE Labels (Groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from surfer import Brain\n",
    "%matplotlib qt4\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Make brain.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "subject_id = \"fscopy\"\n",
    "hemi = \"rh\"\n",
    "surf = \"inflated\"\n",
    "brain = Brain(subject_id, hemi, surf)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Load files.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "fs_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/freesurfs'\n",
    "subj_dir = os.environ[\"SUBJECTS_DIR\"]\n",
    "rois = ['dacc-lh', 'dacc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "        'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "        'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "labels = [os.path.join(fs_dir,'fscopy','label','april2016','%s.label' %roi) for roi in rois]\n",
    "colors =['#810f7c', '#810f7c', \n",
    "        '#004529', '#004529', '#006837', '#006837', '#238443', '#238443', '#41ab5d', '#41ab5d', \n",
    "         '#78c679', '#78c679', '#addd8e', '#addd8e', '#d9f0a3', '#d9f0a3', '#f7fcb9', '#f7fcb9',\n",
    "        '#8c6bb1', '#8c6bb1', '#9ebcda', '#9ebcda']\n",
    "\n",
    "for n, label in enumerate(labels): \n",
    "    if hemi in rois[n]:\n",
    "        brain.add_label(label, color=colors[n], alpha=1)\n",
    "\n",
    "brain.show_view({'azimuth': 135, 'elevation': 79}, roll=107)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot EMOTE Labels (Individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from surfer import Brain\n",
    "%matplotlib qt4\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Make brain.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "subject_id = \"fscopy\"\n",
    "surf = \"inflated\"\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Load files.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "fs_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/freesurfs'\n",
    "subj_dir = os.environ[\"SUBJECTS_DIR\"]\n",
    "rois = ['dacc-lh', 'dacc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "        'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "        'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "labels = [os.path.join(fs_dir,'fscopy','label','april2016','%s.label' %roi) for roi in rois]\n",
    "colors =['#810f7c', '#810f7c', \n",
    "        '#004529', '#004529', '#006837', '#006837', '#238443', '#238443', '#41ab5d', '#41ab5d', \n",
    "         '#78c679', '#78c679', '#addd8e', '#addd8e', '#d9f0a3', '#d9f0a3', '#f7fcb9', '#f7fcb9',\n",
    "        '#8c6bb1', '#8c6bb1', '#9ebcda', '#9ebcda']\n",
    "\n",
    "for roi, label, color in zip(rois,labels,colors):\n",
    "    \n",
    "    hemi = roi.split('-')[-1]\n",
    "    brain = Brain(subject_id, hemi, surf)\n",
    "\n",
    "    brain.remove_labels()\n",
    "    brain.add_label(label, color=color, alpha=1)\n",
    "    \n",
    "    if 'pfc' in roi: view = 'lateral'\n",
    "    else: view = 'medial'\n",
    "    \n",
    "    if hemi == 'lh' and view == 'lateral': brain.show_view({'azimuth': 160, 'elevation': 80}, roll=90, distance=450)\n",
    "    elif hemi == 'rh' and view == 'lateral': brain.show_view({'azimuth': 20, 'elevation': 80}, roll=-85, distance=450)\n",
    "    elif hemi == 'lh' and view == 'medial': brain.show_view({'azimuth': 10, 'elevation': 100}, roll=-85, distance=450)\n",
    "    elif hemi == 'rh' and view == 'medial': brain.show_view({'azimuth': 170, 'elevation': 100}, roll=90, distance=450)\n",
    "    brain.save_image('/space/sophia/2/users/EMOTE-DBS/afMSIT/plots/source/anatomy/%s.png' %roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Sensor Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from mne.io import Raw\n",
    "\n",
    "## Define parameters.\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT_april2016'\n",
    "subject = 'S2'\n",
    "raw_file = os.path.join(root_dir,'raw','%s_msit_raw.fif' %subject)\n",
    "\n",
    "## Open raw. Get channel info.\n",
    "raw = Raw(raw_file, preload=False, verbose=False)\n",
    "chs = raw.info['chs']\n",
    "\n",
    "## Get channel names and locations.\n",
    "ch_info = dict()\n",
    "for d in chs:\n",
    "    if d['loc'].sum():\n",
    "        ch_info[d['ch_name']] = d['loc'][:3] * 1e3\n",
    "        \n",
    "## Iteratively plot.\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,12))\n",
    "for k,v in ch_info.iteritems():\n",
    "    ax.scatter(v[0],v[1],s=125,color='k')\n",
    "    ax.text(v[0]+2,v[1]+2,k,fontsize=16)\n",
    "ax.tick_params(axis='both',which='both',bottom='off',left='off',\n",
    "               labelbottom='off',labelleft='off')\n",
    "plt.suptitle('Representative Eximia Cap', fontsize=30)\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(os.path.join(root_dir,'plots','sensor','eximia_cap.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesing 1: Raw Data\n",
    "### Fixing MEWA: Digitization\n",
    "Something got way messed up. Here we make MNE knows what is EEG and what is extra points.\n",
    "\n",
    "NOTE: Copied over one of the original files for MEWA and renamed it MEWA_msit_unmasked_raw.fif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne.io import Raw\n",
    "from pandas import read_table\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Specify parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT_april2016'\n",
    "raw_file = 'MEWA_msit_unmasked_raw.fif'\n",
    "out_file = 'MEWA_msit_raw.fif'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load and prepare digitizations.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load data. Get digitization from raw.\n",
    "raw = Raw(os.path.join(root_dir,'raw',raw_file),preload=False,verbose=False)\n",
    "digitization = raw.info['dig']\n",
    "\n",
    "## The last 101 points are extra. Set them to kind=4.\n",
    "for d in digitization[-101:]: d['kind'] = 4\n",
    "    \n",
    "## Get coordinates for EEG points (excluding ref/EOG).\n",
    "rr = np.array([d['r'] for d in dig if d['kind']==3])[:-2]\n",
    "\n",
    "## Get channels\n",
    "chs = raw.info['chs']\n",
    "\n",
    "## Update location information. This was a huge pain in the ass to figure out.\n",
    "## We ignore the first four channels (Triggers, EOG) and the last channel (STI014).\n",
    "for ch, r in zip(chs[4:-1], rr): ch['loc'][:3] = r \n",
    "\n",
    "## Update digitization/chs.\n",
    "raw.info['dig'] = digitization\n",
    "raw.info['chs'] = chs\n",
    "raw.save(os.path.join(root_dir,'raw',out_file), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing MEWA: Masking channel jumps\n",
    "Time windows were manually inspected. This step isn't strictly necessary but seemed to help with EOG projections.\n",
    "\n",
    "NOTE: Copied over one of the original files for MEWA and renamed it MEWA_msit_unmasked_raw.fif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from mne.io import Raw, RawArray\n",
    "\n",
    "## Specify parameters.\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT_april2016'\n",
    "raw_file = 'MEWA_msit_unmasked_raw.fif'\n",
    "\n",
    "## Load data.\n",
    "raw = Raw(os.path.join(root_dir,'raw',raw_file),preload=True,verbose=False)\n",
    "\n",
    "## Get data in matrix form.\n",
    "data = raw._data\n",
    "\n",
    "## Get list of usuable channels\n",
    "ch_info = [(n,ch) for n,ch in enumerate(raw.ch_names)]\n",
    "good_ch = [(n,ch) for n,ch in ch_info if ch not in raw.info['bads']]\n",
    "good_ch = np.array(good_ch)[4:-1]\n",
    "\n",
    "## Make mask.\n",
    "mask = np.zeros(data.shape[1])\n",
    "times = [(384,394), (663,669)]\n",
    "for t1, t2 in times:\n",
    "    mask[(raw.times >= t1) & (raw.times <= t2)] += 1\n",
    "mask = mask.astype(bool)\n",
    "\n",
    "## Apply mask.\n",
    "for ch in good_ch[:,0].astype(int):\n",
    "    data[ch,mask] = 0\n",
    "\n",
    "## Make new array. Save.\n",
    "raw = RawArray(data, raw.info, first_samp=raw.first_samp)\n",
    "raw.add_eeg_average_proj()\n",
    "raw.save(os.path.join(root_dir,'raw','MEWA_msit_raw.fif'), overwrite=True, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projections: EOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mne import write_proj\n",
    "from mne.preprocessing import compute_proj_eog\n",
    "from mne.io import Raw\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Setup\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## File params.\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT_april2016'\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "subjects = ['MEWA']\n",
    "\n",
    "# NOTE: Not all subjects work with EOG channel = EOG. \n",
    "# Some require other frontal channels due to concatenation.\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "for subj in subjects:    \n",
    "    print 'Making EOG file for %s.' %subj\n",
    "    \n",
    "    ## Load files.\n",
    "    raw_file  = os.path.join( root_dir, 'raw', '%s_msit_raw.fif' %subj )\n",
    "    raw = Raw(raw_file, preload=True, verbose=False, add_eeg_ref=False)\n",
    "    raw.del_proj(0)\n",
    "    ## Make EOG proj. Save.\n",
    "    proj, _ = compute_proj_eog(raw, n_eeg = 4, average=True, filter_length='20s',\n",
    "                               reject=dict(eeg=5e-4), flat=dict(eeg=5e-8),  ch_name='F2', n_jobs=3)\n",
    "    write_proj(os.path.join( root_dir, 'raw', '%s_msit_eog-proj.fif' %subj ), proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projections: ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mne import read_proj, write_proj\n",
    "from mne.preprocessing import compute_proj_ecg\n",
    "from mne.io import Raw\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Setup\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## File params.\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT_april2016'\n",
    "subjects = ['CHDR']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "for subj in subjects:    \n",
    "    print 'Making ECG file for %s.' %subj\n",
    "    \n",
    "    ## Load files.\n",
    "    raw_file = os.path.join( root_dir, 'raw', '%s_msit_raw.fif' %subj )\n",
    "    eog_file = os.path.join( root_dir, 'raw', '%s_msit-proj.fif' %subj )\n",
    "    raw = Raw(raw_file, preload=True, verbose=False)\n",
    "    eog_proj = read_proj(eog_file)\n",
    "    raw.add_proj(eog_proj, remove_existing=True)\n",
    "    raw.apply_proj()\n",
    "    \n",
    "    ## Make ECG proj. Save.\n",
    "    ecg_proj, _ = compute_proj_ecg(raw, n_eeg = 4, h_freq = 35., average=True, filter_length='20s',\n",
    "                                reject=dict(eeg=5e-4), flat=dict(eeg=5e-8), ch_name='P9', n_jobs=3)\n",
    "    proj = eog_proj + [ecg for ecg in ecg_proj if ecg['desc'] not in [eog['desc'] for eog in eog_proj]]\n",
    "    write_proj(os.path.join( root_dir, 'raw', '%s_msit-proj.fif' %subj ), proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing 2: Epoching\n",
    "### Make Forward Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mne import read_trans, read_bem_solution, read_source_spaces\n",
    "from mne import make_forward_solution, write_forward_solution\n",
    "from mne.io import Raw\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "\n",
    "## Main loop.\n",
    "root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT_april2016'\n",
    "fs_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/freesurfs'\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    print 'Making forward solution for %s.' %subject\n",
    "    \n",
    "    ## Load files.\n",
    "    raw = Raw(os.path.join(root_dir, 'raw', '%s_msit_raw.fif' %subject), preload=False, verbose=False)\n",
    "    trans = read_trans(os.path.join(fs_dir,subject,'mri','T1-neuromag','sets','COR-%s.fif' %subject))\n",
    "    src = read_source_spaces(os.path.join(fs_dir,subject,'bem','%s-oct-6p-src.fif' %subject), verbose=False)\n",
    "    bem = read_bem_solution(os.path.join(fs_dir,subject,'bem','%s-5120-5120-5120-bem-sol.fif' %subject), verbose=False)\n",
    "    \n",
    "    ## Compute and save forward solution.\n",
    "    make_forward_solution(raw.info, trans, src, bem, fname=os.path.join(root_dir,'fwd','%s_msit-fwd.fif' %subject),\n",
    "                          meg=False, eeg=True, mindist=1.0, overwrite=True, n_jobs=3, verbose=False)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne import compute_covariance, Epochs, EpochsArray, find_events, read_proj, pick_types, set_log_level\n",
    "from mne.io import Raw\n",
    "from pandas import read_csv\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "\n",
    "## Filtering parameters.\n",
    "l_freq = 0.5\n",
    "h_freq = 50\n",
    "l_trans_bandwidth = l_freq / 2.\n",
    "h_trans_bandwidth = 1.0\n",
    "filter_length = '20s'\n",
    "n_jobs = 3\n",
    "\n",
    "## Epoching parameters.\n",
    "event_id = dict( FN=1, FI=2, NN=3, NI=4 )      # Alik's convention, isn't he smart!?\n",
    "tmin = -1.5                                    # Leave some breathing room.\n",
    "tmax =  3.4                                    # Trial is 1900ms, leave 1500ms of room.\n",
    "resp_buffer = 1.5                              # 1500ms on either side of response.\n",
    "baseline = (-0.5,-0.1)\n",
    "reject_tmin = -0.5\n",
    "reject_tmax = 1.9\n",
    "reject   = dict(eeg=150e-6)\n",
    "flat     = dict(eeg=5e-7)\n",
    "detrend = None\n",
    "decim = 1\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load behavior.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT'\n",
    "data_file = os.path.join( root_dir, 'behavior', 'afMSIT_group_data.csv' )\n",
    "df = read_csv(data_file)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load behavior.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for subj in subjects:\n",
    "    \n",
    "    print 'Loading data for %s.' %subj\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    # Define paths.\n",
    "    raw_file  = os.path.join( root_dir, 'raw', '%s_%s_raw.fif' %(subj,task) )\n",
    "    proj_file = os.path.join( root_dir, 'raw', '%s_%s-proj.fif' %(subj,task) )\n",
    "    \n",
    "    # Load data.\n",
    "    raw = Raw(raw_file,preload=True,verbose=False)\n",
    "    proj = read_proj(proj_file)\n",
    "    \n",
    "    ## Add projections.\n",
    "    proj = [p for p in proj if 'ref' not in p['desc']]\n",
    "    raw.add_proj(proj, remove_existing=True)\n",
    "    raw.add_eeg_average_proj()\n",
    "    raw.apply_proj()\n",
    "    print raw.info['projs']\n",
    "    \n",
    "    ## Reduce dataframe to subject.\n",
    "    data = df[df.Subject==subj]\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Make events.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    print 'Identifying events for %s.' %subj,  \n",
    "    events = find_events(raw, stim_channel='Trig1', output='onset', min_duration=0.25, verbose=False)\n",
    "\n",
    "    # Error catching.\n",
    "    if data.shape[0] != events.shape[0]: raise ValueError('Mismatching number of stimulus onsets!')\n",
    "    print '%s events found.' %events.shape[0]\n",
    "    \n",
    "    # Update event identifiers.\n",
    "    n = 1\n",
    "    for dbs in [0,1]:\n",
    "        for cond in [0,1]:\n",
    "            ix, = np.where((data.DBS==dbs)&(data.Interference==cond))\n",
    "            events[ix,-1] = n\n",
    "            n+=1\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Filter\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    print 'Applying bandpass filter to raw [%s, %s].' %(l_freq, h_freq)\n",
    "    \n",
    "    Fs = raw.info['sfreq']\n",
    "    raw.filter(l_freq = l_freq, h_freq = h_freq, filter_length=filter_length, n_jobs=n_jobs,\n",
    "               l_trans_bandwidth=l_trans_bandwidth, h_trans_bandwidth=h_trans_bandwidth)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Make stimulus-locked epochs.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    # Build initial epochs object.\n",
    "    picks = pick_types(raw.info, meg=False, eeg=True, exclude='bads')\n",
    "    epochs = Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=baseline, picks=picks,\n",
    "                    reject=reject, flat=flat, reject_tmin=reject_tmin, reject_tmax=reject_tmax, \n",
    "                    proj=True, detrend=detrend, decim=decim)\n",
    "    \n",
    "    # First round of rejections.\n",
    "    epochs.drop_bad()                                                       # Remove bad epochs.\n",
    "    copy = data.ix[[True if not log else False for log in epochs.drop_log]] # Update CSV based on rejections.\n",
    "    \n",
    "    '''NOTE: Making a new dataframe copy is just a shortcut for easy indexing between the Pandas \n",
    "       DataFrame and the Epochs object. This is due to the three rounds of rejections being \n",
    "       applied to the data (e.g. amplitude, behavior exclusion, equalization).'''\n",
    "    \n",
    "    # Drop epochs based on behavior.\n",
    "    epochs.drop(copy.Exclude.astype(bool))\n",
    "    \n",
    "    data = data.ix[[True if not log else False for log in epochs.drop_log]]\n",
    "    print '%s trials remain after rejections.' %(len(epochs))\n",
    "    print epochs\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Make Response-locked epochs.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    print 'Making response-locked epochs.'\n",
    "\n",
    "    # Build response-locked events.\n",
    "    response_indices = raw.time_as_index(0.4 + data.RT)             # Compensating for MSIT-lock.\n",
    "    response_events  = epochs.events.copy()\n",
    "    response_events[:,0] = response_events[:,0] + response_indices\n",
    "\n",
    "    # Get data.\n",
    "    arr = epochs.get_data()\n",
    "    times = epochs.times\n",
    "\n",
    "    # Calculate lengths of response-locked epochs.\n",
    "    response_times = data.RT + 0.4                                  # Compensating for MSIT-lock.\n",
    "    response_windows = np.array([response_times-resp_buffer, response_times+resp_buffer]).T\n",
    "\n",
    "    # Iteratively build epochs array.\n",
    "    trials = []\n",
    "    for n in xrange(len(epochs)):\n",
    "        mask = (times >= response_windows[n,0]) & (times <= response_windows[n,1])\n",
    "        trials.append( arr[n,:,mask] )\n",
    "    trials = np.array(trials).swapaxes(1,2)\n",
    "\n",
    "    # Finally, make epochs objects.\n",
    "    resp_epochs = EpochsArray(trials, epochs.info, response_events, tmin=-resp_buffer, event_id=event_id,)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Save data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    print 'Saving epoch files.'\n",
    "    epochs.save(os.path.join(root_dir,'ave','%s_%s_%s_stim-epo.fif' %(subj,task,h_freq)))\n",
    "    resp_epochs.save(os.path.join(root_dir,'ave','%s_%s_%s_resp-epo.fif' %(subj,task,h_freq)))\n",
    "    data.to_csv(os.path.join(root_dir,'ave','%s_%s_%s-epo.csv' %(subj,task,h_freq)), index=False)\n",
    "    \n",
    "    print '\\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\\n'\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Covariance Matrices / Inverse Solutions / Morph Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mne import EpochsArray, read_epochs, read_forward_solution, set_log_level\n",
    "from mne import compute_covariance, write_cov\n",
    "from mne import compute_morph_matrix, read_source_spaces\n",
    "from mne.filter import low_pass_filter\n",
    "from mne.minimum_norm import make_inverse_operator, write_inverse_operator\n",
    "from scipy.io import savemat\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "\n",
    "## Analysis parameters.\n",
    "fmax = 50\n",
    "\n",
    "## Source localization parameters.\n",
    "loose = 0.2\n",
    "depth = 0.8\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Iteratively load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT'\n",
    "fs_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/freesurfs'\n",
    "src = read_source_spaces(os.path.join(fs_dir,'fscopy','bem','fscopy-oct-6p-src.fif'))\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    print 'Processing %s' %subject\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load files.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Load in files.\n",
    "    epo_file = os.path.join(root_dir,'ave','%s_msit_%s_stim-epo.fif' %(subject,fmax))\n",
    "    epochs = read_epochs(epo_file, verbose=False)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Secondary objects.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    fwd = read_forward_solution(os.path.join(root_dir, 'fwd', '%s_%s-fwd.fif' %(subject,task)), \n",
    "                                surf_ori=True, verbose=False)\n",
    "    \n",
    "    ## Compute/save noise covariance matrix & inverse operator.\n",
    "    noise_cov = compute_covariance(epochs, tmin=-0.5, tmax=0.0, method='shrunk', n_jobs=1)\n",
    "    write_cov(os.path.join(root_dir,'cov','%s_%s_%s-cov.fif' %(subject,task,h_freq)), noise_cov)\n",
    "    inv = make_inverse_operator(epochs.info, fwd, noise_cov, loose=loose, depth=depth, verbose=False)\n",
    "    write_inverse_operator(os.path.join(root_dir,'cov','%s_%s_%s-inv.fif' %(subject,task,fmax)), inv)\n",
    "\n",
    "    ## Pre-compute morph matrix.\n",
    "    vertices_from = [inv['src'][n]['vertno'] for n in xrange(2)]\n",
    "    vertices_to = [src[n]['vertno'] for n in xrange(2)]\n",
    "    morph_mat = compute_morph_matrix(subject, 'fsaverage', vertices_from=vertices_from, \n",
    "                                     vertices_to=vertices_to,subjects_dir=fs_dir, smooth=25)\n",
    "    savemat(os.path.join(root_dir, 'morph_maps', '%s-fsaverage_morph.mat' %subject),\n",
    "            mdict=dict(morph_mat=morph_mat))\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Primary Analyses\n",
    "## Sensor Space Analysis\n",
    "### Prepare time domain epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne import read_epochs\n",
    "from mne.filter import low_pass_filter\n",
    "from pandas import read_csv, concat\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "event_id = ['FN', 'FI', 'NN', 'NI']\n",
    "analysis = 'stim'\n",
    "h_freq = 50\n",
    "\n",
    "## Channel definitions. \n",
    "pick_channels = ['FCZ','P10']\n",
    "\n",
    "## Processing parameters.\n",
    "fmax = 15\n",
    "decim = 3\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Iteratively load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT'\n",
    "\n",
    "for n, subject in enumerate(subjects):\n",
    "    \n",
    "    ## Load behavioral data.\n",
    "    csv = read_csv(os.path.join(root_dir,'ave','%s_%s_%s-epo.csv' %(subject,task,h_freq)))\n",
    "\n",
    "    ## Load in epochs.\n",
    "    epo_file = os.path.join(root_dir,'ave','%s_%s_%s_%s-epo.fif' %(subject,task,h_freq,analysis))\n",
    "    epochs = read_epochs(epo_file, verbose=False)\n",
    "    \n",
    "    ## Restrict to times/channels of interest.\n",
    "    if analysis == 'stim': epochs.crop(-0.5,2.0)\n",
    "    elif analysis == 'resp': epochs.crop(-1.0,1.0)\n",
    "    epochs.pick_channels(pick_channels)\n",
    "\n",
    "    ## Extract epochs.\n",
    "    Fs = epochs.info['sfreq']\n",
    "    times = epochs.times\n",
    "    epochs = epochs.get_data().swapaxes(0,1)\n",
    "    \n",
    "    ## Merge.\n",
    "    if not n: \n",
    "        trials = epochs\n",
    "        df = csv\n",
    "    else:\n",
    "        trials = np.concatenate([trials,epochs], axis=1)\n",
    "        df = concat([df,csv])\n",
    "        \n",
    "## Apply lowpass filter. \n",
    "trials = low_pass_filter(trials, Fs, fmax, filter_length='20s', n_jobs=3, verbose=False)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "## Downsample.\n",
    "trials = trials[:,:,::decim]\n",
    "times = times[::decim]\n",
    "\n",
    "## Convert to uV.\n",
    "trials *= 1e6\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Make directories (if not already made). Save.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Make directory.\n",
    "out_dir = os.path.join(root_dir, 'sensor')\n",
    "if not os.path.isdir(out_dir): os.makedirs(out_dir)\n",
    "    \n",
    "## Save data.\n",
    "for arr, ch in zip(trials,pick_channels): np.savez_compressed(os.path.join(out_dir, 'afMSIT_sensor_%s_%s_%s' %(analysis,ch,fmax)),\n",
    "                                                              data=arr, times=times, fmax=fmax)\n",
    "df.to_csv(os.path.join(out_dir, 'afMSIT_sensor_info.csv'), index=False)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare power amplitudes for single-trial epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne import read_epochs\n",
    "from mne.filter import low_pass_filter\n",
    "from mne.time_frequency import single_trial_power\n",
    "from pandas import read_csv\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "event_id = ['FN', 'FI', 'NN', 'NI']\n",
    "h_freq = 50\n",
    "\n",
    "## Channel definitions. \n",
    "pick_channels = ['FCZ','P10']\n",
    "\n",
    "## Time-frequency parameters.\n",
    "fdict = dict(theta = (4,8), alpha = (8,15), beta = (15,30))\n",
    "freqs = np.logspace( np.log10(2), np.log10(50), num=25)\n",
    "n_cycles = 3\n",
    "baseline = (-0.5, -0.1)\n",
    "Fs = 1450.\n",
    "decim = 14\n",
    "n_jobs = 3\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "epo_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/ave'\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/sensor'\n",
    "df = read_csv(os.path.join(root_dir, 'afMSIT_sensor_info.csv'))\n",
    "\n",
    "for ch in pick_channels:\n",
    "\n",
    "    for analysis in ['stim', 'resp']:\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Iteratively load and merge.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        trials = []\n",
    "        for subject in subjects:\n",
    "\n",
    "            ## Load in epochs.\n",
    "            epo_file = os.path.join(epo_dir, '%s_%s_%s_%s-epo.fif' %(subject,task,h_freq,analysis))\n",
    "            epochs = read_epochs(epo_file, verbose=False)\n",
    "\n",
    "            ## Get label index and extract.\n",
    "            epochs.pick_channels([ch])\n",
    "            trials.append(epochs._data.squeeze())\n",
    "\n",
    "        ## Concatenate.\n",
    "        trials = np.concatenate(trials, axis=0)\n",
    "        times = epochs.times\n",
    "\n",
    "        if not df.shape[0] == trials.shape[0]: raise ValueError('Incompatible dimensions!')\n",
    "        \n",
    "         #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Phase-lock removal.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "       \n",
    "        for subject in df.Subject.unique():\n",
    "            for dbs in [0, 1]:\n",
    "                for cond in [0, 1]:\n",
    "                    ix, = np.where((df.Subject==subject)&(df.DBS==dbs)&(df.Interference==cond))\n",
    "                    trials[ix] -= trials[ix].mean(axis=0)\n",
    "                    \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute power.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "                    \n",
    "        print 'Computing single trial power: %s %s.' %(ch, analysis)\n",
    "        trials = np.expand_dims(trials,1)\n",
    "        power = single_trial_power(trials, sfreq=Fs, frequencies=freqs, n_cycles=n_cycles, \n",
    "                                    baseline=None, n_jobs=n_jobs, verbose=False)\n",
    "        power = power.squeeze()\n",
    "        del trials\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Within-trial normalization.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        power = (power.T / np.median(power, axis=-1).T).T # median, not mean\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Prepare baseline normalization (within subject, DBS).\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        ## Compute baseline normalization.\n",
    "        if analysis == 'stim':\n",
    "\n",
    "            ## Make time mask.\n",
    "            mask = (times >= baseline[0]) & (times <= baseline[1])\n",
    "            \n",
    "            ## Iteratively compute over subjects.\n",
    "            blnorm = []\n",
    "            for subject in df.Subject.unique():\n",
    "                \n",
    "                ## Iteratively compute over DBS conditions..\n",
    "                sbl = []\n",
    "                for dbs in [0,1]:\n",
    "\n",
    "                    ix, = np.where((df.Subject==subject)&(df.DBS==dbs))\n",
    "                    sbl.append( np.apply_over_axes(np.median, power[ix][:,:,mask], axes=[0,2]).squeeze() )\n",
    "            \n",
    "                blnorm.append(sbl)\n",
    "            \n",
    "            ## Merge.\n",
    "            blnorm = np.array(blnorm)\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Apply baseline normalization (within subject, DBS).\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        ## Setup index vectors.\n",
    "        n_trials, n_freqs, n_times = power.shape        \n",
    "        _, subj_ix = np.unique(df.Subject, return_inverse=True)\n",
    "        dbs_ix = df.DBS.as_matrix()\n",
    "        trial_ix = np.arange(n_trials) \n",
    "        \n",
    "        ## Main loop.\n",
    "        for i,j,k in zip(trial_ix,subj_ix,dbs_ix):\n",
    "            \n",
    "            for m in xrange(n_times):\n",
    "                \n",
    "                power[i,:,m] /= blnorm[j,k,:]\n",
    "                \n",
    "        if analysis == 'resp': del blnorm\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Final preprocessing steps.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        power = np.log10(power) * 10\n",
    "            \n",
    "        ## Crop times.\n",
    "        if analysis == 'stim': mask = (times >= -0.5) & (times <= 2.0)\n",
    "        elif analysis == 'resp': mask = (times >= -1.0) & (times <= 1.0)\n",
    "        power = power[:, :, mask]\n",
    "        times = times[mask]\n",
    "            \n",
    "        ## Iteratively average, transform, and save.\n",
    "        for k,v in fdict.iteritems():\n",
    "            \n",
    "            ## Average across frequencies.\n",
    "            ix, = np.where((freqs>=v[0])&(freqs<=v[1]))\n",
    "            band = power[:,ix,:].mean(axis=1)\n",
    "        \n",
    "            ## Save.\n",
    "            f = os.path.join(root_dir, 'afMSIT_sensor_%s_%s_%s' %(analysis, ch, k))\n",
    "            np.savez_compressed(f, data=band[:,::decim], times=times[::decim], freqs=freqs, n_cycles=n_cycles)\n",
    "        \n",
    "        del power\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform permutatations\n",
    "Please see: /space/sophia/2/users/EMOTE-DBS/afMSIT/scripts/cmdline/afMSIT_permutations.py\n",
    "\n",
    "### Identify clusters, perform FDR corrections, and assemble results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from mne.stats import fdr_correction\n",
    "from pandas import DataFrame, concat\n",
    "from scipy.ndimage import measurements\n",
    "from scipy.stats import norm\n",
    "\n",
    "def largest_cluster(arr, threshold):\n",
    "    masked = np.abs( arr ) > threshold\n",
    "    clusters, n_clusters = measurements.label(masked)\n",
    "    cluster_sums = measurements.sum(arr, clusters, index=np.arange(n_clusters)+1)\n",
    "    if not len(cluster_sums): return 0\n",
    "    else: return np.abs(cluster_sums).max()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Specify parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## File parameters.\n",
    "labels = ['FCZ']\n",
    "analysis = 'resp'\n",
    "model_name = 'revised'\n",
    "freqs = ['theta','alpha','beta']\n",
    "domain = ['timedomain', 'frequency'][1]\n",
    "\n",
    "## Statistics parameters.\n",
    "alpha = 0.05\n",
    "min_cluster = 0.05 # seconds\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Initial preparations.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define contrasts.\n",
    "if model_name == 'revised': cols = ['Intercept', 'DBS', 'Interference', 'DBSxInt', 'nsArousal', 'nsValence', 'Trial']\n",
    "\n",
    "## Define threshold.\n",
    "threshold = norm.ppf(1 - alpha/2.)\n",
    "\n",
    "## Read in seeds.\n",
    "f = '/space/sophia/2/users/EMOTE-DBS/afMSIT/scripts/cmdline/seeds.txt'\n",
    "with open(f, 'r') as f: seeds = [s.strip() for s in f.readlines()]\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#    \n",
    "\n",
    "df = []\n",
    "    \n",
    "for label in labels:\n",
    "    \n",
    "    for freq in freqs:\n",
    "    \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Load files. Assemble permutations.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT/sensor'\n",
    "        results_dir = os.path.join(root_dir, model_name)\n",
    "        out_dir = os.path.join(root_dir, 'results')\n",
    "\n",
    "        ## Load true statistics.\n",
    "        npz = np.load(os.path.join(results_dir, 'afMSIT_sensor_%s_%s_%s_obs.npz' %(analysis, label, freq)))\n",
    "        t_scores = npz['t_scores'].squeeze()\n",
    "        times = npz['times'].squeeze()\n",
    "\n",
    "        ## Load null statistics.\n",
    "        for n, seed in enumerate(seeds):\n",
    "            npz = np.load(os.path.join(results_dir, 'afMSIT_sensor_%s_%s_%s_%s.npz' %(analysis, label, freq, seed)))\n",
    "            if not n: permuted = npz['t_scores']\n",
    "            else: permuted = np.concatenate([permuted, npz['t_scores']], axis=0)\n",
    "                \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute cluster statistics.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ## Get info.\n",
    "        n_shuffles, n_eff, n_times  = permuted.shape\n",
    "\n",
    "        ## Iteratively compute clusters.\n",
    "        results = defaultdict(list)\n",
    "\n",
    "        for n, con in enumerate(cols):\n",
    "\n",
    "            ## Find real clusters.\n",
    "            masked = np.abs( t_scores[n] ) > threshold\n",
    "            clusters, n_clusters = measurements.label(masked)\n",
    "            cluster_sums = measurements.sum(t_scores[n], clusters, index=np.arange(n_clusters)+1)\n",
    "\n",
    "            ## Compute null cluster sums.\n",
    "            null_sums = np.array([largest_cluster(permuted[m, n, :], threshold) for m in xrange(n_shuffles)])\n",
    "\n",
    "            ## Compute cluster bounds.\n",
    "            tmin = np.array([times[clusters==i].min() for i in np.arange(n_clusters)+1])\n",
    "            tmax = np.array([times[clusters==i].max() for i in np.arange(n_clusters)+1])\n",
    "\n",
    "            ## Find proportion of clusters that are larger.\n",
    "            p_values = [(np.abs(cs) < null_sums).mean() for cs in cluster_sums]\n",
    "\n",
    "            ## Store results.\n",
    "            for t1, t2, cs, pval in zip(tmin,tmax,cluster_sums,p_values):\n",
    "                results['Freq'] += [freq]\n",
    "                results['Label'] += [label]\n",
    "                results['Contrast'] += [con]\n",
    "                results['Tmin'] += [t1]\n",
    "                results['Tmax'] += [t2]\n",
    "                results['Score'] += [cs]\n",
    "                results['Pval'] += [pval]\n",
    "\n",
    "        ## Organize results and append.\n",
    "        results = DataFrame(results)\n",
    "        results['Tdiff'] = results['Tmax'] - results['Tmin']\n",
    "        results = results[results['Tdiff']>=min_cluster]\n",
    "        df.append(results)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute cluster statistics.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Merge dataframes.\n",
    "df = concat(df)\n",
    "\n",
    "## Organize columns and sort.\n",
    "cols = ['Contrast','Label','Freq','Tmin','Tmax','Tdiff','Score','Pval']\n",
    "df = df[cols].sort_values(['Contrast','Tmin']).reset_index(drop=True)\n",
    "\n",
    "## FDR correct within contrasts.\n",
    "df['FDR'] = 0\n",
    "for contrast in df.Contrast.unique():\n",
    "    _, fdr = fdr_correction(df.loc[df.Contrast==contrast,'Pval'], alpha)\n",
    "    df.loc[df.Contrast==contrast,'FDR'] = fdr\n",
    "    \n",
    "## Save.\n",
    "f = os.path.join(out_dir, '%s_%s_%s_results.csv' %(model_name, analysis,domain))\n",
    "df.to_csv(f, index=False)\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Space Analysis\n",
    "### Source localize single trial epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from mne import read_epochs, read_label, read_source_spaces, set_log_level\n",
    "from mne.minimum_norm import apply_inverse_epochs, read_inverse_operator\n",
    "from scipy.io import loadmat\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "analysis = 'resp'\n",
    "parc = 'april2016'\n",
    "fmax = 50\n",
    "\n",
    "## Source localization parameters.\n",
    "method = 'dSPM'\n",
    "snr = 1.0  \n",
    "lambda2 = 1.0 / snr ** 2\n",
    "pick_ori = 'normal'\n",
    "\n",
    "## Labels\n",
    "rois = ['dacc-lh', 'dacc-rh', 'dmpfc-lh', 'dmpfc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "        'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "        'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Iteratively load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT'\n",
    "fs_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/freesurfs'\n",
    "\n",
    "## Prepare fsaverage source space.\n",
    "src = read_source_spaces(os.path.join(fs_dir,'fscopy','bem','fscopy-oct-6p-src.fif'))\n",
    "vertices_to = [src[n]['vertno'] for n in xrange(2)]\n",
    "labels = [read_label(os.path.join(fs_dir,'fscopy','label','april2016','%s.label' %roi), subject='fsaverage')\n",
    "          for roi in rois]\n",
    "\n",
    "for subject in subjects:\n",
    "\n",
    "    print 'Performing source localization: %s' %subject\n",
    "\n",
    "    ## Load in epochs.\n",
    "    epochs = read_epochs(os.path.join(root_dir,'ave','%s_%s_%s_%s-epo.fif' %(subject,task,fmax,analysis)))\n",
    "    times = epochs.times\n",
    "    \n",
    "    ## Load in secondary files.\n",
    "    inv = read_inverse_operator(os.path.join(root_dir,'cov','%s_%s_%s-inv.fif' %(subject,task,fmax)))\n",
    "    morph_mat = loadmat(os.path.join(root_dir, 'morph_maps', '%s-fsaverage_morph.mat' %subject))['morph_mat']\n",
    "\n",
    "    ## Make generator object.\n",
    "    G = apply_inverse_epochs(epochs, inv, method=method, lambda2=lambda2, pick_ori=pick_ori, return_generator=True)\n",
    "    del epochs, inv\n",
    "\n",
    "    ## Iteratively compute and store label timecourse. \n",
    "    ltcs = []\n",
    "    for g in G:\n",
    "        g = g.morph_precomputed('fsaverage', vertices_to=vertices_to, morph_mat=morph_mat)\n",
    "        ltcs.append( g.extract_label_time_course(labels, src, mode='pca_flip') )\n",
    "    ltcs = np.array(ltcs)\n",
    "    \n",
    "    ## Save.\n",
    "    f = os.path.join(root_dir,'source','stcs','%s_%s_%s_%s_%s_epochs' %(subject,task,analysis,method,fmax))\n",
    "    np.savez_compressed(f, ltcs=ltcs, times=times, labels=np.array([l.name for l in labels]))\n",
    "    del ltcs\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reassemble source localized epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne.filter import low_pass_filter\n",
    "from mne.time_frequency import single_trial_power\n",
    "from pandas import read_csv\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Data parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "method = 'dSPM'\n",
    "h_freq = 50\n",
    "\n",
    "## Label parameters.\n",
    "rois = ['dacc-lh', 'dacc-rh', 'dmpfc-lh', 'dmpfc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "        'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "        'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "\n",
    "## Processing parameters.\n",
    "fmax = 15\n",
    "sfreq = 1450.\n",
    "decim = 3\n",
    "n_jobs = 3\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/source'\n",
    "df = read_csv(os.path.join(root_dir, 'afMSIT_source_info.csv'))\n",
    "\n",
    "for analysis in ['stim', 'resp']:\n",
    "\n",
    "    for roi in rois:\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Iteratively load and merge.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ltcs = []\n",
    "        for subject in subjects:\n",
    "\n",
    "            ## Load NPZ.\n",
    "            npz = np.load(os.path.join(root_dir, 'stcs', '%s_msit_%s_%s_%s_epochs.npz' %(subject,analysis,method,h_freq)))\n",
    "\n",
    "            ## Get label index and extract.\n",
    "            ix = npz['labels'].tolist().index(roi)\n",
    "            arr = npz['ltcs'][:,ix,:]\n",
    "\n",
    "            ## Append.\n",
    "            ltcs.append(arr)\n",
    "\n",
    "        ## Concatenate.\n",
    "        ltcs = np.concatenate(ltcs, axis=0)\n",
    "        times = npz['times']\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Time-domain processing.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ## Make ERP objects.\n",
    "        ERPs = low_pass_filter(ltcs, sfreq, fmax, filter_length='20s', n_jobs=n_jobs)\n",
    "\n",
    "        ## Crop times.\n",
    "        if analysis == 'stim': mask = (times >= -0.5) & (times <= 2.0)\n",
    "        elif analysis == 'resp': mask = (times >= -1.0) & (times <= 1.0)\n",
    "        ERPs = ERPs[:, mask]\n",
    "        times = times[mask]\n",
    "        \n",
    "        ## Save.\n",
    "        np.savez_compressed(os.path.join(root_dir, 'afMSIT_source_%s_%s_%s' %(analysis,roi,fmax)), \n",
    "                            data=ERPs[:,::decim], times=times[::decim], method=method)\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute power of source localized epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne.filter import low_pass_filter\n",
    "from mne.time_frequency import single_trial_power\n",
    "from pandas import read_csv\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Data parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "method = 'dSPM'\n",
    "h_freq = 50\n",
    "\n",
    "## Label parameters.\n",
    "rois = ['dacc-lh', 'dacc-rh', 'dmpfc-lh', 'dmpfc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "        'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "        'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "\n",
    "## Time-frequency parameters.\n",
    "fdict = dict(theta = (4,8), alpha = (8,15), beta = (15,30))\n",
    "freqs = np.logspace( np.log10(2), np.log10(50), num=25)\n",
    "n_cycles = 3\n",
    "baseline = (-0.5, -0.1)\n",
    "Fs = 1450.\n",
    "decim = 14\n",
    "n_jobs = 3\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/source'\n",
    "df = read_csv(os.path.join(root_dir, 'afMSIT_source_info.csv'))\n",
    "\n",
    "for roi in rois:\n",
    "\n",
    "    for analysis in ['stim', 'resp']:\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Iteratively load and merge.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ltcs = []\n",
    "        for subject in subjects:\n",
    "\n",
    "            ## Load NPZ.\n",
    "            npz = np.load(os.path.join(root_dir, 'stcs', '%s_msit_%s_%s_%s_epochs.npz' %(subject,analysis,method,h_freq)))\n",
    "\n",
    "            ## Get label index and extract.\n",
    "            ix = npz['labels'].tolist().index(roi)\n",
    "            arr = npz['ltcs'][:,ix,:]\n",
    "\n",
    "            ## Append.\n",
    "            ltcs.append(arr)\n",
    "\n",
    "        ## Concatenate.\n",
    "        ltcs = np.concatenate(ltcs, axis=0)\n",
    "        times = npz['times']\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Phase-lock removal.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "       \n",
    "        for subject in df.Subject.unique():\n",
    "            for dbs in [0, 1]:\n",
    "                for cond in [0, 1]:\n",
    "                    ix, = np.where((df.Subject==subject)&(df.DBS==dbs)&(df.Interference==cond))\n",
    "                    ltcs[ix] -= ltcs[ix].mean(axis=0)\n",
    "                    \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute power.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "                    \n",
    "        print 'Computing single trial power: %s %s.' %(roi, analysis)\n",
    "        ltcs = np.expand_dims(ltcs,1)\n",
    "        power = single_trial_power(ltcs, sfreq=Fs, frequencies=freqs, n_cycles=n_cycles, \n",
    "                                    baseline=None, n_jobs=n_jobs, verbose=False)\n",
    "        power = power.squeeze()\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Within-trial normalization.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        power = (power.T / np.median(power, axis=-1).T).T # median, not mean\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Prepare baseline normalization (within subject, DBS).\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        ## Compute baseline normalization.\n",
    "        if analysis == 'stim':\n",
    "\n",
    "            ## Make time mask.\n",
    "            mask = (times >= baseline[0]) & (times <= baseline[1])\n",
    "            \n",
    "            ## Iteratively compute over subjects.\n",
    "            blnorm = []\n",
    "            for subject in df.Subject.unique():\n",
    "                \n",
    "                ## Iteratively compute over DBS conditions..\n",
    "                sbl = []\n",
    "                for dbs in [0,1]:\n",
    "\n",
    "                    ix, = np.where((df.Subject==subject)&(df.DBS==dbs))\n",
    "                    sbl.append( np.apply_over_axes(np.median, power[ix][:,:,mask], axes=[0,2]).squeeze() )\n",
    "            \n",
    "                blnorm.append(sbl)\n",
    "            \n",
    "            ## Merge.\n",
    "            blnorm = np.array(blnorm)\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Apply baseline normalization (within subject, DBS).\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        ## Setup index vectors.\n",
    "        n_trials, n_freqs, n_times = power.shape        \n",
    "        _, subj_ix = np.unique(df.Subject, return_inverse=True)\n",
    "        dbs_ix = df.DBS.as_matrix()\n",
    "        trial_ix = np.arange(n_trials) \n",
    "        \n",
    "        ## Main loop.\n",
    "        for i,j,k in zip(trial_ix,subj_ix,dbs_ix):\n",
    "            \n",
    "            for m in xrange(n_times):\n",
    "                \n",
    "                power[i,:,m] /= blnorm[j,k,:]\n",
    "                \n",
    "        if analysis == 'resp': del blnorm\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Final preprocessing steps.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            \n",
    "        ## Convert to decibels.\n",
    "        power = np.log10(power) * 10\n",
    "            \n",
    "        ## Crop times.\n",
    "        if analysis == 'stim': mask = (times >= -0.5) & (times <= 2.0)\n",
    "        elif analysis == 'resp': mask = (times >= -1.0) & (times <= 1.0)\n",
    "        power = power[:, :, mask]\n",
    "        times = times[mask]\n",
    "            \n",
    "        ## Iteratively average, transform, and save.\n",
    "        for k,v in fdict.iteritems():\n",
    "            \n",
    "            ## Average across frequencies.\n",
    "            ix, = np.where((freqs>=v[0])&(freqs<=v[1]))\n",
    "            band = power[:,ix,:].mean(axis=1)\n",
    "        \n",
    "            ## Save.\n",
    "            f = os.path.join(root_dir, 'afMSIT_source_%s_%s_%s' %(analysis, roi, k))\n",
    "            np.savez_compressed(f, data=band[:,::decim], times=times[::decim], freqs=freqs, n_cycles=n_cycles)\n",
    "        \n",
    "        del power\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform permutatations\n",
    "Please see: /space/sophia/2/users/EMOTE-DBS/afMSIT/scripts/cmdline/afMSIT_permutations.py\n",
    "\n",
    "### Identify clusters, perform FDR corrections, and assemble results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from mne.stats import fdr_correction\n",
    "from pandas import DataFrame, concat\n",
    "from scipy.ndimage import measurements\n",
    "from scipy.stats import norm\n",
    "\n",
    "def largest_cluster(arr, threshold):\n",
    "    masked = np.abs( arr ) > threshold\n",
    "    clusters, n_clusters = measurements.label(masked)\n",
    "    cluster_sums = measurements.sum(arr, clusters, index=np.arange(n_clusters)+1)\n",
    "    if not len(cluster_sums): return 0\n",
    "    else: return np.abs(cluster_sums).max()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Specify parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## File parameters.\n",
    "labels = ['dacc-lh', 'dacc-rh', 'dmpfc-lh', 'dmpfc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "          'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "          'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "\n",
    "analysis = 'resp'\n",
    "model_name = 'revised'\n",
    "freqs = ['theta', 'alpha', 'beta']\n",
    "domain = ['timedomain', 'frequency'][1]\n",
    "\n",
    "## Statistics parameters.\n",
    "alpha = 0.05\n",
    "min_cluster = 0.05 # seconds\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Initial preparations.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define contrasts.\n",
    "if model_name == 'revised': cols = ['Intercept', 'DBS', 'Interference', 'DBSxInt', 'nsArousal', 'nsValence', 'Trial']\n",
    "\n",
    "## Define threshold.\n",
    "threshold = norm.ppf(1 - alpha/2.)\n",
    "\n",
    "## Read in seeds.\n",
    "f = '/space/sophia/2/users/EMOTE-DBS/afMSIT/scripts/cmdline/seeds.txt'\n",
    "with open(f, 'r') as f: seeds = [s.strip() for s in f.readlines()]\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#    \n",
    "\n",
    "df = []\n",
    "    \n",
    "for label in labels:\n",
    "    \n",
    "    for freq in freqs:\n",
    "    \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Load files. Assemble permutations.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT/source'\n",
    "        results_dir = os.path.join(root_dir, model_name)\n",
    "        out_dir = os.path.join(root_dir, 'results')\n",
    "\n",
    "        ## Load true statistics.\n",
    "        npz = np.load(os.path.join(results_dir, 'afMSIT_source_%s_%s_%s_obs.npz' %(analysis, label, freq)))\n",
    "        t_scores = npz['t_scores'].squeeze()\n",
    "        times = npz['times'].squeeze()\n",
    "\n",
    "        ## Load null statistics.\n",
    "        for n, seed in enumerate(seeds):\n",
    "            npz = np.load(os.path.join(results_dir, 'afMSIT_source_%s_%s_%s_%s.npz' %(analysis, label, freq, seed)))\n",
    "            if not n: permuted = npz['t_scores']\n",
    "            else: permuted = np.concatenate([permuted, npz['t_scores']], axis=0)\n",
    "                \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute cluster statistics.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ## Get info.\n",
    "        n_shuffles, n_eff, n_times  = permuted.shape\n",
    "\n",
    "        ## Iteratively compute clusters.\n",
    "        results = defaultdict(list)\n",
    "\n",
    "        for n, con in enumerate(cols):\n",
    "\n",
    "            ## Find real clusters.\n",
    "            masked = np.abs( t_scores[n] ) > threshold\n",
    "            clusters, n_clusters = measurements.label(masked)\n",
    "            cluster_sums = measurements.sum(t_scores[n], clusters, index=np.arange(n_clusters)+1)\n",
    "\n",
    "            ## Compute null cluster sums.\n",
    "            null_sums = np.array([largest_cluster(permuted[m, n, :], threshold) for m in xrange(n_shuffles)])\n",
    "\n",
    "            ## Compute cluster bounds.\n",
    "            tmin = np.array([times[clusters==i].min() for i in np.arange(n_clusters)+1])\n",
    "            tmax = np.array([times[clusters==i].max() for i in np.arange(n_clusters)+1])\n",
    "\n",
    "            ## Find proportion of clusters that are larger.\n",
    "            p_values = [((np.abs(cs) < null_sums).sum() + 1.) / (null_sums.shape[0] + 1.) for cs in cluster_sums]\n",
    "            \n",
    "            ## Store results.\n",
    "            for t1, t2, cs, pval in zip(tmin,tmax,cluster_sums,p_values):\n",
    "                results['Freq'] += [freq]\n",
    "                results['Label'] += [label]\n",
    "                results['Contrast'] += [con]\n",
    "                results['Tmin'] += [t1]\n",
    "                results['Tmax'] += [t2]\n",
    "                results['Score'] += [cs]\n",
    "                results['Pval'] += [pval]\n",
    "\n",
    "        ## Organize results and append.\n",
    "        results = DataFrame(results)\n",
    "        results['Tdiff'] = results['Tmax'] - results['Tmin']\n",
    "        results = results[results['Tdiff']>=min_cluster]\n",
    "        df.append(results)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute cluster statistics.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Merge dataframes.\n",
    "df = concat(df)\n",
    "\n",
    "## Organize columns and sort.\n",
    "cols = ['Contrast','Label','Freq','Tmin','Tmax','Tdiff','Score','Pval']\n",
    "df = df[cols].sort_values(['Contrast','Tmin']).reset_index(drop=True)\n",
    "\n",
    "## FDR correct within contrasts.\n",
    "df['FDR'] = 0\n",
    "for contrast in df.Contrast.unique():\n",
    "    _, fdr = fdr_correction(df.loc[df.Contrast==contrast,'Pval'], alpha)\n",
    "    df.loc[df.Contrast==contrast,'FDR'] = fdr\n",
    "    \n",
    "## Save.\n",
    "f = os.path.join(out_dir, '%s_%s_%s_results.csv' %(model_name, analysis,domain))\n",
    "df.to_csv(f, index=False)\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Secondary Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Regressions on DLPFC-5 LH \n",
    "Here we look for the fraction of subjects that show the same DBSon/DBSoff effects in DLPFC-5 LH at the single-subject level.\n",
    "### Perform permutations\n",
    "Please see: /space/sophia/2/users/EMOTE-DBS/afMSIT/scripts/cmdline/subject_permutations.py\n",
    "### Identify clusters, perform FDR corrections, and assemble results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from mne.stats import fdr_correction\n",
    "from pandas import DataFrame, concat\n",
    "from scipy.ndimage import measurements\n",
    "from scipy.stats import norm\n",
    "\n",
    "def largest_cluster(arr, threshold):\n",
    "    masked = np.abs( arr ) > threshold\n",
    "    clusters, n_clusters = measurements.label(masked)\n",
    "    cluster_sums = measurements.sum(arr, clusters, index=np.arange(n_clusters)+1)\n",
    "    if not len(cluster_sums): return 0\n",
    "    else: return np.abs(cluster_sums).max()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Specify parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## File parameters.\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "labels = ['dlpfc_5-lh']\n",
    "\n",
    "space = 'source'\n",
    "analysis = 'stim'\n",
    "model_name = 'revised'\n",
    "freqs = ['theta']\n",
    "domain = ['timedomain', 'frequency'][1]\n",
    "\n",
    "## Statistics parameters.\n",
    "alpha = 0.05\n",
    "min_cluster = 0.05 # seconds\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Initial preparations.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define contrasts.\n",
    "if model_name == 'revised': cols = ['Intercept', 'DBS', 'Interference', 'DBSxInt', 'nsArousal', 'nsValence', 'Trial']\n",
    "\n",
    "## Define threshold.\n",
    "threshold = norm.ppf(1 - alpha/2.)\n",
    "\n",
    "## Read in seeds.\n",
    "f = '/space/sophia/2/users/EMOTE-DBS/afMSIT/scripts/cmdline/seeds.txt'\n",
    "with open(f, 'r') as f: seeds = [s.strip() for s in f.readlines()]\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#    \n",
    "\n",
    "df = []\n",
    "    \n",
    "for subject in subjects: \n",
    "        \n",
    "    for label in labels:\n",
    "\n",
    "        for freq in freqs:\n",
    "\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            ### Load files. Assemble permutations.\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT/%s' %space\n",
    "            results_dir = os.path.join(root_dir, model_name)\n",
    "            out_dir = os.path.join(root_dir, 'results')\n",
    "\n",
    "            ## Load true statistics.\n",
    "            npz = np.load(os.path.join(results_dir, '%s_%s_%s_%s_%s_obs.npz' %(subject, space, analysis, label, freq)))\n",
    "            t_scores = npz['t_scores'].squeeze()\n",
    "            times = npz['times'].squeeze()\n",
    "\n",
    "            ## Load null statistics.\n",
    "            for n, seed in enumerate(seeds):\n",
    "                npz = np.load(os.path.join(results_dir, '%s_%s_%s_%s_%s_%s.npz' %(subject, space, analysis, label, freq, seed)))\n",
    "                if not n: permuted = npz['t_scores']\n",
    "                else: permuted = np.concatenate([permuted, npz['t_scores']], axis=0)\n",
    "\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            ### Compute cluster statistics.\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "            ## Get info.\n",
    "            n_shuffles, n_eff, n_times  = permuted.shape\n",
    "\n",
    "            ## Iteratively compute clusters.\n",
    "            results = defaultdict(list)\n",
    "\n",
    "            for n, con in enumerate(cols):\n",
    "\n",
    "                ## Find real clusters.\n",
    "                masked = np.abs( t_scores[n] ) > threshold\n",
    "                clusters, n_clusters = measurements.label(masked)\n",
    "                cluster_sums = measurements.sum(t_scores[n], clusters, index=np.arange(n_clusters)+1)\n",
    "\n",
    "                ## Compute null cluster sums.\n",
    "                null_sums = np.array([largest_cluster(permuted[m, n, :], threshold) for m in xrange(n_shuffles)])\n",
    "\n",
    "                ## Compute cluster bounds.\n",
    "                tmin = np.array([times[clusters==i].min() for i in np.arange(n_clusters)+1])\n",
    "                tmax = np.array([times[clusters==i].max() for i in np.arange(n_clusters)+1])\n",
    "\n",
    "                ## Find proportion of clusters that are larger.\n",
    "                p_values = [((np.abs(cs) < null_sums).sum() + 1.) / (null_sums.shape[0] + 1.) for cs in cluster_sums]\n",
    "\n",
    "                ## Store results.\n",
    "                for t1, t2, cs, pval in zip(tmin,tmax,cluster_sums,p_values):\n",
    "                    results['Subject'] += [subject]\n",
    "                    results['Freq'] += [freq]\n",
    "                    results['Label'] += [label]\n",
    "                    results['Contrast'] += [con]\n",
    "                    results['Tmin'] += [t1]\n",
    "                    results['Tmax'] += [t2]\n",
    "                    results['Score'] += [cs]\n",
    "                    results['Pval'] += [pval]\n",
    "\n",
    "            ## Organize results and append.\n",
    "            results = DataFrame(results)\n",
    "            results['Tdiff'] = results['Tmax'] - results['Tmin']\n",
    "            results = results[results['Tdiff']>=min_cluster]\n",
    "            df.append(results)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute cluster statistics.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Merge dataframes.\n",
    "df = concat(df)\n",
    "\n",
    "## Organize columns and sort.\n",
    "cols = ['Subject','Contrast','Label','Freq','Tmin','Tmax','Tdiff','Score','Pval']\n",
    "df = df[cols].sort_values(['Subject','Contrast','Tmin']).reset_index(drop=True)\n",
    "\n",
    "## FDR correct within contrasts.\n",
    "df['FDR'] = 0\n",
    "for contrast in df.Contrast.unique():\n",
    "    _, fdr = fdr_correction(df.loc[df.Contrast==contrast,'Pval'], alpha)\n",
    "    df.loc[df.Contrast==contrast,'FDR'] = fdr\n",
    "    \n",
    "## Save.\n",
    "f = os.path.join(out_dir, 'subjects_%s_%s_%s_results.csv' %(model_name, analysis, domain))\n",
    "df.to_csv(f, index=False)\n",
    "        \n",
    "print 'Done.'\n",
    "\n",
    "df[(df.Contrast=='DBS')&(df.FDR<0.05)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reaction Time Correlations\n",
    "### Perform permutations\n",
    "Please see: /space/sophia/2/users/EMOTE-DBS/afMSIT/scripts/cmdline/afMSIT_permutations.py\n",
    "### Identify clusters, perform FDR corrections, and assemble results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import concat, read_csv\n",
    "from mne.stats import fdr_correction\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O Parameters.\n",
    "space = 'source'\n",
    "analysis = 'resp'\n",
    "labels = ['dacc-lh', 'dacc-rh', 'dmpfc-lh', 'dmpfc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "          'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "          'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "domain = 'timedomain'\n",
    "freqs = ['15']\n",
    "model = 'revised'\n",
    "\n",
    "## Cluster parameters.\n",
    "min_cluster = 0.05 # secs\n",
    "alpha = 0.05\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/%s/rt' %space\n",
    "results_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/%s/results' %space\n",
    "\n",
    "df = []\n",
    "for label in labels:\n",
    "    \n",
    "    for freq in freqs:\n",
    "        \n",
    "        df.append( read_csv(os.path.join(root_dir, 'afMSIT_%s_%s_%s_%s_rt.csv' %(space, analysis, label, freq))) )\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Merge, trim, correct, and save.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "## Merge and trim.\n",
    "df = concat(df)\n",
    "df = df[df.Tdiff > min_cluster]\n",
    "\n",
    "## FDR correct.\n",
    "df['FDR'] = fdr_correction(df.Pval, alpha=alpha)[-1]\n",
    "\n",
    "## Save.\n",
    "f = os.path.join(results_dir, '%s_%s_%s_rt.csv' %(model, analysis, domain))\n",
    "df.to_csv(f, index=False)\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MADRS Correlations\n",
    "### Permutations for MADRS Correlations\n",
    "Please see: /space/sophia/2/users/EMOTE-DBS/afMSIT/scripts/cmdline/afMSIT_permutations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "np.random.seed(47404)\n",
    "\n",
    "## Define parameters.\n",
    "n_subj = 8\n",
    "n_shuffles = 1000\n",
    "\n",
    "## Prepare\n",
    "ix = np.arange(n_subj)\n",
    "permutations = np.expand_dims(ix[::-1], 0)\n",
    "    \n",
    "while True: \n",
    "    np.random.shuffle(ix)\n",
    "    permutations = np.concatenate([permutations, np.expand_dims(ix, 0)], axis=0)\n",
    "    permutations = DataFrame(permutations).drop_duplicates()\n",
    "    if permutations.shape[0] < n_shuffles: permutations = permutations.as_matrix()\n",
    "    else: break\n",
    "        \n",
    "## Save.\n",
    "np.save('/space/sophia/2/users/EMOTE-DBS/afMSIT/scripts/cmdline/madrs_permutations', permutations.as_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MADRS Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from scipy.stats import spearmanr\n",
    "from mne.stats import fdr_correction\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "space = 'source'\n",
    "analysis = 'stim'\n",
    "domain = 'frequency'\n",
    "model = 'revised'\n",
    "contrast = 'DBS'\n",
    "fdr = 0.05\n",
    "\n",
    "subjects = np.array(['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA'])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Prepare MADRS Scores.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/%s' %space\n",
    "info = read_csv(os.path.join(root_dir, 'afMSIT_%s_info.csv' %space))\n",
    "\n",
    "## Prepare MADRS scores.\n",
    "ratings = read_csv('/space/sophia/2/users/EMOTE-DBS/afMSIT/behavior/Subject_Rating_Scales.csv')\n",
    "ratings = ratings.set_index('Subject')\n",
    "madrs = ratings.loc[subjects, 'MADRS_Now'] - ratings.loc[subjects, 'MADRS_Base']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "results = read_csv(os.path.join(root_dir, 'results', '%s_%s_%s_results.csv' %(model,analysis,domain)))\n",
    "results = results[(results.Contrast==contrast)&(results.FDR<fdr)].reset_index(drop=True)\n",
    "\n",
    "results['Corr_R'] = 0 \n",
    "results['Corr_P'] = 0 \n",
    "\n",
    "for n in xrange(results.shape[0]):\n",
    "    \n",
    "    label = results.loc[n, 'Label']\n",
    "    freq  = results.loc[n, 'Freq']\n",
    "    tmin  = results.loc[n, 'Tmin']\n",
    "    tmax  = results.loc[n, 'Tmax']\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and prepare data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    npz = np.load(os.path.join(root_dir, 'afMSIT_%s_%s_%s_%s.npz' %(space, analysis, label, freq)))\n",
    "    data = npz['data']\n",
    "    times = npz['times']\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute condition differences.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    delta = np.zeros(subjects.shape[0])\n",
    "    mask = (times >= tmin) & (times <= tmax)    \n",
    "    \n",
    "    for m, subject in enumerate(subjects):\n",
    "        i, = np.where((info['Subject']==subject)&(info[contrast]==0))\n",
    "        j, = np.where((info['Subject']==subject)&(info[contrast]==1))\n",
    "        delta[m] += (data[j][:,mask].mean(axis=0) - data[i][:,mask].mean(axis=0)).mean()\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Perform correlations.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Compute true correlations.\n",
    "    r, p = spearmanr(madrs, delta)\n",
    "    if label == 'dlpfc_5-lh': test = delta.copy()\n",
    "    results.loc[n, 'Corr_R'] = r\n",
    "    results.loc[n, 'Corr_P'] = p\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Save.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Save.\n",
    "results['Corr_FDR'] = fdr_correction(results.Corr_P)[-1]\n",
    "results = results.sort_values('Corr_FDR')\n",
    "\n",
    "f = os.path.join(root_dir, 'results', '%s_%s_%s_madrs.csv' %(model, analysis, domain))\n",
    "results.to_csv(f, index=False)\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special MADRS Correlation for DLPFC_5-LH (Stimulus-Locked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from pandas import read_csv\n",
    "from scipy.stats import spearmanr\n",
    "from mne.stats import fdr_correction\n",
    "np.random.seed(47404)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "space = 'source'\n",
    "analysis = 'stim'\n",
    "domain = 'frequency'\n",
    "model = 'revised'\n",
    "contrast = 'DBS'\n",
    "fdr = 0.05\n",
    "\n",
    "## Subject parameters.\n",
    "subjects = np.array(['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2'])\n",
    "\n",
    "## Bootstrapping parameters.\n",
    "n_straps = 1000\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "true_scores = np.zeros((2,2))\n",
    "bootstraps = np.zeros((2,n_straps))\n",
    "\n",
    "## Looping over including and excluding S2.\n",
    "for n in np.arange(2): \n",
    "\n",
    "    if n: subjects = np.delete(subjects, -1)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare MADRS Scores.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/%s' %space\n",
    "    info = read_csv(os.path.join(root_dir, 'afMSIT_%s_info.csv' %space))\n",
    "\n",
    "    ## Prepare MADRS scores.\n",
    "    ratings = read_csv('/space/sophia/2/users/EMOTE-DBS/afMSIT/behavior/Subject_Rating_Scales.csv')\n",
    "    ratings = ratings.set_index('Subject')\n",
    "    madrs = ratings.loc[subjects, 'MADRS_Now'] - ratings.loc[subjects, 'MADRS_Base']\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and prepare data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load cluster results.\n",
    "    results = read_csv(os.path.join(root_dir, 'results', '%s_%s_%s_results.csv' %(model,analysis,domain)))\n",
    "    results = results[(results.Contrast==contrast)&(results.FDR<fdr)].reset_index(drop=True)\n",
    "    results = results[results.Label == 'dlpfc_5-lh'] # Limit to the theta clusters.\n",
    "\n",
    "    ## Load time series data.\n",
    "    npz = np.load(os.path.join(root_dir, 'afMSIT_%s_%s_%s_%s.npz' %(space, analysis, 'dlpfc_5-lh', 'theta')))\n",
    "    data = npz['data']\n",
    "    times = npz['times']\n",
    "\n",
    "    ## Compute condition differences.\n",
    "    delta = np.zeros(subjects.shape[0])\n",
    "    mask = (times >= results.Tmin.min()) & (times <= results.Tmax.min())    \n",
    "\n",
    "    for m, subject in enumerate(subjects):\n",
    "        i, = np.where((info['Subject']==subject)&(info[contrast]==0))\n",
    "        j, = np.where((info['Subject']==subject)&(info[contrast]==1))\n",
    "        delta[m] += (data[j][:,mask].mean(axis=0) - data[i][:,mask].mean(axis=0)).mean()\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Perform correlations.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Compute true correlations.\n",
    "    true_scores[n] = spearmanr(madrs, delta)\n",
    "    \n",
    "    ## Perform bootstrapping\n",
    "    n_subj = subjects.shape[0]\n",
    "    for m in np.arange(n_straps):\n",
    "        \n",
    "        ix = np.random.choice(np.arange(n_subj), n_subj, replace=True)\n",
    "        bootstraps[n,m], _ = spearmanr(madrs[ix], delta[ix])\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plot Scatterplot.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ax = plt.subplot2grid((2,2),(0,n))\n",
    "    if not n: color = '#2b8cbe'\n",
    "    else: color = '#31a354'\n",
    "\n",
    "    ## Iteratively add points.\n",
    "    for x,y,subj in zip(madrs,delta,madrs.index):\n",
    "\n",
    "        ax.scatter(x,y,s=80,color=color)\n",
    "        ax.text(x+0.5,y+0.05,subj)\n",
    "\n",
    "    ## Add trend line.\n",
    "    x1, x2 = ax.get_xlim()\n",
    "    y1, y2 = ax.get_ylim()\n",
    "\n",
    "    z = np.polyfit(madrs, delta, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(np.linspace(x1,x2,1e3), p(np.linspace(x1,x2,1e3)), linewidth=2, color='k', alpha=0.8)\n",
    "    ax.set_xlim(x1,x2)\n",
    "    ax.set_ylim(y1,y2)\n",
    "\n",
    "    ## Add flourishes.\n",
    "    ax.set_xlabel('MADRS [Post - Pre]', fontsize=18)\n",
    "    ax.set_ylabel('Theta Power [On - Off]', fontsize=18)\n",
    "    if not n: ax.set_title('DLPFC-5 LH (S2 included)', fontsize=24)\n",
    "    else: ax.set_title('DLPFC-5 LH (S2 discarded)', fontsize=24)\n",
    "    ax.text(x1+0.5, y2*0.99, 'r = %0.3f\\np = %0.3f' %(true_scores[n,0],true_scores[n,1]), \n",
    "            fontsize=18, va='top')\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plot Histogram.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ax = plt.subplot2grid((2,2),(1,n))\n",
    "\n",
    "    ## Plot histogram.\n",
    "    x = bootstraps[n]\n",
    "    x = x[~np.isnan(x)]\n",
    "    ax.hist(x, bins=40, color=color)\n",
    "\n",
    "    ## Add true score.\n",
    "    y1, y2 = ax.get_ylim()\n",
    "    ax.vlines(true_scores[n,0], y1, y2, linestyle='--', linewidth=2)\n",
    "    ax.text(true_scores[n,0], y2*0.98, 'r = %0.3f' %true_scores[n,0], \n",
    "            ha='right', va='top', rotation='vertical', weight='heavy')\n",
    "    \n",
    "    ## Add flourishes.\n",
    "    ax.set_xlabel('Spearman R', fontsize=18) \n",
    "    ax.set_ylabel('Frequency', fontsize=18)\n",
    "    if not n: ax.set_title('%s Bootstraps (S2 included)' %n_straps, fontsize=20)\n",
    "    else: ax.set_title('%s Bootstraps (S2 discarded)' %n_straps, fontsize=20)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Save.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#        \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Hypomania Correlation for DLPFC_5-LH (Stimulus-Locked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from pandas import read_csv\n",
    "from scipy.stats import spearmanr\n",
    "from mne.stats import fdr_correction\n",
    "np.random.seed(47404)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "space = 'source'\n",
    "analysis = 'stim'\n",
    "domain = 'frequency'\n",
    "model = 'revised'\n",
    "contrast = 'DBS'\n",
    "fdr = 0.05\n",
    "\n",
    "## Subject parameters.\n",
    "subjects = np.array(['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA'])\n",
    "\n",
    "## Bootstrapping parameters.\n",
    "n_straps = 1000\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Prepare MADRS Scores.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/%s' %space\n",
    "info = read_csv(os.path.join(root_dir, 'afMSIT_%s_info.csv' %space))\n",
    "\n",
    "## Prepare MADRS scores.\n",
    "ratings = read_csv('/space/sophia/2/users/EMOTE-DBS/afMSIT/behavior/Subject_Characteristics.csv')\n",
    "ratings = ratings.set_index('Name')\n",
    "mania = ratings.loc[subjects, 'Hypomania']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load cluster results.\n",
    "results = read_csv(os.path.join(root_dir, 'results', '%s_%s_%s_results.csv' %(model,analysis,domain)))\n",
    "results = results[(results.Contrast==contrast)&(results.FDR<fdr)].reset_index(drop=True)\n",
    "results = results[results.Label == 'dlpfc_5-lh'] # Limit to the theta clusters.\n",
    "\n",
    "## Load time series data.\n",
    "npz = np.load(os.path.join(root_dir, 'afMSIT_%s_%s_%s_%s.npz' %(space, analysis, 'dlpfc_5-lh', 'theta')))\n",
    "data = npz['data']\n",
    "times = npz['times']\n",
    "\n",
    "## Compute condition differences.\n",
    "delta = np.zeros(subjects.shape[0])\n",
    "mask = (times >= results.Tmin.min()) & (times <= results.Tmax.min())    \n",
    "\n",
    "for m, subject in enumerate(subjects):\n",
    "    i, = np.where((info['Subject']==subject)&(info[contrast]==0))\n",
    "    j, = np.where((info['Subject']==subject)&(info[contrast]==1))\n",
    "    delta[m] += (data[j][:,mask].mean(axis=0) - data[i][:,mask].mean(axis=0)).mean()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Perform correlations.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Compute true correlations.\n",
    "true_scores = spearmanr(mania, delta)\n",
    "\n",
    "## Perform bootstrapping\n",
    "n_subj = subjects.shape[0]\n",
    "bootstraps = np.zeros(n_straps)\n",
    "\n",
    "for m in np.arange(n_straps):\n",
    "\n",
    "    ix = np.random.choice(np.arange(n_subj), n_subj, replace=True)\n",
    "    bootstraps[m], _ = spearmanr(mania[ix], delta[ix])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot Scatterplot.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = plt.subplot2grid((1,2),(0,0))\n",
    "color = '#2b8cbe'\n",
    "\n",
    "## Iteratively add points.\n",
    "for x,y,subj in zip(mania,delta,mania.index):\n",
    "\n",
    "    ax.scatter(x,y,s=80,color=color)\n",
    "    ax.text(x+0.01,y+0.05,subj)\n",
    "\n",
    "## Add trend line.\n",
    "x1, x2 = ax.get_xlim()\n",
    "y1, y2 = ax.get_ylim()\n",
    "\n",
    "z = np.polyfit(mania, delta, 1)\n",
    "p = np.poly1d(z)\n",
    "ax.plot(np.linspace(x1,x2,1e3), p(np.linspace(x1,x2,1e3)), linewidth=2, color='k', alpha=0.8)\n",
    "ax.set_xlim(x1,x2)\n",
    "ax.set_ylim(y1,y2)\n",
    "\n",
    "## Add flourishes.\n",
    "ax.set_xlabel('Hypomania', fontsize=18)\n",
    "ax.set_ylabel('Theta Power [On - Off]', fontsize=18)\n",
    "ax.set_title('DLPFC-5 LH', fontsize=24)\n",
    "ax.text(x1+0.01, y2*0.99, 'r = %0.3f\\np = %0.3f' %(true_scores[0],true_scores[1]), \n",
    "        fontsize=18, va='top')\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot Histogram.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "ax = plt.subplot2grid((1,2),(0,1))\n",
    "\n",
    "## Plot histogram.\n",
    "x = bootstraps[~np.isnan(bootstraps)]\n",
    "ax.hist(x, bins=40, color=color)\n",
    "\n",
    "## Add true score.\n",
    "y1, y2 = ax.get_ylim()\n",
    "ax.vlines(true_scores[0], y1, y2, linestyle='--', linewidth=2)\n",
    "ax.text(true_scores[0], y2*0.98, 'r = %0.3f' %true_scores[0], \n",
    "        ha='right', va='top', rotation='vertical', weight='heavy')\n",
    "\n",
    "## Add flourishes.\n",
    "ax.set_xlabel('Spearman R', fontsize=18) \n",
    "ax.set_ylabel('Frequency', fontsize=18)\n",
    "ax.set_title('%s Bootstraps' %n_straps, fontsize=20)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Save.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#        \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intertrial Interval Spectra Comparison\n",
    "### Make ITI epochs\n",
    "Each trial is 1.93s. On average 5.6s between each trial. Minimum time between trials is 3.0s between trials. We take 2s windows of time, centered at the time between trials, using the baseline for the proceeding trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne import compute_covariance, Epochs, EpochsArray, find_events, read_proj, pick_types, set_log_level\n",
    "from mne.io import Raw\n",
    "from pandas import read_csv\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU', 'CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "\n",
    "## Filtering parameters.\n",
    "l_freq = 0.5\n",
    "h_freq = 50\n",
    "l_trans_bandwidth = l_freq / 2.\n",
    "h_trans_bandwidth = 1.0\n",
    "filter_length = '20s'\n",
    "n_jobs = 3\n",
    "\n",
    "## Epoching parameters.\n",
    "event_id = dict( FN=1, FI=2, NN=3, NI=4 )      # Alik's convention, isn't he smart!?\n",
    "tmin = -1.25\n",
    "tmax = 1.25\n",
    "baseline = (-0.5, -0.1)\n",
    "reject   = dict(eeg=150e-6)\n",
    "flat     = dict(eeg=5e-7)\n",
    "detrend = None\n",
    "decim = 1\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load behavior.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT'\n",
    "data_file = os.path.join( root_dir, 'behavior', 'afMSIT_group_data.csv' )\n",
    "df = read_csv(data_file)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for subj in subjects:\n",
    "    \n",
    "    print 'Loading data for %s.' %subj\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    # Define paths.\n",
    "    root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT'\n",
    "    raw_file  = os.path.join( root_dir, 'raw', '%s_%s_raw.fif' %(subj,task) )\n",
    "    proj_file = os.path.join( root_dir, 'raw', '%s_%s-proj.fif' %(subj,task) )\n",
    "    \n",
    "    # Load data.\n",
    "    raw = Raw(raw_file,add_eeg_ref=False,preload=True,verbose=False)\n",
    "    proj = read_proj(proj_file)\n",
    "    \n",
    "    ## Add projections.\n",
    "    proj = [p for p in proj if 'ref' not in p['desc']]\n",
    "    raw.add_proj(proj, remove_existing=True)\n",
    "    raw.set_eeg_reference()\n",
    "    raw.apply_proj()\n",
    "    \n",
    "    ## Reduce dataframe to subject.\n",
    "    data = df[df.Subject==subj]\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Make events.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    trial_onsets  = find_events(raw, stim_channel='Trig1', output='onset', min_duration=0.25, verbose=False)\n",
    "    trial_offsets = find_events(raw, stim_channel='Trig2', output='offset', min_duration=0.25, verbose=False)\n",
    "    \n",
    "    ## Identify time at halfway point between end of trial(n-1) and start of trial(n).\n",
    "    iti_lengths = raw.times[trial_onsets[1:,0]] - raw.times[trial_offsets[:-1,0]]\n",
    "    iti_centers = raw.times[trial_offsets[:-1,0]] + iti_lengths / 2.\n",
    "\n",
    "    ## Prepend first ITI center as 1.5 prior to first onset (corresponding to a 3s \"ITI\" before first trial).\n",
    "    first_onset = raw.times[trial_onsets[0,0]]\n",
    "    iti_centers = np.insert(iti_centers,0,first_onset - 1.5)\n",
    "\n",
    "    ## Modify first event of DBS-on so that center of ITI prior to first onset occurs 1.5s before onset.\n",
    "    ix = np.where( (data.DBS==1) & (data.Trial==1) )[0][0]\n",
    "    first_onset = raw.times[trial_onsets[ix,0]]\n",
    "    iti_centers[ix] = first_onset - 1.5\n",
    "\n",
    "    ## Make events.\n",
    "    events = np.zeros_like(trial_offsets)\n",
    "    events.T[0] += raw.time_as_index(iti_centers)\n",
    "    \n",
    "    # Update event identifiers.\n",
    "    n = 1\n",
    "    for dbs in [0,1]:\n",
    "        for cond in [0,1]:\n",
    "            ix, = np.where((data.DBS==dbs)&(data.Interference==cond))\n",
    "            events[ix,-1] = n\n",
    "            n+=1\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Filter\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    Fs = raw.info['sfreq']\n",
    "    raw.filter(l_freq = l_freq, h_freq = h_freq, filter_length=filter_length, n_jobs=n_jobs,\n",
    "               l_trans_bandwidth=l_trans_bandwidth, h_trans_bandwidth=h_trans_bandwidth)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Make epochs.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    # Make initial ITI epochs.\n",
    "    picks = pick_types(raw.info, meg=False, eeg=True, exclude='bads')\n",
    "    epochs = Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax, \n",
    "                    baseline=None, picks=picks, reject=None, flat=None, proj=True, \n",
    "                    detrend=detrend, decim=decim)\n",
    "\n",
    "    # Make baseline periods.\n",
    "    bl = Epochs(raw, trial_onsets, tmin=baseline[0], tmax=baseline[1], \n",
    "                baseline=None, picks=picks, reject=None, flat=None, proj=True, \n",
    "                detrend=detrend, decim=decim)\n",
    "    \n",
    "    # Baseline correction.\n",
    "    arr = epochs.get_data()\n",
    "    bl = bl.get_data().mean(axis=-1)\n",
    "    arr = (arr.T - bl.T).T\n",
    "    \n",
    "    # Finalize ITI epochs.\n",
    "    epochs = EpochsArray(arr, epochs.info, events=epochs.events, tmin=epochs.tmin, \n",
    "                         baseline=None, reject=reject, flat=flat, \n",
    "                         proj=True, verbose=False)\n",
    "    print epochs\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Save data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    epochs.save(os.path.join(root_dir,'ave','%s_%s_%s_iti-epo.fif' %(subj,task,h_freq)))\n",
    "    \n",
    "    print '\\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\\n'\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make source localized ITI epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from mne import read_epochs, read_label, read_source_spaces, set_log_level\n",
    "from mne.minimum_norm import apply_inverse_epochs, read_inverse_operator\n",
    "from scipy.io import loadmat\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "analysis = 'iti_bl'\n",
    "parc = 'april2016'\n",
    "fmax = 50\n",
    "\n",
    "## Source localization parameters.\n",
    "method = 'dSPM'\n",
    "snr = 1.0  \n",
    "lambda2 = 1.0 / snr ** 2\n",
    "pick_ori = 'normal'\n",
    "\n",
    "## Labels\n",
    "rois = ['dlpfc_5-lh']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Iteratively load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT'\n",
    "fs_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/freesurfs'\n",
    "\n",
    "## Prepare fsaverage source space.\n",
    "src = read_source_spaces(os.path.join(fs_dir,'fscopy','bem','fscopy-oct-6p-src.fif'))\n",
    "vertices_to = [src[n]['vertno'] for n in xrange(2)]\n",
    "labels = [read_label(os.path.join(fs_dir,'fscopy','label','april2016','%s.label' %roi), subject='fsaverage')\n",
    "          for roi in rois]\n",
    "\n",
    "for subject in subjects:\n",
    "\n",
    "    print 'Performing source localization: %s' %subject\n",
    "\n",
    "    ## Load in epochs.\n",
    "    epochs = read_epochs(os.path.join(root_dir,'ave','%s_%s_%s_%s-epo.fif' %(subject,task,fmax,analysis)))\n",
    "    times = epochs.times\n",
    "    \n",
    "    ## Load in secondary files.\n",
    "    inv = read_inverse_operator(os.path.join(root_dir,'cov','%s_%s_%s-inv.fif' %(subject,task,fmax)))\n",
    "    morph_mat = loadmat(os.path.join(root_dir, 'morph_maps', '%s-fsaverage_morph.mat' %subject))['morph_mat']\n",
    "\n",
    "    ## Make generator object.\n",
    "    G = apply_inverse_epochs(epochs, inv, method=method, lambda2=lambda2, pick_ori=pick_ori, return_generator=True)\n",
    "    del inv\n",
    "\n",
    "    ## Iteratively compute and store label timecourse. \n",
    "    ltcs = []\n",
    "    for g in G:\n",
    "        g = g.morph_precomputed('fsaverage', vertices_to=vertices_to, morph_mat=morph_mat)\n",
    "        ltcs.append( g.extract_label_time_course(labels, src, mode='pca_flip') )\n",
    "    ltcs = np.array(ltcs)\n",
    "    \n",
    "    ## Save.\n",
    "    f = os.path.join(root_dir,'source','stcs','%s_%s_%s_%s_%s_epochs' %(subject,task,analysis,method,fmax))\n",
    "    np.savez_compressed(f, ltcs=ltcs, times=times, labels=np.array([l.name for l in labels]), conds=epochs.events.T[-1])\n",
    "    del ltcs\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make power baseline for ITI spectral epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne.filter import low_pass_filter\n",
    "from mne.time_frequency import single_trial_power\n",
    "from pandas import read_csv\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Data parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "analysis = 'stim'\n",
    "method = 'dSPM'\n",
    "h_freq = 50\n",
    "\n",
    "## Label parameters.\n",
    "rois = [ 'dlpfc_5-lh']\n",
    "\n",
    "## Time-frequency parameters.\n",
    "freqs = np.logspace( np.log10(2), np.log10(50), num=25)\n",
    "n_cycles = 3\n",
    "baseline = (-0.5, -0.1)\n",
    "Fs = 1450.\n",
    "decim = 14\n",
    "n_jobs = 3\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/source'\n",
    "df = read_csv(os.path.join(root_dir, 'afMSIT_source_info.csv'))\n",
    "\n",
    "for roi in rois:\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Iteratively load and merge.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ltcs = []\n",
    "    for subject in subjects:\n",
    "\n",
    "        ## Load NPZ.\n",
    "        npz = np.load(os.path.join(root_dir, 'stcs', '%s_msit_%s_%s_%s_epochs.npz' %(subject,analysis,method,h_freq)))\n",
    "\n",
    "        ## Get label index and extract.\n",
    "        ix = npz['labels'].tolist().index(roi)\n",
    "        arr = npz['ltcs'][:,ix,:]\n",
    "\n",
    "        ## Append.\n",
    "        ltcs.append(arr)\n",
    "\n",
    "    ## Concatenate.\n",
    "    ltcs = np.concatenate(ltcs, axis=0)\n",
    "    times = npz['times']\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Phase-lock removal.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    for subject in df.Subject.unique():\n",
    "        for dbs in [0, 1]:\n",
    "            for cond in [0, 1]:\n",
    "                ix, = np.where((df.Subject==subject)&(df.DBS==dbs)&(df.Interference==cond))\n",
    "                ltcs[ix] -= ltcs[ix].mean(axis=0)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute power.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    print 'Computing single trial power: %s %s.' %(roi, analysis)\n",
    "    ltcs = np.expand_dims(ltcs,1)\n",
    "    power = single_trial_power(ltcs, sfreq=Fs, frequencies=freqs, n_cycles=n_cycles, \n",
    "                                baseline=None, n_jobs=n_jobs, verbose=False)\n",
    "    power = power.squeeze()\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Within-trial normalization.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    power = (power.T / np.median(power, axis=-1).T).T # median, not mean\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare baseline normalization (within subject, DBS).\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Compute baseline normalization.\n",
    "    if analysis == 'stim':\n",
    "\n",
    "        ## Make time mask.\n",
    "        mask = (times >= baseline[0]) & (times <= baseline[1])\n",
    "\n",
    "        ## Iteratively compute over subjects.\n",
    "        blnorm = []\n",
    "        for subject in df.Subject.unique():\n",
    "\n",
    "            ## Iteratively compute over DBS conditions..\n",
    "            sbl = []\n",
    "            for dbs in [0,1]:\n",
    "\n",
    "                ix, = np.where((df.Subject==subject)&(df.DBS==dbs))\n",
    "                sbl.append( np.apply_over_axes(np.median, power[ix][:,:,mask], axes=[0,2]).squeeze() )\n",
    "\n",
    "            blnorm.append(sbl)\n",
    "\n",
    "        ## Merge.\n",
    "        blnorm = np.array(blnorm)\n",
    "      \n",
    "        ## Save.\n",
    "        np.save(os.path.join(root_dir, 'afMSIT_source_iti_%s_bl' %roi), blnorm)\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute power of source timecourses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne.filter import low_pass_filter\n",
    "from mne.time_frequency import single_trial_power\n",
    "from pandas import DataFrame\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Data parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "method = 'dSPM'\n",
    "h_freq = 50\n",
    "\n",
    "## Label parameters.\n",
    "rois = ['dlpfc_5-lh']\n",
    "\n",
    "## Time-frequency parameters.\n",
    "freqs = np.logspace( np.log10(2), np.log10(50), num=25)\n",
    "n_cycles = 3\n",
    "Fs = 1450.\n",
    "decim = 14\n",
    "n_jobs = 3\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/source'\n",
    "\n",
    "for roi in rois:\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Iteratively load and merge.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ltcs = []\n",
    "    df = dict(Condition = [], Subject = [])\n",
    "    \n",
    "    for subject in subjects:\n",
    "\n",
    "        ## Load NPZ.\n",
    "        npz = np.load(os.path.join(root_dir, 'stcs', '%s_msit_iti_%s_%s_epochs.npz' %(subject,method,h_freq)))\n",
    "\n",
    "        ## Get label index and extract.\n",
    "        ix = npz['labels'].tolist().index(roi)\n",
    "        arr = npz['ltcs'][:,ix,:]\n",
    "\n",
    "        ## Phase-lock removal.\n",
    "        for cond in [1,2,3,4]:\n",
    "            ix, = np.where(npz['conds']==cond)\n",
    "            arr[ix] -= arr[ix].mean(axis=0)\n",
    "        \n",
    "        ## Append.\n",
    "        ltcs.append(arr)\n",
    "        df['Condition'] += npz['conds'].tolist()\n",
    "        df['Subject'] += [subject] * len(npz['conds'])\n",
    "\n",
    "    ## Concatenate.\n",
    "    ltcs = np.concatenate(ltcs, axis=0)\n",
    "    times = npz['times']\n",
    "    df = DataFrame(df)\n",
    "    df['DBS'] = (df.Condition > 2).astype(int)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute power.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    print 'Computing single trial power: %s.' %roi\n",
    "    ltcs = np.expand_dims(ltcs,1)\n",
    "    power = single_trial_power(ltcs, sfreq=Fs, frequencies=freqs, n_cycles=n_cycles, \n",
    "                                baseline=None, n_jobs=n_jobs, verbose=False)\n",
    "    power = power.squeeze()\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Within-trial normalization.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    power = (power.T / np.median(power, axis=-1).T).T # median, not mean\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Apply baseline normalization (within subject, DBS).\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load baseline for for normalization.\n",
    "    blnorm = np.load(os.path.join(root_dir, 'afMSIT_source_iti_%s_bl.npy' %roi))\n",
    "    \n",
    "    ## Setup index vectors.\n",
    "    n_trials, n_freqs, n_times = power.shape        \n",
    "    _, subj_ix = np.unique(df.Subject, return_inverse=True)\n",
    "    dbs_ix = df.DBS.as_matrix()\n",
    "    trial_ix = np.arange(n_trials) \n",
    "\n",
    "    ## Main loop.\n",
    "    for i,j,k in zip(trial_ix,subj_ix,dbs_ix):\n",
    "\n",
    "        for m in xrange(n_times):\n",
    "\n",
    "            power[i,:,m] /= blnorm[j,k,:]\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Final preprocessing steps.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Crop times.\n",
    "    mask = (times >= -1) & (times <= 1)\n",
    "    power = power[:, :, mask]\n",
    "    times = times[mask]\n",
    "\n",
    "    ## Decimate\n",
    "    power = power[:,:,::decim]\n",
    "    times = times[::decim]\n",
    "    \n",
    "    ## Save.\n",
    "    f = os.path.join(root_dir, 'afMSIT_source_iti_%s_spectra' %roi)\n",
    "    np.savez_compressed(f, power=power, times=times, freqs=freqs, n_cycles=n_cycles, \n",
    "                        subject=df.Subject.as_matrix(), conds=df.Condition.as_matrix())\n",
    "\n",
    "    del power\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resting State Spectral Comparisons\n",
    "### Projections: EOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mne import write_proj\n",
    "from mne.preprocessing import compute_proj_eog\n",
    "from mne.io import Raw\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Setup\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## File params.\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/resting/raw'\n",
    "subjects = ['S2']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "raw_files = os.listdir(root_dir)\n",
    "raw_files = [f for f in raw_files if f.split('_')[0] in subjects and f.endswith('raw.fif')]\n",
    "\n",
    "for raw_file in raw_files:    \n",
    "    \n",
    "    ## Load files.\n",
    "    raw_file = os.path.join(root_dir, raw_file)\n",
    "    raw = Raw(raw_file, preload=True, verbose=False, add_eeg_ref=False)\n",
    "    raw.add_eeg_average_proj()\n",
    "    \n",
    "    proj, _ = compute_proj_eog(raw, n_eeg = 4, average=True, filter_length='20s',\n",
    "                               reject=dict(eeg=5e-4), flat=dict(eeg=5e-8),  ch_name='EOG', n_jobs=3)\n",
    "    write_proj(raw_file.replace('_raw.fif', '_eog-proj.fif'), proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projections: ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mne import read_proj, write_proj\n",
    "from mne.preprocessing import compute_proj_ecg\n",
    "from mne.io import Raw\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Setup\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## File params.\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/resting/raw'\n",
    "subjects = ['CHDR']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "raw_files = os.listdir(root_dir)\n",
    "raw_files = [f for f in raw_files if f.split('_')[0] in subjects and f.endswith('raw.fif')]\n",
    "\n",
    "for raw_file in raw_files:    \n",
    "    \n",
    "    ## Load files.\n",
    "    raw_file = os.path.join(root_dir, raw_file)\n",
    "    raw = Raw(raw_file, preload=True, verbose=False, add_eeg_ref=False)\n",
    "    raw.add_eeg_average_proj()\n",
    "    \n",
    "    ## Make ECG proj. Save.\n",
    "    proj, _ = compute_proj_ecg(raw, n_eeg = 4, h_freq = 35., average=True, filter_length='20s',\n",
    "                                reject=dict(eeg=5e-4), flat=dict(eeg=5e-8), ch_name='P9', n_jobs=3)\n",
    "    write_proj(raw_file.replace('_raw.fif', '_ecg-proj.fif'), proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from mne import pick_channels, read_proj, set_log_level\n",
    "from mne.io import Raw\n",
    "from mne.time_frequency import psd_multitaper\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define Parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "subjects = ['BRTU', 'CHDR', 'JADE', 'S2']\n",
    "conds = ['resting_dbsoff_eo', 'resting_dbson_eo']\n",
    "\n",
    "## Filtering parameters.\n",
    "fmin, fmax = 0.5, 30\n",
    "tmin, tmax = 10, 40\n",
    "chs = ['FCZ']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/resting/raw'\n",
    "\n",
    "psds = []\n",
    "for subject in subjects:\n",
    "    \n",
    "    psds.append([])\n",
    "    \n",
    "    for cond in conds:\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Prepare data. \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        ## Load data.\n",
    "        raw = os.path.join(root_dir, '%s_%s_raw.fif' %(subject,cond))\n",
    "        raw = Raw(raw, preload=True, add_eeg_ref=False, verbose=False)\n",
    "        \n",
    "        ## Load/apply projection(s).\n",
    "        proj = os.path.join(root_dir, '%s_%s-proj.fif' %(subject,cond))\n",
    "        \n",
    "        if os.path.isfile(proj): \n",
    "            proj = read_proj(proj)\n",
    "            raw.add_proj(proj)\n",
    "        else:\n",
    "            raw.set_eeg_reference()\n",
    "        \n",
    "        raw.apply_proj()\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute PSD.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        picks = pick_channels(raw.ch_names, chs)\n",
    "        psd, freqs = psd_multitaper(raw, fmin=fmin, fmax=fmax, tmin=10, tmax=40, picks=picks, n_jobs=3)\n",
    "        psds[-1].append(psd)\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Merge and plot.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Merge.\n",
    "psds = np.array(psds).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Normalize.\n",
    "psd = psds.mean(axis=0)\n",
    "auc = psd.sum(axis=-1)\n",
    "psd = (psd.T / auc).T\n",
    "\n",
    "## Plot.\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "for arr, label, color in zip(psd, ['DBSoff','DBSon'], ['#0571b0','#ca0020']):\n",
    "    ax.plot(freqs, arr, linewidth=4, label=label, color=color, alpha=0.7)\n",
    "ax.legend(loc=1, frameon=False, fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.close('all')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Plotting (Primary Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Domain Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from pandas import read_csv\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## File parameters.\n",
    "labels = ['dacc-lh', 'dacc-rh', 'dmpfc-lh', 'dmpfc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "          'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "          'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "# labels = ['FCZ']\n",
    "\n",
    "space = 'source'\n",
    "model_name = 'revised'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/%s' %space\n",
    "img_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/plots/%s' %space\n",
    "info = read_csv(os.path.join(root_dir, 'afMSIT_%s_info.csv' %space))\n",
    "\n",
    "for label in labels:\n",
    "\n",
    "    for freq in [15]:\n",
    "\n",
    "        ## Intialize figure.\n",
    "        fig = plt.figure(figsize=(16,8))\n",
    "        axes = []\n",
    "        \n",
    "        ## Initialize labels.\n",
    "        if space == 'sensor': ylabel = 'Voltage (uV)'\n",
    "        elif space == 'source': ylabel = 'dSPM'\n",
    "        title = '%s ERP' %(label.replace('_',' ').upper())\n",
    "        out_path = os.path.join(img_dir, 'timedomain', '%s_%s.png' %(model_name,title.replace(' ','_').lower()))\n",
    "            \n",
    "        for n, analysis in enumerate(['stim', 'resp']):\n",
    " \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            ### Load data.\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "            ## Load source data.\n",
    "            npz = np.load(os.path.join(root_dir, 'afMSIT_%s_%s_%s_%s.npz' %(space,analysis,label,freq)))\n",
    "            data = npz['data']\n",
    "            times = npz['times']\n",
    "            \n",
    "            ## Load cluster results.\n",
    "            f = os.path.join(root_dir, 'results', '%s_%s_timedomain_results.csv' %(model_name, analysis))\n",
    "            clusters = read_csv(f)\n",
    "            \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            ### Plot DBS Difference.\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            \n",
    "            ax = plt.subplot2grid((2,2),(n,0))\n",
    "            for m, color, legend in zip([0,1],['#0571b0','#ca0020'],['DBSoff','DBSon']):\n",
    "                \n",
    "                ix, = np.where(info.DBS==m)\n",
    "                mu = data[ix].mean(axis=0)\n",
    "                se = data[ix].std(axis=0) / np.sqrt(len(ix))\n",
    "                ax.plot(times, mu, linewidth=2, color=color, label=legend)\n",
    "                ax.fill_between(times, mu-se, mu+se, color=color, alpha=0.2)\n",
    "        \n",
    "            ## Plot significant clusters.\n",
    "            for ix in np.where((clusters.Label==label)&(clusters.Freq==freq)&\n",
    "                               (clusters.Contrast=='DBS')&(clusters.FDR<0.05))[0]:\n",
    "                y1, y2 = ax.get_ylim()\n",
    "                tmin, tmax = clusters.loc[ix,'Tmin'], clusters.loc[ix,'Tmax']\n",
    "                ax.fill_between(np.linspace(tmin,tmax,1e3), y1, y2, color='k', alpha=0.2) \n",
    "                ax.set_ylim(y1,y2)\n",
    "                \n",
    "            axes.append(ax)\n",
    "                \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            ### Plot Interference Difference.\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "            ax = plt.subplot2grid((2,2),(n,1))\n",
    "            for m, color, legend in zip([0,1],['#7b3294','#008837'],['Neu','Int']):\n",
    "                \n",
    "                ix, = np.where(info.Interference==m)\n",
    "                mu = data[ix].mean(axis=0)\n",
    "                se = data[ix].std(axis=0) / np.sqrt(len(ix))\n",
    "                ax.plot(times, mu, linewidth=2, color=color, label=legend)\n",
    "                ax.fill_between(times, mu-se, mu+se, color=color, alpha=0.2)\n",
    "                \n",
    "            ## Plot significant clusters.\n",
    "            for ix in np.where((clusters.Label==label)&(clusters.Freq==freq)&\n",
    "                               (clusters.Contrast=='Interference')&(clusters.FDR<0.05))[0]:\n",
    "                y1, y2 = ax.get_ylim()\n",
    "                tmin, tmax = clusters.loc[ix,'Tmin'], clusters.loc[ix,'Tmax']\n",
    "                ax.fill_between(np.linspace(tmin,tmax,1e3), y1, y2, color='k', alpha=0.2)    \n",
    "                ax.set_ylim(y1,y2)\n",
    "            \n",
    "            axes.append(ax)\n",
    "            \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Add Flourishes\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        for i, ax in enumerate(axes):\n",
    "\n",
    "            ax.legend(loc=2, fontsize=18, markerscale=2, frameon=False, borderpad=0, handletextpad=0.2)\n",
    "            ax.set_xlabel('Time (s)', fontsize=18)\n",
    "            if not i%2: ax.set_ylabel(ylabel, fontsize=20)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "            if not i/2: ax.set_title('Stimulus-Locked', fontsize=24)\n",
    "            else: ax.set_title('Response-Locked', fontsize=24)\n",
    "\n",
    "            ## Time-lock specific.\n",
    "            if not i/2:\n",
    "                y1, y2 = ax.get_ylim()\n",
    "                for x,s in zip([0, 0.4, 1.127],['IAPS','MSIT','Resp']): \n",
    "                    ax.text(x+0.02,y1+np.abs(y1*0.05),s,fontsize=16)\n",
    "                    ax.vlines(x,y1,y2,linestyle='--',alpha=0.3)\n",
    "                ax.set_ylim(y1,y2)\n",
    "\n",
    "            else:\n",
    "                y1, y2 = ax.get_ylim()\n",
    "                ax.text(0.02,y1+np.abs(y1*0.05),'Resp',fontsize=16)\n",
    "                ax.vlines(0.0,y1,y2,linestyle='--',alpha=0.3)\n",
    "                ax.set_ylim(y1,y2)\n",
    "\n",
    "        plt.suptitle(title, y=0.99, fontsize=28)\n",
    "        plt.subplots_adjust(left=0.05, right=0.975, hspace=0.35, wspace=0.15)\n",
    "        #plt.show()\n",
    "        plt.savefig(out_path)\n",
    "        plt.close()\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from pandas import read_csv\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## File parameters.\n",
    "labels = ['dacc-lh', 'dacc-rh', 'dmpfc-lh', 'dmpfc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "          'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "          'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "# labels = ['FCZ']\n",
    "\n",
    "space = 'source'\n",
    "model_name = 'revised'\n",
    "baseline = (-0.5, -0.1)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/%s' %space\n",
    "img_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/plots/%s' %space\n",
    "info = read_csv(os.path.join(root_dir, 'afMSIT_%s_info.csv' %space))\n",
    "\n",
    "for label in labels:\n",
    "\n",
    "    for freq in ['theta','alpha','beta']:\n",
    "\n",
    "        ## Intialize figure.\n",
    "        fig = plt.figure(figsize=(16,8))\n",
    "        axes = []\n",
    "        \n",
    "        ## Initialize labels.\n",
    "        ylabel = 'Power (dB)'\n",
    "        title = '%s %s Power' %(label.replace('_',' ').upper(), freq.capitalize())\n",
    "        out_path = os.path.join(img_dir, 'frequency', '%s_%s.png' %(model_name,title.replace(' ','_').lower()))\n",
    "            \n",
    "        for n, analysis in enumerate(['stim', 'resp']):\n",
    " \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            ### Load data.\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "            ## Load source data.\n",
    "            npz = np.load(os.path.join(root_dir, 'afMSIT_%s_%s_%s_%s.npz' %(space,analysis,label,freq)))\n",
    "            data = npz['data']\n",
    "            times = npz['times']\n",
    "            \n",
    "            ## Load cluster results.\n",
    "            f = os.path.join(root_dir, 'results', '%s_%s_frequency_results.csv' %(model_name, analysis))\n",
    "            clusters = read_csv(f)\n",
    "            \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            ### Plot DBS Difference.\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            \n",
    "            ax = plt.subplot2grid((2,2),(n,0))\n",
    "            for m, color, legend in zip([0,1],['#0571b0','#ca0020'],['DBSoff','DBSon']):\n",
    "                \n",
    "                ix, = np.where(info.DBS==m)\n",
    "                mu = data[ix].mean(axis=0)\n",
    "                if analysis == 'stim': mu -= mu[(times >= baseline[0])&(times <= baseline[1])].mean()\n",
    "                se = data[ix].std(axis=0) / np.sqrt(len(ix))\n",
    "                ax.plot(times, mu, linewidth=2, color=color, label=legend)\n",
    "                ax.fill_between(times, mu-se, mu+se, color=color, alpha=0.2)\n",
    "        \n",
    "            ## Plot significant clusters.\n",
    "            for ix in np.where((clusters.Label==label)&(clusters.Freq==freq)&\n",
    "                               (clusters.Contrast=='DBS')&(clusters.FDR<0.05))[0]:\n",
    "                y1, y2 = ax.get_ylim()\n",
    "                tmin, tmax = clusters.loc[ix,'Tmin'], clusters.loc[ix,'Tmax']\n",
    "                ax.fill_between(np.linspace(tmin,tmax,1e3), y1, y2, color='k', alpha=0.2) \n",
    "                ax.set_ylim(y1,y2)\n",
    "                \n",
    "            axes.append(ax)\n",
    "                \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            ### Plot Interference Difference.\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "            ax = plt.subplot2grid((2,2),(n,1))\n",
    "            for m, color, legend in zip([0,1],['#7b3294','#008837'],['Neu','Int']):\n",
    "                \n",
    "                ix, = np.where(info.Interference==m)\n",
    "                mu = data[ix].mean(axis=0)\n",
    "                if analysis == 'stim': mu -= mu[(times >= baseline[0])&(times <= baseline[1])].mean()\n",
    "                se = data[ix].std(axis=0) / np.sqrt(len(ix))\n",
    "                ax.plot(times, mu, linewidth=2, color=color, label=legend)\n",
    "                ax.fill_between(times, mu-se, mu+se, color=color, alpha=0.2)\n",
    "                \n",
    "            ## Plot significant clusters.\n",
    "            for ix in np.where((clusters.Label==label)&(clusters.Freq==freq)&\n",
    "                               (clusters.Contrast=='Interference')&(clusters.FDR<0.05))[0]:\n",
    "                y1, y2 = ax.get_ylim()\n",
    "                tmin, tmax = clusters.loc[ix,'Tmin'], clusters.loc[ix,'Tmax']\n",
    "                ax.fill_between(np.linspace(tmin,tmax,1e3), y1, y2, color='k', alpha=0.2)    \n",
    "                ax.set_ylim(y1,y2)\n",
    "            \n",
    "            axes.append(ax)\n",
    "            \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Add Flourishes\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        for i, ax in enumerate(axes):\n",
    "\n",
    "            ax.legend(loc=2, fontsize=18, markerscale=2, frameon=False, borderpad=0, handletextpad=0.2)\n",
    "            ax.set_xlabel('Time (s)', fontsize=18)\n",
    "            if not i%2: ax.set_ylabel(ylabel, fontsize=20)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "            if not i/2: ax.set_title('Stimulus-Locked', fontsize=24)\n",
    "            else: ax.set_title('Response-Locked', fontsize=24)\n",
    "\n",
    "            ## Time-lock specific.\n",
    "            if not i/2:\n",
    "                y1, y2 = ax.get_ylim()\n",
    "                for x,s in zip([0, 0.4, 1.127],['IAPS','MSIT','Resp']): \n",
    "                    ax.text(x+0.02,y1+np.abs(y1*0.05),s,fontsize=16)\n",
    "                    ax.vlines(x,y1,y2,linestyle='--',alpha=0.3)\n",
    "                ax.set_ylim(y1,y2)\n",
    "\n",
    "            else:\n",
    "                y1, y2 = ax.get_ylim()\n",
    "                ax.text(0.02,y1+np.abs(y1*0.05),'Resp',fontsize=16)\n",
    "                ax.vlines(0.0,y1,y2,linestyle='--',alpha=0.3)\n",
    "                ax.set_ylim(y1,y2)\n",
    "\n",
    "        plt.suptitle(title, y=0.99, fontsize=28)\n",
    "        plt.subplots_adjust(left=0.05, right=0.975, hspace=0.35, wspace=0.15)\n",
    "        #plt.show()\n",
    "        plt.savefig(out_path)\n",
    "        plt.close()\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Secondary Figures\n",
    "## Intertrial Interval Spectra Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from cmap_utils import *\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load and extract data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load data.\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/source'\n",
    "npz = np.load( os.path.join(root_dir, 'afMSIT_source_iti_dlpfc_5-lh_spectra.npz') )\n",
    "\n",
    "## Extract data.\n",
    "power = npz['power']\n",
    "times = npz['times']\n",
    "freqs = npz['freqs']\n",
    "dbs = (npz['conds'] > 2).astype(int)\n",
    "\n",
    "## Compute contrasts.\n",
    "dbs_off = np.log10( np.median( power[dbs==0], axis=0 ) ) * 10\n",
    "dbs_on = np.log10( np.median( power[dbs==1], axis=0 ) ) * 10\n",
    "contrast = dbs_on - dbs_off\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot spectrograms.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "fig, axes = plt.subplots(1,3,figsize=(15,5),sharex=True)\n",
    "vmin, vmax = np.floor( np.min([dbs_off, dbs_on]) ), np.ceil( np.max([dbs_off, dbs_on]) )\n",
    "\n",
    "\n",
    "## Plot DBS-off.\n",
    "cmap = center_color_map(dbs_off, 'jet')\n",
    "cbar = axes[0].imshow(dbs_off , aspect='auto', origin='lower', cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(cbar, ax=axes[0],  ticks=np.arange(vmin, vmax+0.01, 0.5))\n",
    "axes[0].set_ylabel('Frequency', fontsize=18)\n",
    "\n",
    "## Plot DBS-on.\n",
    "cmap = center_color_map(dbs_on, 'jet')\n",
    "cbar = axes[1].imshow(dbs_on , aspect='auto', origin='lower', cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(cbar, ax=axes[1], ticks=np.arange(vmin, vmax+0.01, 0.5))\n",
    "\n",
    "## Plot contrast.\n",
    "cmap = center_color_map(contrast, 'jet')\n",
    "cbar = axes[2].imshow(contrast , aspect='auto', origin='lower', cmap=cmap, vmin=-1, vmax=1)\n",
    "plt.colorbar(cbar, ax=axes[2], ticks=np.arange(-1,1.1,0.5))\n",
    "\n",
    "## Add flourishes.\n",
    "for ax, title in zip(axes, ['DBSoff', 'DBSon', 'On-Off']):\n",
    "    \n",
    "    ax.set_xticks(np.linspace(0, times.shape[0], 6))\n",
    "    ax.set_xticklabels(np.linspace(times.min(), times.max(), 6).round(2))\n",
    "    ax.set_xlabel('Time (s)', fontsize=16)\n",
    "    \n",
    "    ax.set_yticks([0,5,10,15,18,20,22,24])\n",
    "    ax.set_yticklabels(freqs[[0,5,10,15,18,20,22,24]].astype(int))\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    \n",
    "plt.subplots_adjust(left=0.05, right=0.98, top=0.8)\n",
    "plt.suptitle('Intertrial Power Spectra', fontsize=28)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intertrial Interval Power Spectrum Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from mne import EpochsArray, create_info\n",
    "from mne.time_frequency import psd_multitaper\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Data parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "method = 'dSPM'\n",
    "h_freq = 50\n",
    "roi = 'dlpfc_5-lh'\n",
    "sfreq = 1450\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Setup.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/source'\n",
    "\n",
    "epochs, conds = [], []\n",
    "for subject in subjects:\n",
    "\n",
    "    ## Load NPZ.\n",
    "    npz = np.load(os.path.join(root_dir, 'stcs', '%s_msit_iti_%s_%s_epochs.npz' %(subject,method,h_freq)))\n",
    "    epochs.append(npz['ltcs'])\n",
    "    conds.append(npz['conds'])\n",
    "\n",
    "## Concatenate.\n",
    "epochs = np.concatenate(epochs, axis=0)\n",
    "conds = np.concatenate(conds, axis=0)\n",
    "times = npz['times']\n",
    "\n",
    "## Make into epochs object.\n",
    "n_trials, n_channels, n_times = epochs.shape\n",
    "\n",
    "## Make info.\n",
    "info = create_info([roi], sfreq, ['eeg'])\n",
    "\n",
    "## Make events.\n",
    "events = np.zeros((n_trials,3), dtype=int)\n",
    "events.T[0] = np.arange(n_trials)\n",
    "events.T[-1] = conds\n",
    "event_id = dict( FN=1, FI=2, NN=3, NI=4 )\n",
    "\n",
    "epochs = EpochsArray(epochs, info, events, tmin=-1.25, event_id=event_id, verbose=False)\n",
    "\n",
    "## Compute PSD.\n",
    "psd_dbsoff, freqs = psd_multitaper(epochs[['FI','FN']], fmin=0, fmax=50, n_jobs=3, verbose=False)\n",
    "psd_dbson,  freqs = psd_multitaper(epochs[['NI','NN']], fmin=0, fmax=50, n_jobs=3, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "\n",
    "for arr, label, color in zip([psd_dbsoff,psd_dbson], ['DBSoff','DBSon'], ['#0571b0','#ca0020']):\n",
    "    \n",
    "    \n",
    "    mu = arr.squeeze().mean(axis=0)\n",
    "    mask = (freqs <= 30)\n",
    "    auc = mu[mask].sum()\n",
    "    \n",
    "    arr2 = arr / auc\n",
    "    \n",
    "    mu = arr2.squeeze().mean(axis=0)\n",
    "    sd = arr2.squeeze().std(axis=0)\n",
    "    se = sd / np.sqrt(arr2.shape[0])\n",
    "    \n",
    "    ax.plot(freqs, mu, linewidth=3, label=label, color=color)\n",
    "    ax.fill_between(freqs, mu-se, mu+se, color=color, alpha=0.2)\n",
    "    \n",
    "ax.legend(loc=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot spectrograms.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "fig, axes = plt.subplots(1,3,figsize=(15,5),sharex=True)\n",
    "vmin, vmax = np.floor( np.min([dbs_off, dbs_on]) ), np.ceil( np.max([dbs_off, dbs_on]) )\n",
    "\n",
    "\n",
    "## Plot DBS-off.\n",
    "cmap = center_color_map(dbs_off, 'jet')\n",
    "cbar = axes[0].imshow(dbs_off , aspect='auto', origin='lower', cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(cbar, ax=axes[0],  ticks=np.arange(vmin, vmax+0.01, 0.5))\n",
    "axes[0].set_ylabel('Frequency', fontsize=18)\n",
    "\n",
    "## Plot DBS-on.\n",
    "cmap = center_color_map(dbs_on, 'jet')\n",
    "cbar = axes[1].imshow(dbs_on , aspect='auto', origin='lower', cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(cbar, ax=axes[1], ticks=np.arange(vmin, vmax+0.01, 0.5))\n",
    "\n",
    "## Plot contrast.\n",
    "cmap = center_color_map(contrast, 'jet')\n",
    "cbar = axes[2].imshow(contrast , aspect='auto', origin='lower', cmap=cmap, vmin=-1, vmax=1)\n",
    "plt.colorbar(cbar, ax=axes[2], ticks=np.arange(-1,1.1,0.5))\n",
    "\n",
    "## Add flourishes.\n",
    "for ax, title in zip(axes, ['DBSoff', 'DBSon', 'On-Off']):\n",
    "    \n",
    "    ax.set_xticks(np.linspace(0, times.shape[0], 6))\n",
    "    ax.set_xticklabels(np.linspace(times.min(), times.max(), 6).round(2))\n",
    "    ax.set_xlabel('Time (s)', fontsize=16)\n",
    "    \n",
    "    ax.set_yticks([0,5,10,15,18,20,22,24])\n",
    "    ax.set_yticklabels(freqs[[0,5,10,15,18,20,22,24]].astype(int))\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    \n",
    "plt.subplots_adjust(left=0.05, right=0.98, top=0.8)\n",
    "plt.suptitle('Intertrial Power Spectra', fontsize=28)\n",
    "plt.show()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot power spectrum densities.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "\n",
    "ax.plot(freqs, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Section 6: Movies\n",
    "## Source localize evoked potentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from mne import BiHemiLabel, read_epochs, read_label, read_source_spaces, set_log_level\n",
    "from mne.minimum_norm import apply_inverse, read_inverse_operator\n",
    "from scipy.io import loadmat\n",
    "set_log_level(verbose=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Subject level parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "analysis = 'resp'\n",
    "parc = 'april2016'\n",
    "fmax = 50\n",
    "\n",
    "## Source localization parameters.\n",
    "method = 'dSPM'\n",
    "snr = 1.0  \n",
    "lambda2 = 3.0 / snr ** 2 # Note increased SNR\n",
    "pick_ori = 'normal'\n",
    "\n",
    "## Labels\n",
    "rois = ['dacc-lh', 'dacc-rh', 'dlpfc_1-lh', 'dlpfc_1-rh', 'dlpfc_2-lh', 'dlpfc_2-rh', \n",
    "        'dlpfc_3-lh', 'dlpfc_3-rh', 'dlpfc_4-lh', 'dlpfc_4-rh', 'dlpfc_5-lh', 'dlpfc_5-rh', \n",
    "        'dlpfc_6-lh', 'dlpfc_6-rh', 'pcc-lh', 'pcc-rh', 'racc-lh', 'racc-rh']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Initialize.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/afMSIT'\n",
    "fs_dir = '/autofs/space/sophia_002/users/EMOTE-DBS/freesurfs'\n",
    "\n",
    "## Prepare fsaverage source space.\n",
    "src = read_source_spaces(os.path.join(fs_dir,'fscopy','bem','fscopy-oct-6p-src.fif'))\n",
    "vertices_to = [src[n]['vertno'] for n in xrange(2)]\n",
    "\n",
    "## Prepare BiHemiLabel.\n",
    "label_dir = os.path.join(fs_dir,'fscopy','label','april2016')\n",
    "label_lh = np.sum([read_label('%s/%s.label' %(label_dir,roi), subject='fsaverage') for roi in rois if roi.endswith('-lh')])\n",
    "label_rh = np.sum([read_label('%s/%s.label' %(label_dir,roi), subject='fsaverage') for roi in rois if roi.endswith('-rh')])\n",
    "label = BiHemiLabel(label_lh, label_rh, name='afmsit')\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "for subject in subjects:\n",
    "\n",
    "    print 'Performing source localization: %s' %subject\n",
    "\n",
    "    ## Load in epochs.\n",
    "    epochs = read_epochs(os.path.join(root_dir,'ave','%s_%s_%s_%s-epo.fif' %(subject,task,fmax,analysis)))\n",
    "    times = epochs.times\n",
    "    \n",
    "    ## Load in secondary files.\n",
    "    inv = read_inverse_operator(os.path.join(root_dir,'cov','%s_%s_%s-inv.fif' %(subject,task,fmax)))\n",
    "    morph_mat = loadmat(os.path.join(root_dir, 'morph_maps', '%s-fsaverage_morph.mat' %subject))['morph_mat']\n",
    "\n",
    "    ## Iterate over {DBS x Interference}\n",
    "    for event in epochs.event_id.iterkeys():\n",
    "        \n",
    "        ## Compute evoked. Source localize.\n",
    "        evoked = epochs[event].average()\n",
    "        stc = apply_inverse(evoked, inv, method=method, lambda2=lambda2, pick_ori=pick_ori)\n",
    "        \n",
    "        ## Morph to common (fsaverage) space. Restrict to vertices in label.\n",
    "        stc = stc.morph_precomputed('fsaverage', vertices_to=vertices_to, morph_mat=morph_mat)\n",
    "        stc = stc.in_label(label)\n",
    "    \n",
    "        ## Save.\n",
    "        f = os.path.join(root_dir,'source','stcs','%s_%s_%s_%s_%s_%s_evoked' %(subject,task,analysis,method,fmax,event))\n",
    "        stc.save(f)\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reassemble source localized STCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne import read_source_estimate\n",
    "from mne.filter import low_pass_filter\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "subjects = ['BRTU','CHDR', 'CRDA', 'JADE', 'JASE', 'M5', 'MEWA', 'S2']\n",
    "task = 'msit'\n",
    "analysis = 'resp'\n",
    "method = 'dSPM'\n",
    "h_freq = 50\n",
    "\n",
    "## Preprocessing parameters.\n",
    "fmax = 15\n",
    "n_jobs = 3\n",
    "\n",
    "## Contrast parameters.\n",
    "events = ['FN','FI','NN','NI']\n",
    "condict = dict(DBS = np.array([-1,-1,1,1]), Intereference = np.array([-1,1,-1,1]))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "root_dir = '/space/sophia/2/users/EMOTE-DBS/afMSIT/source'\n",
    "\n",
    "SPMs = []\n",
    "for event in events:\n",
    "        \n",
    "    stcs = []\n",
    "    for subject in subjects:\n",
    "        \n",
    "        ## Load data.\n",
    "        f = os.path.join(root_dir, 'stcs', '%s_msit_%s_%s_%s_%s_evoked-lh.stc' %(subject,analysis,method,h_freq,event))\n",
    "        stc = read_source_estimate(f)\n",
    "        \n",
    "        ## Low pass filter.\n",
    "        stc._data = low_pass_filter(stc.data.astype(np.float64), stc.sfreq, fmax, filter_length='20s', n_jobs=n_jobs)\n",
    "                \n",
    "        ## Crop times. Downsample.\n",
    "        stc.resample(500., npad='auto')\n",
    "        if analysis == 'stim': stc = stc.crop(-0.5, 2.0)\n",
    "        elif analysis == 'resp': stc = stc.crop(-1.0,1.0)\n",
    "\n",
    "        ## Append.\n",
    "        stcs.append(stc)\n",
    "        \n",
    "    ## Average. Save.\n",
    "    stc = np.mean(stcs)\n",
    "    SPMs.append(stc)\n",
    "    stc.save(os.path.join(root_dir, 'afMSIT_source_%s_%s_%s' %(analysis, event, fmax)))\n",
    "    del stcs\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute the usual contrasts.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for k,v in condict.iteritems():\n",
    "    \n",
    "    stc = np.dot(SPMs, v)\n",
    "    stc.save(os.path.join(root_dir, 'afMSIT_source_%s_%s_%s' %(analysis, k, fmax)))\n",
    "    \n",
    "print 'Done.'"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_position": {
    "height": "767px",
    "left": "0px",
    "right": "1089px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "827px",
   "left": "0px",
   "right": "auto",
   "top": "106px",
   "width": "241px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
